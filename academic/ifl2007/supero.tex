\documentclass{llncs}

\usepackage{url}
\usepackage{bar}
\usepackage{comment}

%include polycode.fmt
%include supero.fmt

\newenvironment{fig}
    {\begin{figure}[tbp]\hrule}
    {\end{figure}}

\newcommand{\figend}{\hrule}
\newcommand{\perc}{\%
    }
\newcommand{\noexample}{\hfill$\Box$}


\begin{document}

\title{Supero: Making Haskell Faster}

\author{Neil Mitchell and Colin Runciman}

\institute{University of York, UK, \url{http://www.cs.york.ac.uk/~ndm}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Haskell is a functional language, with features such as higher order functions and lazy evaluation, which allow succinct programs. These high-level features are difficult for fast execution, but GHC is a mature and widely used optimising compiler. This paper presents a whole-program approach to optimisation, which produces speed improvements of between 10\% and 60\% when used with GHC, on eight benchmarks.
\end{abstract}

\section{Introduction}

Haskell \cite{haskell} can be used in a highly declarative manner, to express specifications which are themselves executable. Take for example the task of counting the number of words in a file read from the standard input. In Haskell, one could write:

\begin{code}
main = print . length . words =<< getContents
\end{code}

From right to left, the |getContents| function reads the input as a list of characters, |words| splits this list into a list of words, |length| counts the number of words, and finally |print| writes the value to the screen.

An equivalent C program is given in Figure \ref{fig:c_words}. Compared to the C program, the Haskell version is more concise and more easily seen to be correct. Unfortunately, the Haskell program (compiled with GHC) is also three times slower than the C version (compiled with GCC). This slowdown is caused by several factors:

\begin{fig}
\bigskip
\begin{verbatim}
int main()
{
	int i = 0;
	int c, last_space = 1, this_space;
	while ((c = getchar()) != EOF) {
		this_space = isspace(c);
		if (last_space && !this_space)
			i++;
		last_space = this_space;
	}
	printf("%i\n", i);
	return 0;
}
\end{verbatim}
\figend
\caption{Word counting in C.}
\label{fig:c_words}
\end{fig}

\begin{description}
\item[Intermediate Lists] The Haskell program produces and consumes many intermediate lists as it computes the result. The |getContents| function produces a list of characters, |words| consumes this list and produces a list of lists of characters, |length| then consumes the outermost list. The C version uses no intermediate data structures.
\item[Functional Arguments] The |words| function is defined using the |dropWhile| function, which takes a predicate and discards elements from the input list until the predicate becomes true. The predicate is passed as an invariant function argument in all applications of |dropWhile|.
\item[Laziness and Thunks] The Haskell program proceeds in a lazy manner, first demanding one character from |getContents|, then processing it with each of the functions in the pipeline. At each stage, a lazy thunk for the remainder of each function is created.
\end{description}

Using the optimiser developed in this paper we can eliminate all these overheads. We obtain a program that performs \textit{faster} than the C version. The central idea of the optimiser is to evaluate as much of the program as possible at compile time, leaving a residual program consisting only of actions dependent on the input data.

Our goal is an automatic optimisation that makes high-level Haskell programs run as fast as low-level equivalents, eliminating the current need for hand-tuning and low-level techniques to obtain competitive performance. We require no annotations on any part of the program, including the library functions.

\subsection{Roadmap}

We first introduce a Core language in \S\ref{sec:core}, on which all transformations are applied. Next we describe our optimisation method in \S\ref{sec:optimisation}. We then give a number of benchmarks, comparing both against C (compiled with GCC) in \S\ref{sec:c_results} and Haskell (compiled with GHC) in \S\ref{sec:haskell_results}. Finally, we review related work in \S\ref{sec:related} and conclude in \S\ref{sec:conclusion}.

\section{Core Language}
\label{sec:core}

\begin{fig}
\begin{code}
expr  =  v                                          {-" \text{  variable} "-}
      |  c                                          {-" \text{  constructor} "-}
      |  f                                          {-" \text{  function} "-}
      |  x y                                        {-" \text{  application} "-}
      |  \v -> x                                    {-" \text{  lambda abstraction} "-}
      |  let v = x in y                             {-" \text{  let binding} "-}
      |  case x of {p_1 -> y_1 ; ... ; p_n -> y_n}  {-" \text{  case expression} "-}

pat   =  c vs_
\end{code}

Where |v| ranges over variables, |c| ranges over constructors, |f| ranges over functions, |x|, |y| and |z| range over expressions and |p| ranges over patterns.
\bigskip
\figend
\caption{Core syntax}
\label{fig:core}
\end{fig}

All our optimisations operate on a standard Core language, documented in \cite{me:yhc_core}. The expression type is given in Figure \ref{fig:core}. A program is a mapping of function names to expressions. Our Core language is higher order and lazy, but lacks much of the syntactic sugar found in Haskell. Pattern matching occurs only in case expressions, and all case expressions are exhaustive. All names are fully qualified. Haskell's type classes have been removed by the dictionary transformation \cite{wadler:type_classes}.

The Yhc compiler, a fork of nhc \cite{nhc}, can output Core files. Yhc can also link in all definitions from all required libraries, producing a single Core file representing the whole program.

The primary difference between Yhc-Core and GHC-Core \cite{ghc_core} is that Yhc-Core is untyped. The Core is generated from well-typed Haskell, and is guaranteed not to fail with a type error. All the transformations could be implemented equally well in a typed Core language, but we prefer to work in an untyped language for simplicity of implementation.

In order to avoid accidental variable name clashes while performing transformations, we demand that all variables within a program are unique. All transformations may assume this invariant, and must ensure it as a postcondition.

\section{Optimisation}
\label{sec:optimisation}

Our optimisation procedure takes a Core program as input, and produces a new equivalent Core program as output. To improve the program we do not make small local changes to the original, but instead \textit{evaluate it} so far as possible at compile time, leaving a \textit{residual program} to be run.

Each function in the output program is an optimised version of some associated expression in the input program. Optimisation starts at the |main| function, and optimises the expression associated with |main|. Once the expression has been optimised, the outermost element in the expression becomes part of the residual program. All the subexpressions are assigned names, and will be given definitions in the residual program. If any expression (up to alpha renaming) already has a name in the residual program, then the same name is used. Each of these named inner expressions is then optimised as before.

\begin{fig}
\begin{code}
_O\<case x of alts_  \> =  case _O\<x\> \? of alts_
_O\<let v = x in y   \> =  let v = _O\<x\> \? in _O\<y\>
_O\<x y              \> =  _O\<x\> \? y
_O\<f                \> =  unfold f  , where f {-" \hbox{ is a non-primitive, non-CAF function} "-}
                        =  f         , otherwise
_O\<v                \> =  v
_O\<c                \> =  c
_O\< \v -> x         \> =  \v -> x
\end{code}
\figend
\caption{Optimisation rules.}
\label{fig:optimise}
\end{fig}

\begin{fig}
\begin{code}
case (case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) of alts_
    => case x of  {  p_1  -> case y_1 of alts_
                  ;  ...
                  ;  p_n  -> case y_n of alts_ }

case c xs_ of {... ; c vs_ -> y ; ...}
    => y[vs_/xs_]

case v of {... ; c vs_ -> x ; ...}
    => case v of {... ; c vs_ -> x[v/c vs_]; ...}

case (let v = x in y) of alts_
    => let v = x in case y of alts_

(let v = x in y) z
    => let v = x in y z

(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}

(\v -> x) y
    => let v = y in x

let v = x in (case y of {p_1 -> y_1 ; ... ; p_n -> y_n})
    => case y of  {  p_1  -> let v = x in y_1
                  ;  ...
                  ;  p_n  -> let v = x in y_n}
    where v {-" \hbox{is not used in} "-} y

let v = x in y
    => y[v/x]
    where x {-" \hbox{is a lambda, variable, or used once in } "-} y

let v = c x_1 ... x_n in y
    =>  let v_1 = x_1 in
        ...
        let v_n = x_n in
        y[v/c x_1 ... x_n]
    where v_1 ... v_n {-" \hbox{ are fresh} "-}
\end{code}
\figend
\caption{Simplification rules.}
\label{fig:simplify}
\end{fig}

Optimisation uses the |_O| rules in Figure \ref{fig:optimise}, and the simplification rules in Figure \ref{fig:simplify}. We define |_OO| to be the result of applying both |_O| and the simplification rules until no further changes are made. Optimisation is like evaluation, but stops if the expression to reduce is a free variable, a constructor, a primitive, or a CAF (constant applicative form -- see \S\ref{sec:caf} for more details). The one difference is that in a |let| expression the bound expression and the inner expression are \textit{both} optimised -- see \S\ref{sec:let} for the reasons. The simplification rules are all standard, and similar rules would be found in most optimising compilers.

\subsubsection{Example 1} \hfill

\begin{code}
main = \xs -> map inc xs

map = \f -> \xs -> case  xs of
                         []    -> []
                         y:ys  -> f y : map f ys

inc = \x -> x+1
\end{code}

This program defines a |main| function which increments each value in the list by one. Our |main| function is not a valid Haskell program, as it has the wrong type, but serves to illustrate the techniques. Note that |f| is passed around at runtime, when it could be frozen in at compile time. By following the optimisation procedure we end up with:

\begin{code}
main = \xs -> case  xs of
                    []    -> []
                    y:ys  -> f0 y ys

f0 = \y -> \ys -> (y+1) : main ys
\end{code}

And finally by performing some trivial inlining we can obtain:

\begin{code}
main = \xs -> case  xs of
                    []    -> []
                    y:ys  -> (y+1) : main ys
\end{code}

The residual program is now optimised -- there is no runtime passing of the |inc| function, only a direct arithmetic operation. \noexample

\subsubsection{Example 2} \hfill \vspace{2mm}

\noindent Our next example shows how our optimisation rules can carry out list deforestation \cite{wadler:deforestation}.

\begin{code}
main xs = map (+1) (map (*2) xs)

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

The main definition is transformed (after trivial inlining) into:

\begin{code}
main xs = case  xs of
                []    -> []
                y:ys  -> (y*2)+1 : main ys
\end{code}

\noindent The intermediate list has been removed, and the higher order functions eliminated by specialisation.\noexample


\subsection{Termination}
\label{sec:termination}

A problem with the method as presented so far is that it may not terminate. There are several ways that non-termination can arise. We consider, and eliminate, each in turn.

\subsubsection{Infinite Unfolding}

Consider the definition:

\begin{code}
name = \x -> name x
\end{code}

If the expression |name x| was being optimised then the optimisation function |_OO| would not terminate. We can solve this problem by either bounding the number of unfoldings, or by keeping a list of previously encountered intermediate expressions in |_OO|. In practice, this situation is rare, and either choice is acceptable. We choose to bound the number of unfoldings. A large limiting value is used, which does not impact either compilation time or memory consumption in the common case.

\subsubsection{Accumulating parameters}

Consider the definition:

\begin{code}
reverseAcc = \xs -> \ys -> case  xs of
                                 []    -> []
                                 z:zs  -> reverseAcc zs (z:ys)
\end{code}

This function is the standard |reverse| function, with an accumulator. The problem is that successive iterations of the optimisation produce progressively larger subexpressions. A definition is first created for |reverseAcc _ _|, then for |reverseAcc _ (_:_)|, then |reverseAcc _ (_:_:_)|. The residual program is infinite.

The solution is to bound the size of the input expression associated with each definition in the residual program. The size of the expression being optimised can be reduced by lifting subexpressions into a let binding, then placing this let binding in the residual program. By bounding the size of the expression, we bound the number of functions in the residual program.

If the bound is too high, optimisation takes too long and the residual program is excessively large. If the bound is too low then too little is achieved by optimisation. We return to the issue of the size of this bound in \S\ref{sec:results_bound}.

\subsubsection{Direct Repetition}
\label{sec:direct}

We claim that |_OO| terminates with bounded unfoldings and bounded expression size. It is often useful to detect an expression which appears to be repeating, and preemptively bound it. Consider the |reverseAcc| example -- the recursive pattern is an instance of \textit{direct repetition}. Let |alpha| be a context, and |alpha <^ e ^>| be the result of substituting |e| for the hole in the context |alpha|. An expression |x| is directly repeating if |x ~= alpha <^ alpha <^ beta ^> ^>| where |beta| is an expression, |alpha| is a non-empty context and |~=| is equality where all variables are considered equal.

\subsubsection{Example 3} \hfill\vspace{2mm}

\noindent The following expressions have direct repetition.

\begin{code}
x:y:xs   where alpha = x:hole  , beta = xs
f (f x)  where alpha = f hole  , beta = x
case x_1 of {[] -> nil ; y:ys -> case x_2 of {[] -> nil ; z:zs -> cons}}
    where alpha = case x_1 of {[] -> nil; y:ys -> hole}, beta = cons
\end{code}
\noexample

If direct repetition is encountered, then the repeating expression is lifted to a top-level let binding, and output directly into the residual program.

\subsubsection{Example 4} \hfill\vspace{2mm}

\noindent Take the |reverseAcc| example. During optimisation, the expression becomes:

\begin{code}
reverseAcc xs (y_1:y_2:ys)
\end{code}

\noindent The second argument to |reverseAcc| is an instance of direct repetition, and is lifted to a let binding.

\begin{code}
let v = y_1:y_2:ys
in reverseAcc xs v
\end{code}

\noindent Now the expression bound at the let, and the inner expression, are optimised separately. \noexample


\subsection{Let Bindings}
\label{sec:let}

The rule for let bindings in Figure \ref{fig:optimise} may seem curious. The other rules simply follow evaluation order, but the let rule optimises \textit{both} the bound expression and the inner expression. This is a critical choice, which enhances the optimisation performed by the system, without duplicating computation of let bindings.

In the Core language a let expression introduces a binding, which is shared. Given the expression |let v = x in y|, even if |v| is referred to multiple times in |y|, then the expression |x| is computed at most once. It is important that sharing of \textit{expensive} functions is preserved. Yet, by inlining \textit{cheap} let expressions, better optimisation can be achieved. Taking the following fragment from a previous example:

\begin{code}
let f = inc
in f y : map f ys
\end{code}

If |f| is not inlined, then the recursive call to |map| would still contain a functional variable to be passed at runtime. But how can we tell whether |inc| is cheap enough to be inlined? The solution is to optimise |inc| first:

\begin{code}
let f = \x -> x + 1
in f y : map f ys
\end{code}

It is now clear that |f| is a lambda, so no shared computation is lost by inlining it.

\subsection{CAF's}
\label{sec:caf}

A CAF (constant applicative form) is a top level definition of zero arity. In Haskell, CAFs are computed at most once per program run, and retained as long as references to them remain. Consider the program:

\begin{code}
caf = expensive

main = caf + caf
\end{code}

In this program |caf| would only be computed once. If a CAF is inlined then this may result in a computation being performed more than would otherwise occur. To ensure that we do not duplicate computations, we never inline CAF's.

\section{Performance Compared With C Programs}
\label{sec:c_results}

\begin{comment}
The most comprehensive inter-language benchmarking effort is the Programming Language Shootout\footnote{\url{http://shootout.alioth.debian.org/}}. In the shootout a variety of tasks are implemented in many languages and benchmarked against each other. Unfortunately, in order to ensure a balanced comparison, many restrictions are placed upon the solutions -- some of which severely hamper lazy languages. Inspired by their attempt, we have defined some simpler benchmarks on which it is possible to directly compare Haskell to C.
\end{comment}

The benchmarks we have chosen are inspired by the Unix \texttt{wc} command -- namely character, word and line counting. We require the program to read from the standard input, and write out the number of elements in the file. To ensure that we test computation speed, not IO speed (which is usually determined by the buffering strategy, rather than optimisation) we demand that all input is read using the standard C |getchar| function only. Any buffering improvements, such as reading in blocks or memory mapping of files, could be performed equally in all compilers.

All the C versions are implemented following a similar pattern to Figure \ref{fig:c_words}. Characters are read in a loop, with an accumulator recording the current value. Depending on the program, the body of the loop decides when to increment the accumulator. The Haskell versions all follow the same pattern as in the Introduction, merely replacing |words| with |lines|, or removing the |words| function for character counting.

We performed all benchmarks on a machine running Windows XP, with a 3GHz processor and 1Gb RAM. All benchmarks were run over a 50Mb log file, repeated 10 times, and the lowest value was taken. The C versions used GCC\footnote{\url{http://gcc.gnu.org/}} version 3.4.2 with -O3. The Haskell version used GHC \cite{ghc} 6.6.1 with -O2. The Supero version was compiled using our optimiser, then written back as a Haskell file, and compiled once more with GHC 6.6.1 and -O2.

\begin{fig}
\vspace{7mm}
\makebox[33mm][r]{
    \begin{barenv}
    \legend{1}{C$\;\;\;\;\;\;$}
    \legend{2}{Supero+GHC$\;\;\;\;\;\;$}
    \legend{3}{GHC$\;\;\;\;\;\;$}
    \end{barenv}
}
\begin{barenv}
\setwidth{20}
\setdepth{0}
\sethspace{0.05}
\setstretch{6}
\setnumberpos{empty}
% \setxaxis{0}{1}{1}
% \setxname{$n$}
\setyaxis{0}{25}{5} \setyname{Seconds}
\bar{6}{1}
\bar{6.3}{2}[characters]
\bar{12.5}{3}
\bar{0}{0}
\bar{6.2}{1}
\bar{6.5}{2}[lines]
\bar{17}{3}
\bar{0}{0}
\bar{7.3}{1}
\bar{6.9}{2}[words]
\bar{22}{3}
\end{barenv}
\vspace{7mm}
\figend
\caption{Benchmarks with C, Supero+GHC and GHC alone.}
\label{fig:c_results}
\end{fig}

The results are given in Figure \ref{fig:c_results}. In all the benchmarks C and Supero are within 10\% of each other, while GHC trails further behind.

\subsection{Identified Haskell Speedups}

During initial trials using these benchmarks, we identified two unnecessary bottlenecks in the Haskell version of word counting. Both were remedied before the presented results were obtained.

\paragraph{Slow \textsf{isSpace} function}

The first issue is that |isSpace| in Haskell is much more expensive than |isspace| in C. The simplest solution is to use a FFI (Foreign Function Interface) \cite{spj:awkward_squad} call to the C |isspace| function in all cases, removing this factor from the benchmark. A GHC bug (number 1473) has been filed about the slow performance of |isSpace|.

\begin{fig}
\begin{code}
words :: String -> [String]
words s = case  dropWhile isSpace s of
                []  ->  []
                x   ->  w : words y
                        where (w, y) = break isSpace x

words' s = case  dropWhile isSpace s of
                 []    ->  []
                 x:xs  ->  (x:w) : words' (drop1 z)
                           where (w, z) = break isSpace xs

drop1 []      = []
drop1 (x:xs)  = xs
\end{code}
\figend
\caption{The |words| function from the Haskell standard libraries, and an improved |words'|.}
\label{fig:words}
\end{fig}

\paragraph{Inefficient \textsf{words} function}

The second issue is that the standard definition of the |words| function (given in Figure \ref{fig:words}) performs two additional |isSpace| tests per word. By appealing to the definitions of |dropWhile| and |break| it is possible to show that in |words| the first character of |x| is not a space, and that if |y| is non-empty then the first character is a space. The revised |words'| function uses these facts to avoid the redundant |isSpace| tests.

\subsection{Potential GHC Speedups}

We have identified three factors limiting the performance of residual programs when compiled by GHC. These problems cannot be solved at the level of Core transformations. We suspect that by fixing these problems, the Supero execution time would improve by between 5\% and 15\%.

\paragraph{Strictness inference}

The GHC compiler is overly conservative when determining strictness for functions which use the FFI (GHC bug 1592). The |getchar| function is treated as though it may raise an exception, and terminate the program, so strict arguments are not determined to be strict. If GHC provided some way to mark an FFI function as not generating exceptions, this problem could be solved. The lack of strictness information means that in the line and word counting programs, every time the accumulator is incremented, the number is first unboxed and then reboxed \cite{spj:unboxing}.

\paragraph{Heap checks}

The GHC compiler follows the standard STG machine \cite{spj:implementation} design, and inserts heap checks before allocating memory. The purpose of a heap check is to ensure that there is sufficient memory on the heap, so that allocation of memory is a cheap operation guaranteed to succeed. GHC also attempts to lift heap checks: if two branches of a case expression both have heap checks, they are replaced with one shared heap check before the case expression. Unfortunately, with lifted heap checks, a tail-recursive function that allocates memory only upon exit can have the heap test executed on every iteration (GHC bug 1498). This problem affects the character counting example, but if the strictness problems were solved, it would apply equally to all the benchmarks.

\paragraph{Stack checks}

The final source of extra computation relative to the C version are stack checks. Before using the stack to store arguments to a function call, a test is performed to check that there is sufficient space on the stack. Unlike the heap checks, it is necessary to analyse a large part of the flow of control to determine when these checks are unnecessary. So it is not clear how to reduce stack checks in GHC.

\subsection{Why Supero Outperforms C for the Wordcount Benchmark}

The most curious result is that Supero outperforms C on wordcounting, by about 6\% -- even with the problems discussed! The C program presented in Figure \ref{fig:c_words} is not optimal. The variable \verb"last_space" is a boolean, indicating whether the previous character was a space, or not. Each time round the loop a test is performed on \verb"last_space", even though its value was determined and tested on the previous iteration. The way to optimise this code is to have two specialised variants of the loop, one for when \verb"last_space" is true, and one for when it is false. When the value of \verb"last_space" changes, the program would transition to the other loop. This pattern effectively encodes the boolean variable in the program counter, and is what the Haskell program has managed to generate from the high-level code.

However, in C it is quite challenging to capture the required control flow! The program needs two loops, where both loops can transition to the other. Using \texttt{goto} turn off many critical optimisations in the C compiler. Tail recursion is neither required by the C standard, nor supported by most compilers. The only way to express the necessary pattern is using nested while loops, but unlike newer imperative languages such as Java, C does not have named loops -- so the inner loop cannot break from the outer loop if it reaches the end of the file. The only solution is to place the nested while loops in a function, and use \texttt{return} to break from the inner loop. This solution would not scale to a three-valued control structure, and substantially increases the complexity of the code.

\section{Performance Compared With GHC Alone}
\label{sec:haskell_results}

The standard set of Haskell benchmarks is the nofib suite \cite{nofib}. It is divided into three categories of increasing size: imaginary, spectral and real. Many small Haskell programs increase in size substantially once the libraries are included, particularly when type classes are involved. Because of the relatively large source code size of even small examples, we have limited our focus to five benchmarks drawn from the imaginary section. We have chosen programs which do not perform large amounts of IO.

The benchmarks are: digits-of-e1 and digits-of-e2, both of which compute the digits of |e| by different methods; exp3\_8 computes $3^8$ using Peano numbers and the |Num| class; primes computes a list of prime numbers; and queens counts the safe layouts of queen pieces on a chess board. All benchmarks were run with parameters that require runtimes of between 3 and 5 seconds for GHC.

\begin{fig}
\vspace{7mm}
\begin{barenv}
\setwidth{20}
\setdepth{0}
\sethspace{0.5}
\setstretch{1}
\setnumberpos{empty}
% \setxaxis{0}{1}{1}
% \setxname{$n$}
\setyaxis{0}{100}{10} \setyname{\% of GHC run-time}
\bar{0}{0}
\bar{90}{1}[digits-of-e1]
\bar{0}{0}
\bar{75}{1}[digits-of-e2]
\bar{0}{0}
\bar{35}{1}[exp3\_8]
\bar{0}{0}
\bar{87}{1}[primes]
\bar{0}{0}
\bar{78}{1}[queens]
\bar{0}{0}
\end{barenv}
\vspace{3mm}
\figend
\caption{Runtime, relative to GHC.}
\label{fig:haskell_results}
\end{fig}

\begin{table}[tb]
\hrule
\vspace{3mm}

\begin{tabular}{lrrrrr}
\textbf{Program} & \hspace{5mm}\textbf{Source} & \hspace{5mm}\textbf{Residual} & \hspace{5mm}\textbf{Bound} & \hspace{5mm}\textbf{\% GHC Size} & \hspace{5mm}\textbf{\% GHC Time} \\
digits-of-e1 &  521 & 1676 & 13 & 110 & 90 \\
digits-of-e2 & 1235 &  515 & 12 &  99 & 75 \\
exp3\_8      &  380 & 1138 &  5 & 104 & 35 \\
primes       &  422 &  356 & 12 & 101 & 87 \\
queens       &  637 & 4265 &  8 & 116 & 78 \\
\end{tabular}
\vspace{2mm}

\textbf{Program} is the name of the program; \textbf{Source} is the number of lines of pretty printed source code including all libraries; \textbf{Residual} is the number of lines after optimisation; \textbf{Bound} is the termination bound used; \textbf{Size} is the size of the resultant binary as a percentage of the GHC binary size; \textbf{Time} is the runtime as a percentage of GHC run-time.

\vspace{4mm}
\hrule
\vspace{2mm}
\caption{Result on the nofib suite.}
\label{tab:haskell_results}
\end{table}

The results of these benchmarks are given in Figure \ref{fig:haskell_results}, along with detailed breakdowns in Table \ref{tab:haskell_results}. In all benchmarks Supero+GHC performs at least 10\% faster than GHC alone, and in one case is nearly three times faster. Binaries were at most 10\% larger than those from GHC alone, and in one case the binary was even marginally smaller.

\subsection{GHC's optimisations}

For these benchmarks it is important to clarify which optimisations are performed by GHC, and which are performed by Supero. Core output from Yhc, compiled using GHC without any prior optimisation, would \textit{not} perform as well as the original program compiled using GHC. GHC has two special optimisations that work in a restricted number of cases, but which Supero is unable to take advantage of.

\paragraph{Dictionary Removal} Functions which make use of type classes are given an additional dictionary argument. In practice, GHC specialises many such functions by creating code with a particular dictionary frozen in. This optimisation is specific to type classes -- a tuple of higher order functions is not similarly specialised. After compilation with Yhc, the type classes have already been converted to tuples, so Supero must be able to remove the dictionaries itself. One benchmark where dictionary removal is critical is digits-of-e2.

\paragraph{List Fusion} GHC relies on names of functions, particularly |foldr|/|build| \cite{spj:rules}, to apply special optimisation rules such as list fusion. Many of GHC's library functions, for example |iterate|, are defined in terms of |foldr| to take advantage of these special properties. After transformation with Yhc, these names are destroyed, so no rule based optimisation can be performed. One example where list fusion is critical is primes, although it occurs in most of the benchmarks to some extent.

Supero has no special purpose optimisations which rely on named functions or desugaring knowledge. The one benchmark where no GHC specific optimisations apply is exp3\_8, which operates solely on Peano numbers -- a type GHC has no inbuilt knowledge of. Hence the advantage of Supero in exp3\_8: while GHC is limited to basic inline/simplify transformations, Supero is able to remove some intermediate data structures.

\subsection{Termination Bound}
\label{sec:results_bound}

Table \ref{tab:haskell_results} includes a column indicating the size bound that was applied to expressions. Out of the five benchmarks, both primes and queens could be run at any greater bound and would still produce the same program -- the direct repetition criteria (see \S\ref{sec:direct}) bounds the expressions on its own. For the remaining programs, a bound was chosen to ensure that the compilation process was quick (under two seconds). By increasing the termination bound the size of the residual program would increase, but the generated program may execute faster.

The existence of a termination bound requiring different values for different programs is a cause for concern. In a large program it is likely that different parts of the program would require different bounds on the size of the generated expression -- something not currently possible. We suspect that the most promising direction is to augment the direct repetition criterion to obtain termination in all practical cases without resorting to a depth bound.

\section{Related Work}
\label{sec:related}

\paragraph{Partial evaluation} There has been a lot of work on partial evaluation \cite{jones:partial_evaluation}, where a program is specialised with respect to some static data. The emphasis is on determining which variable can be entirely computed at compile time, and which must remain in the residual program. Partial evaluation is particularly appropriate for specialising an interpreter with an expression tree to generate a compiler automatically, often with an order of magnitude speedup, known as the First Futamura Projection \cite{futanama:projections}. The difference between our work and partial evaluation is that we fold back definitions, and perform no binding time analysis. Our method is certainly less appropriate for specialising an interpreter, but in the absence of static data, is still able to show improvements.

\paragraph{Deforestation} The deforestation technique \cite{wadler:deforestation} removes intermediate lists in computations. This technique has been extended in many ways to encompass higher order deforestation \cite{marlow:higher_order_deforestation} and work on other data types \cite{coutts:string_fusion}. Probably the most practically motivated work on deforestation has come from those attempting to restrict deforestation, in particular shortcut deforestation \cite{gill:shortcut_deforestation}, and newer approaches such as stream fusion \cite{coutts:stream_fusion}. In this work certain named functions are automatically fused together. By rewriting library functions in terms of these special functions, fusion occurs. Shortcut deforestation is limited to cases where the correct underlying function is used -- sometimes requiring unnatural definitions.

\paragraph{GRIN} The GRIN approach \cite{grin} is currently being implemented in the jhc compiler \cite{jhc}, with promising initial results. GRIN works by first translating to a monadic intermediate language, then repeatedly performing a series of optimisations, using whole program transformation. The intermediate language is at a much lower level than our Core language, so jhc is able to perform detailed optimisations that we are unable to express.

\paragraph{Other Transformations} One of the central operations within our optimisation in inlining, a technique that has been used extensively within GHC \cite{spj:inlining}. We generalise the constructor specialisation technique \cite{spj:specconstr}, by allowing specialisation on any arbitrary expression, including constructors.

\begin{comment}
One optimisation we do not currently support is the use of user provided transformation rules \cite{spj:rules}, which can be used to automatically replace certain expressions with others -- for example |sort . nub| removes duplicates then sorts a list, but can be done asymptotically faster in a single operation.
\end{comment}

\paragraph{Lower Level Optimisations} Our optimisation works at the Core level, but even once optimal Core has been generated there is still some work before optimal machine code can be produced. Key optimisations include strictness analysis and unboxing \cite{spj:unboxing}. In GHC both of these optimisations are done at the Core level, using a Core language extended with unboxed types. After this lower level Core has been generated, it is then transformed in to STG machine instructions \cite{spj:stg}, before being transformed into assembly code. There is still work being done to modify the lowest levels to take advantage of the current generation of microprocessors \cite{marlow:pointer_tagging}. We rely on GHC to perform all these optimisations after Supero generates a residual program.

\section{Conclusions and Future Work}
\label{sec:conclusion}

We have introduced an optimising front-end which can enhance the results of back-end compilation using GHC -- at least for some small programs. Our optimiser is simple -- the Core transformation is expressed in just 300 lines of Haskell, yet it replicates many of the performance enhancements of GHC in a more general way. Our initial results are promising, but incomplete. There are three main obstacles that need to be tackled:

\begin{description}
\item[Termination] We are confident that Supero terminates, but only by use of a crude bound on expression size whose optimal value varies for different programs. To increase the applicability of our optimiser, we would like to remove the depth bound, or at least reduce our reliance upon it.
\item[Benchmarks] Eight small benchmarks are not enough. We would like to obtain results for all the remaining benchmarks in the nofib suite.
\item[Performance] The performance results presented in \S\ref{sec:haskell_results} are disappointing. Earlier versions of Supero were able to obtain a 50\% speed up in the primes benchmark, but decreased performance in other benchmarks. We suspect that much better performance can be obtained.
\end{description}

The Programming Language Shootout\footnote{\url{http://shootout.alioth.debian.org/}} has shown that low-level Haskell can compete with with low-level imperative languages such as C. Our goal is that Haskell programs can be written in a high-level declarative style, yet still perform competitively.

\paragraph{Acknowledgements} We would like to thank Simon Peyton Jones, Simon Marlow and Tim Chevalier for help understanding the low-level details of GHC, and Peter Jonsson for helpful discussions.

\bibliographystyle{plain}

\bibliography{supero}

\end{document}
