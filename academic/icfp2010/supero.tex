\documentclass[draft]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{balance}

\include{paper}
%include paper.fmt

% Consistency:
% v,w,x,y,xs,ys,z,zs are all variables
% e is an expression
% p is a pattern
% f,g are functions
% m,n are lengths
% i,j are indexes

%format <? = "[\!["
%format ?> = "]\!]"
%format += = "+\!\!\!\!="
%format ==> = "\Longrightarrow{}"
%format <=| = "\unlhd{}"
%format <| = "\lhd{}"

%format w_1  = "\Varid{w_1}"
%format w_2  = "\Varid{w_2}"
%format w_3  = "\Varid{w_3}"
%format w_n  = "\Varid{w_n}"
%format w_12  = "\Varid{w_12}"
%format w_123  = "\Varid{w_123}"
%format v_3  = "\Varid{v_3}"
%format v_4  = "\Varid{v_4}"
%format map_1 = "\Varid{map_1}"
%format map_2 = "\Varid{map_2}"
%format e_1' = "\Varid{e_1^{\prime}}"
%format e_2' = "\Varid{e_2^{\prime}}"
%format e_m' = "\Varid{e_m^{\prime}}"
%format e_1 = "\Varid{e_1}"
%format p_m = "\Varid{p_m}"
%format s_m = "\Varid{s_m}"
%format v_j = "\Varid{v_j}"
%format v_5 = "\Varid{v_5}"
%format v_6 = "\Varid{v_6}"
%format x_3 = "\Varid{x_3}"
%format x_4 = "\Varid{x_4}"
%format x_m = "\Varid{x_m}"
%format s_m_1 = "\Varid{s_{m-1}}"
%format w_m = "\Varid{w_m}"

\newcommand{\unknown}{XXX}
\newcommand{\name}[3]{\ensuremath{\langle\mathsf{#1},\mathsf{#2},\mathsf{#3}\rangle}}
\newcommand{\lemma}[1]{\subsubsection*{\textit{Lemma: #1}}}

\newcommand{\setsup}{\supset_{\mathrm{set}}}
\newcommand{\setequiv}{\equiv_{\mathrm{set}}}
\newcommand{\bagsub}{\subset_{\mathrm{bag}}}
\newcommand{\bagequiv}{\equiv_{\mathrm{bag}}}
\newcommand{\set}{\mathrm{set}}
\newcommand{\veeskip}{\;\vee\;}

\newcommand{\subsubsubsection}[1]{\subsubsection*{#1}}


\begin{document}

\conferenceinfo{ICFP 2010}{}
% \CopyrightYear{2009}
% \copyrightdata{978-1-60558-508-6/09/09}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Rethinking Supercompilation}
% \subtitle{}

\authorinfo{Neil Mitchell}
           {\verb"ndmitchell@gmail.com"}

\maketitle

\begin{abstract}
Supercompilation is a program optimisation technique, which can often eliminate the overhead of program abstractions. We present a new design for a supercompiler, rethinking many of the decisions that have become common in previous supercompilation work. The result is a supercompiler that we find simpler, and offers good performance both at compile time and runtime. We have implemented our supercompiler and benchmarked it on a small selection of programs from the nofib benchmark suite, which run faster than when compiled with GHC. We have put our supercompiler to practical use, optimising an HTML parsing library used in biological research, reducing the runtime by over an hour.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
Languages

\keywords
Haskell, optimisation, supercompilation

\section{Introduction}

\todo{perhaps switch to tagsoup as the opening example, so I can come back to it? yes - if it works}

Supercompilation is a technique for program optimisation, that is particularly suited to removing the overhead introduced by abstractions. Consider a program that counts the number of words read from the standard input -- in Haskell \cite{haskell} this can be compactly written as:

\begin{code}
main = print . length . words =<< getContents
\end{code}

Reading the program right to left, we first read the standard input as a string (|getContents|), then split it in to words (|words|), count the number of words (|length|), and print the result (|print|). An equivalent C program is unlikely to use such a high degree of abstraction, and is more likely to get characters and operate on them in a loop using some state which is updated.

Sadly the C program is three times faster, even using the advanced optimising compiler GHC \cite{GHC}. The abstractions that make the program concise have a significant runtime cost. In a previous paper on supercompilation \cite{me:supero} we showed how supercompilation can remove these abstractions, to the stage where the Haskell is even faster than the C version (by about 6\%). In the Haskell program after optimisation all the intermediate lists have been removed, and the |length . words| part of the pipeline is translated into a state machine.

One informal description of supercompilation is that you simply ``run the program at compile time''. This description leads to two questions -- what happens if you are blocked on information only available at runtime, and how do you ensure termination? Answering these questions provides the design for a supercompiler. This paper strives to provide simple answers, but which do not reduce the optimisation power. In doing so, we make a large number of departures from the current consensus of supercompiler design.

Our supercompiler is better because we've tried to keep it simple. Our document describes all the little details, in full colour. We have left nothing out. In doing so, we've ended up with a supercompiler that is also fast at compile time, and performs well at runtime. We hope that from the descriptions given in this paper a user is able to write their own supercompiler. Indeed, we have written a supercompiler following this design which is available online at \verb"http://community.haskell.org/~ndm". Much of the code (for example Figure \ref{fig:manager}) has been copied verbatim in to our compiler.

\subsection{Contributions}

Our primary contribution is the design of a new supercompiler (\S\ref{sec:method}). Our supercompiler has many differences from previous supercompilers (\S\ref{sec:comparison}), including a new core language, a substantially different treatment of let expressions, a transformation that never manipulates inside expressions and an entirely new termination criteria. The result is simpler than previous supercompilers, yet still powerful.

In addition, we make the following contributions:

\begin{itemize}
\item We give examples of how our supercompiler performs (\S\ref{sec:manager_example}, \S\ref{sec:term_example}, \S\ref{sec:examples}), including how it subsumes list fusion and specialisation, and what happens when the termination criteria are needed.
\item We benchmark the supercompiler on a small range of examples (\S\ref{sec:benchmarks}) -- our supercompiler achieves an improvement of \unknown{}\% when used in combination with GHC, as opposed to GHC alone.
\item We use our supercompiler to optimise an HTML parser, showing a \unknown{}\% speed increase (\S\ref{sec:tagsoup}). This HTML parser is regularly run on many gigabytes of data, and previously consumed several hours of CPU time each run.
\end{itemize}

\section{Method}
\label{sec:method}

This section describes our supercompiler. We first present a Core language (\S\ref{sec:core}), along with simplification rules (\S\ref{sec:simplify}). We then present the overall algorithm (\S\ref{sec:manager}), which uses answers to the following questions:

\begin{itemize}
\item How do you evaluate an open term? (\S\ref{sec:eval})
\item What happens if you can't evaluate an open term further? (\S\ref{sec:eval_split})
\item How do you know when to stop? (\S\ref{sec:term})
\item What happens if you have to stop? (\S\ref{sec:term_split})
\end{itemize}

\noindent Throughout this section we use the following example:

\begin{code}
root g f x = map g (map f x)

map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

This example applies |map| twice -- the expression |map f x| produces a list that is immediately consumed by |map g|. A good supercompiler should remove the intermediate list. Our supercompiler always optimises from the function named |root|.

\subsection{Core Language}
\label{sec:core}

\begin{figure}
\begin{code}
type Var   =   String -- variable/function names
type Con   =   String -- constructor names

data Exp   =   App Var [Var]
           |   Con Con [Var]
           |   Let [(Var,Exp)] Var
           |   Case Var [(Pat, Exp)]
           |   Lam Var Exp

type Pat   =   Exp -- restricted to |Con|
\end{code}
\caption{Core Language}
\label{fig:core}
\end{figure}

Our Core language for expressions is given in Figure \ref{fig:core}, and has much in common with Administrative Normal Form \cite{flanagan:continuations}. There are a few points to note:

\begin{itemize}
\item We require variables in many places that would normally permit expressions, including let bodies and application. A standard Core language (such as from \citet{ghc_core}) could be translated to ours by inserting let bindings.
\item In many Core languages a distinction is made between standard let bindings and recursive let bindings. We allow expressions bound at a let to refer to variables bound at the same let, but disallow recursion. An alternative description is that we allow recursive let, but only if there is an equivalent nested non-recursive let that can represent the same expression. For example we allow |let x = y; y = C in C| but not |let x = y; y = x in C|.
\item We don't have default patterns in case expressions, or literals. Both can be added, but are of little interest when describing a supercompiler.
\item We assume that the Core language is well-typed, in particular that we never over-apply a constructor or perform case analysis on a function.
\item We have vector application, rather than binary application, the reasons for which are described in \S\ref{sec:binaryapp}. We can use an application with no arguments to represent just a variable.
\item We require all free variables (those from the environment) to occur in the first argument of |App|.
\end{itemize}

We define the arity of a variable to be the number of arguments that need to be applied before reduction takes place. For example a variable bound to a lambda would have an arity of the number of lambda variables it was bound to. For primitives, the arity is the number of arguments before the primitive is actually evaluated. For example, in our example |map| has arity 2 and |root| has arity 3. The arity of |f|, |g| and |x| are all 0, as we don't know their true arity.

We write expressions using standard Haskell syntax (i.e. |let| for |Let|, |case| for |Case| etc.). Rewriting the |map|/|map| example in our Core language gives:

\begin{code}
root = \g f x ->  let  v_1 =  map f x
                       v_2 =  map g v_1
                  in   v_2
map = \f x -> case  x of
                    []    ->   let  v_1 = []
                               in   v_1
                    y:ys  ->   let  v_1 = f y
                                    v_2 = map f y
                                    v_3 = (:) v_1 v_2
                               in   v_3
\end{code}

Our Core language can be rather verbose, so sometimes we write expressions using a standard Core language, assuming these are translated to our Core language when necessary. For example, we might write |map| as:

\begin{code}
map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

\subsection{Simplified Core}
\label{sec:simplify}

We now define a simplified form for our Core language. When working with Core expressions we assume they are always simplified, and after constructing new expressions we always simplify them. Our simplified form requires that all variables are unique, that the root of a function be a sequence of lambda expressions, followed by a let expression, and that the expressions bound at the root let must \textit{not} have the following form:

\begin{itemize}
\item |App v []|, where |v| is bound locally -- we can remove the binding by replacing the bound variable with |v| everywhere.
\item |App v vs|, where |v| is bound to a |Con| -- the |App| can be replaced with a |Con| of higher arity.
\item |App v vs|, where |v| is bound to |App w ws| and the arity of |w| is higher than the length of |ws| -- the |App| can be replaced with a |App| with more arguments.
\item |App v vs|, where |v| is bound to a |Lam| -- the |App| can be replaced with the body of the lambda, with the variable substituted.
\item |Case v w|, where |v| is bound to a |Con| -- the |Case| can be replaced with the appropriate alternative.
\item |Let vs w| -- the nested bindings can be brought to the root |Let|, renaming variables if necessary.
\end{itemize}

We also remove any bindings which are not used. If there is only one bound variable, and that variable is bound to a lambda, we remove the let. These simplifications may result in a root let expression with no bindings. As an example, we can apply these rules to the following expression:

\begin{code}
let  v_1  = f
     v_2  = Con x
     v_3  = v_2 y
     v_4  = let w_1 = y in v_1 w_1
     v_5  = case v_3 of Con a b -> v_4 a
in   v_5
\end{code}

\noindent To give:

\begin{code}
let  v_4  = f y
     v_5  = v_4 x
in   v_5
\end{code}

\noindent If we knew that the arity of |f| was 2 this would be further simplified to:

\begin{code}
let  v_5  = f y x
in   v_5
\end{code}

\subsection{Manager}
\label{sec:manager}

\begin{figure}
\begin{code}
type Env = Var -> Maybe Exp
data Tree = Tree
    {pre :: Exp, gen :: [Var] -> Exp, children :: [Tree]}

supercompile :: Env -> [(Var,Exp)]
supercompile env =
    assign $ flatten $ optimise env $ fromJust $ env "root"

optimise :: Env -> Exp -> Tree
optimise env = f []
    where  f t x  | terminate (<=|) t x = g x (stop t x) t
                  | otherwise = g x (reduce env x) (x:t)
           g x (gen,cs) t = Tree x gen (map (f t) cs)

reduce :: Env -> Exp -> ([Var] -> Exp, [Exp])
reduce env = f []
    where f t x  | terminate (<|) t x = stop t x
                 | otherwise = case  step env x of
                                     Just x'  -> f (x:t) x'
                                     Nothing  -> split x

flatten :: Tree -> [Tree]
flatten = nubBy (\x y -> pre x == pre y) . f []
    where f seen t  =  if pre t `elem` seen then [] else
                       t : concatMap (f (pre t:seen)) (children t)

assign :: [Tree] -> [(Var,Exp)]
assign ts = [(f t, gen t (map f (children t))) |  t <- ts]
    where  f t = fromJust $ lookup (pre t) names
           names = zip (map pre ts) freshNames
\end{code}
\caption{The |supercompile| function.}
\label{fig:manager}
\end{figure}

\begin{figure}
\begin{code}
step :: Env -> Exp -> Maybe Exp -- \S\ref{sec:eval}
split :: Exp -> ([Var] -> Exp, [Exp]) -- \S\ref{sec:eval_split}

type History = [Exp]
(<|),(<=|) :: Exp -> Exp -> Bool -- \S\ref{sec:term}
terminate  :: (Exp -> Exp -> Bool)
           -> History -> Exp -> Bool -- \S\ref{sec:term}
stop :: History -> Exp -> ([Var] -> Exp, [Var]) -- \S\ref{sec:term_split}
\end{code}
\caption{Auxiliary definitions for Figure \ref{fig:manager}.}
\label{fig:manager2}
\end{figure}

Our supercompiler is based around a manager, that integrates the answers to the questions of supercompilation. The manager itself has two main purposes: to ensure termination, and to tie back recursive functions. In our experience the act of tieing recursion is often one of the trickiest aspects when writing a supercompiler, so we deliberately choose to give plenty of detail. The code for our manager is given in Figure \ref{fig:manager}, making use of a some auxiliary functions whose types are given in \ref{fig:manager2}. We start by giving an intuition for how the manager works, then describe each part separately.

The supercompiler takes a source program, and generates a target program. Functions in these programs are distinct -- target expressions cannot refer to source functions. The source and target program are semantically identical, but it is hoped the target program runs faster. We use the type |Env| to represent a mapping from function names to expressions, allowing a result of |Nothing| to indicate a primitive function. We can think of a root lambda expression as a function, or as an \textit{open expression} whose free variables are given by the lambda.

The manager first builds a tree tree, where each node in the tree is has a source expression (|pre|). The associated target expression may call other target functions, but these functions do not yet have names. Therefore, we store target expressions as a generator that when given the function names produces the target expression (|gen|), and a list of trees representing the target functions it needs to call (|children|). We then flatten this tree, ensuring identical functions are only represented once, and supply names to each node to generate the target program. If a target function is recursive then the initial tree will be infinite, but the flattened tree will always be finite due to the termination scheme defined in \S\ref{sec:term}.

\newcommand{\function}[1]{\paragraph{\textsf{#1:}}\hspace{-3mm}}

\function{supercompile} This function puts all the parts together. Reading from right to left, we first generate a potentially infinite tree by optimising the function |root|, we then flatten the tree to a finite number of resultant functions, and finally assign names to each of the result functions.

\function{optimise} This function constructs the tree of result functions. While the tree may be infinite, we demand that any infinite path from the root must encounter the same |pre| value more than once. We require that for any infinite sequence |ts| with no repeats, there must exist an |i| such that |terminate (<=||) (take i ts) (ts !! i+1)| returns |True|. If we are forced to terminate we call |stop|, which splits the expression into several subexpressions. We require that |split ts| only produces subexpressions which pass the termination test. If the termination criteria do not force us to stop, then we call |reduce| to evaluate the expression.

\function{reduce} This function optimises a function by repeatedly evaluating it by calling |step|. If we can't evaluate any further we call |split|. We use a local termination test to ensure the evaluation terminates. We require that for any infinite sequence |ts|, there must exist an |i| such that |terminate (<||) (take i ts) (ts !! i+1)| returns |True|.

\function{flatten} This function takes a tree and extracts a finite number of functions from it, assuming the termination restrictions given in |optimise|. Our |flatten| function will only keep one tree associated with each source expression. These trees may be different if one resulted from a call to |stop|, while another resulted from a call to |reduce| -- but all are semantically equivalent.

\function{assign} This function assigns names to each target function, and constructs the target expressions by calling |gen|. We assume the function |freshNames| returns the infinite list of fresh function names.

\subsubsection{The Example}
\label{sec:manager_example}

Revisiting our initial example, |supercompile| first calls |optimise| with:

\begin{code}
\g f x -> map g (map f x)
\end{code}

The termination context is empty, so we call |reduce|, which calls |step| repeatedly until we reach the expression:

\begin{code}
\g f x ->  let  v = case  w of
                          [] -> []
                          y:ys -> g y : map g ys
                w = case  x of
                          [] -> []
                          z:zs -> f z : map f zs
           in   v
\end{code}

The |step| function now returns |Nothing|, since the root cannot be reduced without the result of |x|. We therefore call |split| which results in:

\begin{code}
\g f x -> case  x of
                []    -> <? let v = ...; w = ...; x = [] in v ?>
                z:zs  -> <? let v = ...; w = ...; x = z:zs in v ?>
\end{code}

To represent a value of type |([Var] -> Exp, [Exp])| we use the |<? bullet ?>| notation to identify particular children of an expression. The first component of the resulting pair takes a function name for each child and constructs the target expression. The second component is the list of children. For each child, all the free variables defined in the outer expression become arguments to the lambda, and are passed forward by the target expression.

When optimising the first child expression, where |x = []|, the simplification rules from \S\ref{sec:simplify} immediately produce |[]| as the result. For the other child, we first get:

\begin{code}
\g f z zs ->
    let  v = case  w  of [] -> []; y  :ys  -> g  y  : map g  ys
         w = case  x  of [] -> []; z  :zs  -> f  z  : map f  zs
         x = z:zs
    in   v
\end{code}

Which simplifies to:

\begin{code}
\g f z zs ->  let  v = q : qs
                   q = g y
                   qs = map g ys
                   y = f z
                   ys = map f zs
              in   v
\end{code}

Calling |step| produces |Nothing|, as the root of this expression is a |(:)| that can't be evaluated. We therefore call |split| which results in:

\begin{code}
\g f z zs ->  let  q = <? g (f z) ?>
                   qs = <? map g (map f zs) ?>
                   v = q : qs
              in   v
\end{code}

When optimising |g (f z)| we get no optimisation, as there is no available information. To optimise |map g (map f zs)| we do exactly the same steps as we have already done. However, the |flatten| function will spot that both nodes have the same |pre| expression (modulo free variables), and give them both the same name, creating a recursive function. We then assign names using |assign|. For the purposes of display (not optimisation), we apply a number of simplifications given in \S\ref{sec:postprocess}. The end result is:

\begin{code}
root g f x = case  x of
                   []    -> []
                   z:zs  -> g (f z) : root g f zs
\end{code}

The final version has automatically removed the intermediate list, with no additional knowledge about the |map| function or it's fusion rules.

\subsection{Evaluation}
\label{sec:eval}

\begin{figure}
\begin{code}
force :: Exp -> Maybe Var
force (Case  v _  )  = Just v
force (App   v _  )  = Just v
force _              = Nothing

next :: Exp -> Maybe Var
next (Lam _ x) = next x
next (Let bind v) = last $ Nothing : f v
    where f v = case  lookup v bind of
                      Nothing -> []
                      Just e -> Just v : maybe [] f (force e)
\end{code}
\caption{Function to find the top of the next binding to evaluate.}
\label{fig:stack}
\end{figure}

Evaluation is based around the |step| function. Given an expression, |step| replaces a variable with the expression bound in the environment and returns |Just|, or if no suitable function is found returns |Nothing|. We always replace the variable that would be evaluated next during normal evaluation.

To determine which variable would be evaluated next, we define the |force| and |next| functions in Figure \ref{fig:stack}. The |force| function determine which variable will be evaluated next given an expression -- simply a case scrutinee or an applied variable. The |next| function determines which let bound variable will be evaluated next, by following the forced variables from the bound expressions. Looking at the original example:

\begin{code}
\g f x ->  let  v_1 =  map f x
                v_2 =  map g v_1
           in   v_2
\end{code}

The |next| function will return |Just v_2|. Calling |force| on |map g v_1| returns |map|, but since |map| is not bound at the let expression we go no further. Therefore, to evaluate this expression we will start by evaluating |v_2|, and thus |map|. We insert a fresh variable |v_3| for the body of |map|, and replace the |map| in |v_2| with |v_3|. This transformation results in:

\begin{code}
\g f x ->  let  v_1 =  map f x
                v_2 =  v_3 g v_1
                v_3 =  \f x -> case  x of
                                     []    -> []
                                     y:ys  -> f y : map f ys
           in   v_2
\end{code}

Simplification will immediately reduce the lambda at |v_3|, replacing |v_2| with a case expression on |v_1|.

More generally, we match any expression with the following pattern:

\begin{code}
\free ->  let  s    = f w_1 w_n
               v_1  = e_1
               v_n  = e_n
          in   v
where Just e' = env f
\end{code}

Here we have used |s| to represent the next variable to be evaluated. We allow any other variables |v_1..v_n| bound to expressions |e_1..e_n| to be present. Given this configuration we can rewrite to:

\begin{code}
\free ->  let  s'   = e'
               s    = s' w_1 w_n
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

As always, after generating a new expression we immediately apply the simplification rules (\S\ref{sec:simplify}).

\subsection{Evaluation Splitting}
\label{sec:eval_split}

If evaluation cannot proceed we split to produce a target expression, and a list of child expressions for further optimisation, using the |<? bullet ?>| notation described in \S\ref{sec:manager_example}. When splitting an expression there are three concerns:

\paragraph{Permit further optimisation:} We aim to place any constructs blocking evaluation in the target expression, so that the children are suitable for further optimisation.

\paragraph{No unbounded loss of sharing:} A complex variable may not be duplicated if that causes it to be evaluated multiple times at runtime. The target program cannot remove sharing present in the source program.

\paragraph{Keep expressions together:} If we turn a bound variable in to a free variable we loose optimisation opportunities, as the right-hand side of the variable is no longer available for optimisation. We aim to keep as many expressions together as possible, but not at the cost of loosing sharing.

\smallskip
We split in one of three different ways, depending on the type of the next expression to be evaluated (as described in \S\ref{sec:eval}). We now describe each of the three ways to split, in each case we start with an example, then define the general rule.

\subsubsection{Case Expression}

If the next expression is a case expression then we make the target a similar case expression, and under each alternative we create a child expression with the case scrutinee bound to the appropriate pattern. For example, given:

\begin{code}
\x ->  let   v = case  x of
                       []    -> []
                       y:ys  -> add y ys
       in    v
\end{code}

We produce the residual expression:

\begin{code}
\x ->  case x of
       []    -> <?  let  v =  case x of [] -> []; y:ys -> add y ys
                         x =  []
                    in   x ?>
       y:ys  -> <?  let  v =  case x of [] -> []; y:ys -> add y ys
                         x =  y:ys
                    in   x ?>
\end{code}

Looking more closely at the second alternative, we start with the expression:

\begin{code}
\y ys ->  let  v  = case x of [] -> []; y:ys -> add y ys
               x  = y : ys
          in   v
\end{code}

This expression immediately simplifies to:

\begin{code}
\y ys ->  let  v = add y ys
          in   v
\end{code}

One important point is that for the first alternative we \textit{do not} pass in the variables |y| and |ys| -- these variables are not included in the lambda. In general we do not pass onwards any free variables which are obviously redundant after the simplification rules have been applied. By making this restriction we ensure that the number of free variables in a function cannot grow without bound -- as the expressions are bounded and therefore they can only use a finite number of free variables. We limit the free variables for all types of split operation.

The general rule is that the target is the case on the top of the stack, and the alternatives are the entire expression but with the scrutinee variable bound to the associated patterns. More generally, if |s| is the next expression to evaluate:

\begin{code}
\free ->  let  s    = case x of p_1 -> e_1' ; p_m -> e_m'
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

\noindent becomes:

\begin{code}
\free -> case x of
    p_1  -> <? let  s    = case x of p_1 -> e_1'; p_m -> e_m'
                    v_1  = e_1; v_n = e_n; x = p_1 in v ?>
    p_m  -> <? let  s    = case x of p_1 -> e_1'; p_m -> e_m'
                    v_1  = e_1; v_n = e_n; x = p_m in v ?>
\end{code}

\subsubsection{Lambda}
\label{sec:eval_split_lambda}

If the top of the stack is a lambda then we place a lambda in the target program. The key point when splitting a lambda is that we do not reduce sharing. Consider the following example:

\begin{code}
\x ->  let  s    = \y -> add v_1 y
            v_1  = expensive v_2
            v_2  = f x
       in   s
\end{code}

Here the |add| function takes two arguments, but has only been given one so far. It is tempting to rewrite |\x -> ...| as |\x y -> ...|, but this potentially duplicates the expensive computation of |v_1|. Instead we place every variable binding, and the lambda, in to the target expression:

\begin{code}
\x ->  let  s    = \y -> <? add v_1 y ?>
            v_1  = <? expensive v_2 ?>
            v_2  = <? f x ?>
       in   s
\end{code}

However, we have now split the bindings for |v_1| and |v_2| apart, when there is no real need. We therefore move binding |v_2| under |v_1|, because it is only referred to by |v_1|, to give:

\begin{code}
\x ->  let  s    = \y -> <? add v_1 y ?>
            v_1  = <? let v_2 = f x in expensive v_2 ?>
       in   s
\end{code}

We now continue to optimise the body of |v_1| and the under the lambda of |s_1|, which will now be able to evaluate add. More generally, given:

\begin{code}
\free ->  let  s    = \x -> e'
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

We rewrite:

\begin{code}
\free ->  let  s    = \x -> <? e' ?>
               v_1  = <? e_1 ?>
               v_n  = <? e_n ?>
          in   v
\end{code}

We then repeatedly move any binding |v_i| under |v_j| if either: 1) |v_i| is only used within the body of |v_j|; or 2) the expression bound to |v_i| is cheap. We define an expression to be cheap if it is a constructor, or an application to a variable |v| with fewer arguments than the arity of |v| (a partial application). The intention of moving bindings is to increase sharing, which can be done provided we don't duplicate work (condition 1) or the work duplicated is bounded (condition 2).

\subsubsection{Anything Else}
\label{sec:eval_split_other}

The final rule applies to any expression where the next expression is not a case expression or a lambda, including a constructor, a variable, and an application to an unknown variable. We do not deal with variables from the environment on the top of the stack, as those would be reduced by |step|, and not result in a call to |split|. Given the example:

\begin{code}
\x y ->  let  v_1 = expensive
              v_2 = v_1 x
              v_3 = Con v_2 y v_2
         in   v_3
\end{code}

We simply put all the variables other than the first on the stack inside |<? bullet ?>| brackets:

\begin{code}
\x y ->  let  v_1 = <? expensive ?>
              v_2 = <? v_1 x ?>
              v_3 = Con v_2 y v_2
         in   v_3
\end{code}

We then perform the same sharing transformation as for lambda expressions, noting that |v_1| is only used within |v_2|, to give:

\begin{code}
\x y ->  let  v_2 = <? let v_1 = expensive ; v_2 = v_1 x in v_2 ?>
              v_3 = Con v_2 x v_2
         in   v_3
\end{code}

More generally, given an expression:

\begin{code}
\free ->  let  s   = e'
               v_1 = e_1
               v_n = e_n
          in   v
\end{code}

We rewrite to:

\begin{code}
\free ->  let  s   = e'
               v_1 = <? e_1 ?>
               v_n = <? e_n ?>
          in   v
\end{code}

We then repeatedly move any binding |v_i| under |v_j| according to the criteria given in \S\ref{sec:eval_split_lambda}.

\subsection{Termination}
\label{sec:term}

The termination rule is responsible for ensuring that whenever we proceed along a list of expressions we eventually stop. The intuition is that each expression contains a list of expressions at the root let, and each expression tracks where it came from in the source program. Each successive root let expression must contain either new subexpressions, or fewer subexpressions compared to previous versions.

In this section we first describe the |terminate|, $\lhd$ and $\unlhd$ functions from a mathematical perspective, then how we apply these functions to expressions. Finally we show an example of how the termination rule catches non-termination.

\subsubsection{Termination Rule}
\label{sec:term_rule}

Our termination orderings are defined over bags (also known as multisets) of values drawn from a finite alphabet $\Sigma$. A bag of values is unordered, but may contain elements more than once. We define our two orderings as:

\[
x \lhd y = \set(x) \not\equiv \set(y)  \veeskip \# x < \# y
\]
\[
x \unlhd y = x \equiv y \veeskip x \lhd y
\]

We use $\set(x)$ to transform a bag to a set, and $\#$ as the cardinality operator to take the number of elements in a bag. A sequence $x_1 \ldots x_n$ is well-formed under $\lhd$ if for all indices $i < j \Rightarrow x_j \lhd x_i$ (and respectively for $\unlhd$).

The following sequences are well-formed under both $\unlhd$ and $\lhd$:

\begin{code}
[a,aaaaab,aaabb,b]
[abc,ab,ac,a]
[aaaaabbb,aaab,aab]
\end{code}

The following sequences are well-formed under $\unlhd$, but not under $\lhd$:

\begin{code}
[aaa,aaa]
[aabb,ab,ab]
\end{code}

The following sequences are not well-formed under $\unlhd$ or $\lhd$:

\begin{code}
[abc,abcc]
[aa,aaa]
\end{code}

We define the |terminate| function from Figure \ref{fig:manager2} as:

\begin{code}
terminate  :: (Exp -> Exp -> Bool) -> History -> Exp -> Bool
terminate (<) hist x = not $ all (x <) hist
\end{code}

The |terminate| function checks that given a well-formed sequence (|hist|), adding the expression |x| will keep the sequence well-formed.

\lemma{Any well-formed sequence under $\lhd$ is finite}

Given a finite alphabet $\Sigma$, any well-formed sequence under $\lhd$ is finite. Consider a well-formed sequence $x_1\ldots$. We can partition this sequence into at most $2^\Sigma$ subsequences using set equality. Consider any subsequence $y_1\ldots$. For any two elements in the subsequence, $\set(y_i) \not\equiv \set(y_j)$ will be false, due to the partitioning. Therefore, for the sequence to be well-formed, $i < j \Rightarrow \# y_j < \# y_i$. Therefore there can be at most $\#y_1+1$ elements in any particular subsequence. Combined with a finite number of subsequences, we conclude that any well-formed sequence is finite.

\lemma{Any well-formed sequence under $\unlhd$ has a finite number of distinct elements.}

Given a finite alphabet $\Sigma$, any well-formed sequence under $\unlhd$ has a finite number of distinct elements. For a sequence to be well-formed under $\unlhd$ but not $\lhd$ it must have elements which are duplicates. If we remove all duplicates we end up with a well-formed sequenced under $\lhd$, which must be finite. Therefore there must be a finite number of distinct elements.

\subsubsection{Tracking Names}

Every expression in the source program is assigned a name. A name is a triple, \name{\mathit{f}}{\mathit{e}}{\mathit{a}} where $f$ is a function name, $e$ is an expression index and $a$ is an argument count. We label every expression in the source program with $f$ being the function it comes from, $e$ being a unique index within that function and $a$ being the number of arguments to a constructor or application, and $0$ otherwise.\footnote{It is possible to merge the function name and subexpression index, by making the subexpression index globally unique, but this change would make it harder to understand and debug.} When manipulating expressions, we need to track names:

\begin{itemize}
\item If we rename bound variables, we do not update the expression names.
\item If we extract a subexpression we use the name already assigned to that subexpression.
\item If we insert a new constructor (when splitting on a case) we use the name assigned to the pattern in the associated case alternative.
\item If we add variables to the end of a constructor or application, we change the argument count. For example, |let v = C x; w = v y in ...| being transformed to |let v = C x; w = C x y in ...| would have the name for |C x y| set to the name of |C x| but the constructor count incremented.
\end{itemize}

We map an expression to a bag of names by taking the names of all subexpressions bound at the root let. We never look at the name of a root let expression, and do not update it when making transformations.

\lemma{For any source program, there are a finite number of names}

All subexpressions are assigned expression indices in advance, so there are only a finite number of function name/index values. We only increase the argument count when increasing the number of arguments applied to a constructor or application, which is bounded by the arity of that constructor or the source function. Therefore, there are only a finite number of names.

\lemma{There are a finite number of expressions for any bag}

Given a bag of names, there are only a finite number of expressions that could have generated it. We first assume that when simplifying an expression we always normalise the free variables -- naming the let body |v_1|, and naming all other variables as they are reached from |v_1|. Each name refers to one particular subexpression, but may have different variable names. A finite number of subexpressions can only be combined to produce a finite number of expressions, if we ignore variable names, which the normalisation solves.

\lemma{The termination properties required by \S\ref{sec:manager} are satisfied}

These termination properties required by the manager are satisfied by the previous lemmas. We have shown that the alphabet of names, $\Sigma$, is finite. For |terminate (<=||)| we have shown that there can only be a finite number of distinct name bags, and that each name bag can only correspond to a finite number of original expressions. For |terminate (<||)| we have shown that there can only be a finite number of name bags.

\subsubsection{Termination Splitting}
\label{sec:term_split}

If we are forced to terminate we call |stop|, which splits the expression into several subexpressions, much like |split| (see \S\ref{sec:eval_split}). We require that |split t x| only produces subexpressions which are not forced to terminate by |terminate (<=||) t|. We trivially satisfy this requirement by using the termination criteria when writing |split|.

Given an expression:

\begin{code}
\free ->  let  v_1 = e_1
               v_n = e_n
          in   v
\end{code}

We first split every variable bound at the let, to give:

\begin{code}
\free ->  let  v_1 = <? e_1 ?>
               v_n = <? e_n ?>
          in   v
\end{code}

We now merge variable |v_i| under |v_j| using the same conditions as splitting after evaluation, as described in \S\ref{sec:eval_split_lambda}. In addition, do not merge |v_i| under |v_j| if the resulting expression bound to |v_j| violates the termination criteria |terminate (<=||) t|.

As a heuristic, we merge variable |v| before |w| if the name associated with |v| occurs fewer times in the original let expression. By favouring names used fewer times we hope to encourage splitting off an expression that is growing in number, and will thus continue to grow in recursive calls. This heuristic has no effect on the correctness, but can sometimes result in better optimisation.

We can be sure that the resultant children do not violate the termination criteria, as we never combine expressions in a way that violates the criteria, and all the initial expressions will produce singleton name bags, which trivially satisfy $\unlhd$ for any expression.

\subsubsection{Example}
\label{sec:term_example}

Many simple example programs (such as |map|/|map|) never trigger the termination criteria. The standard example of a function that does require termination is |reverse|, which can be written in a simplified form as:

\begin{code}
main xs = rev [] xs
rev acc xs = case  xs of
                   []    -> acc
                   y:ys  -> rev (y:acc) ys
\end{code}

The |rev| function builds up an accumulator argument, which will be equal to the size of |xs|. If we tried to specialise the accumulator argument then we'd create an infinite number of distinct specialisations. To supercompile this program, the |optimise| function starts with an empty termination context and the expression |rev [] xs|, and calls |reduce|, resulting in:

\begin{code}
\xs -> case  xs of
             []    -> <? [] ?>
             y:ys  -> <? rev (y:acc) ys ?>
\end{code}

Focusing on the second alternative, we now add |rev [] xs| to the termination context, and continue optimising |rev (y:acc) ys|. This leads to the sequence of expressions:

\begin{code}
\x_1 -> rev [] x_1
\x_1 x_2 -> rev (x_1:[]) x_2
\x_1 x_2 x_3 -> rev (x_1:x_2:[]) x_3
...
\end{code}

We can rewrite these expressions in our core language, with annotations for the names:

\newlength{\lenmain}
\newlength{\lenrev}
\settowidth{\lenmain}{|main|}
\settowidth{\lenrev}{|rev|}
\addtolength{\lenmain}{-\lenrev}
\newcommand{\namemain}[1]{\name{main}{#1}{0}}
\newcommand{\namerev}[1]{\name{rev\hspace{\lenmain}}{#1}{0}\hspace{1mm}}

\begin{code}
\x_1 ->
    let  v_1 = {-"\namemain{1}"-}  rev v_2 x_1
         v_2 = {-"\namemain{2}"-}  []
    in   v_1
\x_1 x_2 ->
    let  v_1 = {-"\namerev{1}"-}   rev v_2 x_2
         v_2 = {-"\namerev{2}"-}   x_1:v_3
         v_3 = {-"\namemain{2}"-}  []
    in   v_1
\x_1 x_2 x_3 ->
    let  v_1 = {-"\namerev{1}"-}   rev v_2 x_3
         v_2 = {-"\namerev{2}"-}   x_1:v_3
         v_3 = {-"\namerev{2}"-}   x_2:v_4
         v_4 = {-"\namemain{2}"-}  []
    in   v_1
\end{code}

Applying our termination criteria, the first two expressions are a well-formed sequence, but comparing the second and third expressions we see that $e_2 \unlhd e_3$ is violated. The first item is permitted because the history is empty. The second is permitted as it contains new elements in the set (such as |{-"\name{rev}{1}{0}"-}|). The third item has the same set of names as the second, and has a higher cardinality. Therefore, when optimising, we call |stop| on the third expression. After calling |stop| we get:

\begin{code}
\x_1 x_2 x_3 ->
    let  v_1 = <?  let  v_1  = {-"\namerev{1}"-}   rev v_2 x_3
                        v_2  = {-"\namerev{2}"-}   x_1:v_3
                   in   v_1 ?>
         v_3 = <?  let  v_3  = {-"\namerev{2}"-}   x_2:v_4
                        v_4  = {-"\namemain{2}"-}  []
                   in   v_3 ?>
    in   v_1
\end{code}

Part of the accumulator has been bound to |v_3|, and separated from the main expression. Continuing to optimise we get the sequence:

\begin{code}
\x_1 -> rev [] x_1                          -- |reduce|
\x_1 x_2 -> rev (x_1:[]) x_2                -- |reduce|
\x_1 x_2 x_3 -> rev (x_1:x_2:[]) x_3        -- |stop|
\x_1 x_2 x_3 -> rev (x_1:x_2) x_3           -- |reduce|
\x_1 x_2 x_3 x_4 -> rev (x_1:x_2:x_3) x_4   -- |stop|
\x_1 x_2 x_3 -> rev (x_1:x_2) x_3           -- |reduce|
\x_1 x_2 x_3 x_4 -> rev (x_1:x_2:x_3) x_4   -- |stop|
\x_1 x_2 x_3 -> rev (x_1:x_2) x_3           -- |reduce|
... -- repeat the last 2 lines
\end{code}

As required, we only have a finite number of unique expressions, and will end up with a recursive function in the target program.

\subsection{Post-processing}
\label{sec:postprocess}

Our |split| function is structured to produce only one simple expression per target function -- for example a function will never contain two constructors. While most opportunities to remove intermediate structure have been exploited, the target program will usually contain lots of small functions. We can eliminate many of these functions by inlining all functions which are called only once. For example, given the source program:

\begin{code}
main x = x : x : []
\end{code}

After supercompilation, we get the target program:

\begin{code}
main x = x : f x
f x = x : nil
nil = []
\end{code}

We can then inline all functions will are called only once:

\begin{code}
main x = x : x : []
\end{code}

It is important that the only optimisation intended from this post-processing is the reduction of function call overhead. This use of inlining is substantially different from other compilers \cite{spj:inlining}, where inlining is used to bring expressions together to trigger other optimisations.

\subsection{Alternative Designs}
\label{sec:extensions}

In this section we describe some possible design alternatives for our supercompiler.

\subsubsection{Binary Application}
\label{sec:binaryapp}

Our first version of this supercompiler had binary application, rather than vector application. The |App Var [Var]| construct was replaced by a combination of |Var Var| and |App Var Var|. The reason for originally choosing binary application is that it is closer to other Core languages, and the simplification does not need to track arity information. There were three main reasons for moving to vector application:

\begin{itemize}
\item With binary application there was no way to indicate the number of arguments passed to primitives. In the absence of such information, the only safe alternative is to pass one argument at a time, which resulted in separate split functions.
\item Vector application makes it easier to identify partial applications when increasing sharing (see \S\ref{sec:eval_split_lambda}).
\item Vector application reduces the number of names in an expression, improving the time taken to compile.
\end{itemize}

\subsubsection{Alternative Termination Orderings}

Our original termination ordering was:

\[
x \lhd y = x \setsup y \vee x \bagsub y
\]

Both this ordering and the one described in \S\ref{sec:term_rule} can be proved using the same argument. We choose to use our new ordering because it is simpler, more directly follows from the proof, and can be implemented very efficiently. Choosing a termination ordering is a tricky business -- no termination ordering can be best for every program, so there is always more room for experimentation.

\subsubsection{Common Subexpression Elimination}

Common Subexpression Elimination (CSE) involves spotting where a program would otherwise compute two identical expressions, and reducing them both to a single shared expression. The advantage of CSE is that performance can be increased (sometimes asymptotically), but it can also introduce space leaks \cite{chitil:cse}. Our Core language is well suited to CSE -- two variables can be merged if they have the same associated expression. In addition to the possibility of space leaks, merging two variables may stop the new variable from being placed below another expression when splitting. We have not investigated the performance impact of CSE on supercompilation, but think it is a worthwhile area for future research.

\subsubsection{Inlining Simple Functions}

The GHC compiler inlines many non-recursive functions during the simplification phase \cite{spj:inlining}. It is certainly possible that our simplification rules could be extended to inline some functions, such as |id|, provided no new names were introduced (and thus termination was unaffected). Another alternative would be to inline simple functions in the source program (such as |otherwise| and |(.)|), before supercompilation started. The primary motivation for inlining simple functions would be to reduce the complexity of the main supercompilation phase, and avoid inopportune termination splits. We have deliberately \textit{not} inlined any functions outside the |step| function, as there is a risk that doing so would hide weaknesses in the supercompiler. However, we think it would be worthwhile to pursue.

\todo{give sensible section name}
\section{Running a Supercompiler}
\label{sec:examples}

Supercompilation naturally subsumes many other program transformations, including constructor specialisation \cite{spj:specconstr} and deforestation \cite{gill:shortcut_deforestation,wadler:deforestation}. Supercompilation (in the form presented here) does not remove the need for strictness analysis or say how to compile to native code. In order generate executable programs from a supercompiled program, we compile the resulting Core using GHC \cite{ghc6_12}.

We now give an example where our supercompiler massively outperforms GHC, and discuss what optimisations are being done. Our example is:

\begin{code}
root n = map square (iterate (+1) 1) !! n
    where square x = x * x
\end{code}

Running this program with |n = 400000|, GHC takes 0.149 seconds, while our supercompiler combined with GHC takes 0.011 seconds. Running for larger values of |n| is not feasible as the GHC only variant overflows the stack. After optimising using our supercompiler, then using strictness analysis and unboxing from GHC \cite{spj:unboxing}, our resulting program is:

\begin{code}
go :: Int# -> Int# -> Int#
go x y = case x of
    0 -> y * y
    _ -> go (x - 1) (y + 1)
\end{code}

All the intermediate lists have been removed and all the numbers have been unboxed (GHC uses |Int#| for unboxed integers, and all the arithmetic is performed on unboxed integers). This main loop runs without performing any memory allocation. The program compiled with GHC alone is much less efficient. GHC fuses the |map|/|iterate| function, and specialises the resulting combination on the |square| function, but not the increment -- |(+1)| is passed as a higher order function. Because lists and higher-order functions remain, GHC is not able determine strictness information or apply unboxing.

GHC can specialise functions on data values using constructor specialisation, but cannot specialise on functions. To allow specialisation some functions are written in a particular style:

\begin{code}
foldr f z xs = go xs
    where  go []      = z
           go (y:ys)  = f y (go ys)
\end{code}

Here, provided lambda-lifting has not been performed, the function |foldr| appears to be non-recursive. GHC can inline non-recursive functions, allowing the function |f| to be known. In contrast, our supercompiler has specialised all the functions to their higher-order parameters.

GHC uses rewrite rules to perform eliminate unnecessary lists \cite{spj:rules}. A rewrite rule specifies a pattern to replace with another pattern at compile time. GHC contains a set of rules that replace |map| and |iterate| with alternative versions, and then further rules to fuse them together. GHC does not contain a rule that successfully merges the |(!!)| operator with the rest of the program. Our supercompiler does not rely on rules, which means that changing the data type or redefining |map| locally would have no effect, while in either case GHC would not be able to fuse the lists. Supercompilation can also apply the same techniques to fuse intermediate trees, tuples and any other form of data.

Some of GHC's optimisations, specifically strictness analysis and unboxing, work much better in the absence of lists and higher-order functions. The transformations performed by our supercompiler have allowed these phases to achieve their full potential.

\section{Benchmarks}
\label{sec:benchmarks}

\todo{This section is not finished}

The standard nofib ones, eek!

\subsection{Execution Speed}

See how great we do, yay.

\subsection{Compilation Speed}

Our compiler is quite quick, we've broken in down in to the time to compile and the result. The tricks we use to speed up compilation are variable normalising, using a map for let expressions, and we could also reduce the termination keys in to |Int|, but we haven't bothered yet.

Compared to \cite{me:ifl2007post}, we are much faster. The simplification is easier since we don't descend in to variables. The separate termination histories make it much faster, as they are much smaller. We can write a linear time termination check.

Our compiler is whole program, although we could split it up by defining interface points which are not violated. We haven't bothered to do so yet.

\section{HTML Parsing}
\label{sec:tagsoup}

\todo{This section is not finished}

Tagsoup is used by \cite{malde:using_tagsoup}

The TagSoup library \cite{tagsoup} is a simple parser for XML/HTML, based on the HTML 5 specification. Given a String, TagSoup produces a list of tokens (such as tag open, tag close, attribute). The parser was deliberately written in a way that mirrors the HTML 5 specification, which is based around a state passing approach. Each rule has been modelled in the most direct way, and then a supporting library simplifies it. For example, section 9.2.4.10 of the HTML 5 specification states:

\begin{quote}
9.2.4.10 Attribute value (double-quoted) state

Consume the next input character:

U+0022 QUOTATION MARK (") - Switch to the after attribute value (quoted) state.

U+0026 AMPERSAND (\&) - Switch to the character reference in attribute value state, with the additional allowed character being U+0022 QUOTATION MARK (").

EOF - Parse error. Reconsume the EOF character in the data state.

Anything else - Append the current input character to the current attribute's value. Stay in the attribute value (double-quoted) state.
\end{quote}

And the corresponding code is:

\begin{code}
-- 9.2.4.10 Attribute value (double-quoted) state
attValueDQuoted S{..} = pos $ case hd of
    '\"' -> afterAttValueQuoted tl
    '&' -> charRefAttValue attValueDQuoted (Just '\"') tl
    _ | eof -> errWant "\"" & dat s
    _ -> hd & attValueDQuoted xml tl
\end{code}

Here |tl| is the next state, |hd| is the current character, and the initial |pos| call emits position information. The |(&)| operator is used to place a token on the output stream. However, this high level of abstraction has a noticeable performance penalty, for each output token there are several intermediate values created. While work on list fusion can often reduce intermediate lists, the values here have much more structure than lists, and thus this work is not appropriate. There is a strong desire not to complicate the specification by adding details that improve performance.

The code is split as 308 lines translated from the spec, followed by 191 lines implementing the operations and putting together the results in the right format.

We slightly prime the supercompiler. The optimisation is controlled by 4 booleans, and by freezing them we manage to take fast paths -- for example when generating a stream without position information then |pos| calls are entirely eliminated. We did the same trick with GHC, but it was negligible -- mainly we suspect that the SpecConstr wasn't able to specialise through all the loops, which would have eliminated it. Plus there is no opportunity for fusion.

One might ask whether optimising an HTML parser is worthwhile, but the answer is decidedly yes. The TagSoup library is used for DNA processing, and nightly gets run over 40Gb of XML files. The bottleneck is currently TagSoup, but with these transformations we eliminate that.

Note that our HTML parser is a perfect use case for supercompilation. There is no hot-spot in the program that takes up more time, the problem is that the overhead of the abstraction is throughout. The abstraction is nicely chosen to map to an external document, so the abstraction cannot easily be altered. We are also constrained by speed. Supercompilation delivers nicely, removing all the abstractions to make them valuable at compile time, and yet removed by runtime.

%if 0
\subsection{Equality of Open Expressions}
\label{sec:hlint}

The HLint program \cite{hlint} is a tool for helping improve Haskell source code. A large number of its hints are based on replacing one open expression with another. For example, if the user writes |concat (map f (xs++ys))| it will suggest replacing it with |concatMap f (xs++ys)|. It works by having a list of open expressions it uses for replacement:

\begin{code}
forall f x . concat (map f x) ==> concatMap f x
\end{code}

These rules are written in a supporting file, and there are many of them. Recently one bug was filed stating that one of the rules (involving |foldl| and |map| fusion) was incorrect. After fixing that another incorrect hint was discovered. The intention is clearly that in most cases the left and right expressions are equal. More accurately, the left and right open expressions should be equal. A supercompiler transforms a program and often produces equivalent expressions.

We supercompiled the left and right hand sides. Of the \unknown{} rules, they fit in to three categories:

\paragraph{Type class based on both sides}

If both the left and the right have type classes, i.e. |not (a == b) ==> a /= b|, we can't check anything. This rule is not true in Haskell unless the standard typeclass rules have been followed. This pattern accounts for \unknown{} rules.

\paragraph{Generalisation}

For example |(\(x,y) -> (f x, g y)) ==> f *** g|, while this is somewhat true, it is actually a generalisation. (Note that there is a second condition that f and g do not contain x or y in them). The first expression works on tuples only, while the second is generalise to all arrrows -- of which tuples are one special case. This pattern accounts for 2 rules.

\paragraph{Remaining examples}

These are examples where the equality of both sides is true, and both should be equivalent. Of the remaining ones, we can prove them. Most examples are trivial, but some are slightly more involved (i.e. map fusion rules). For example we were able to prove:

\begin{code}
\end{code}

We can't prove \unknown{} examples. Here are two representative examples:

\paragraph{Generalisation}

\begin{code}
(if a then True else False) ==> a
\end{code}

Here the examples are not equivalent, technically, as the right could be of any type. It would be possible to compress the right hand side in a post processing, but we don't yet do that.

\paragraph{Strictness}

We cannot prove:

\begin{code}
error "Use isPrefixOf" = (take i s == t) ==> ((i == length t) && (t `isPrefixOf` s))
\end{code}

The idea here is that people write |take 4 s == "ICFP"|, when they should have written |"ICFP" `isPrefixOf` s|. Usually the first term |(4 == length "ICFP")| will be eliminated. In addition the rule has a side condition that both |i| and |t| must be concrete literals. The supercompiler produces different expressions for both sides. this is in fact important -- the first expression is lazy in the spine of the |t|, while the right hand side is not. Unless the condition is applied, which the supercompiler can't see.

We currently have no way to tell the supercompiler that an expression is fully evaluated, but perhaps we should and include strictness information throughout.

\paragraph{Finding a bug}

We found a bug. In HLint 1.6.19 there was a rule to reduce |foldr/map| with the rule:

\begin{code}
foldr f z (map g x) ==> foldr (f . g) z x
\end{code}

Unfortunately we also got the equivalent for |foldr1|, namely:

\begin{code}
foldr1 f (map g x) ==> foldr1 (f . g) x
\end{code}

This rule is not true, and our supercompiler spotted it. \footnote{Note that in this case the types are different, which should have given the game away. Unfortunately our type check method simply checks the types can unify (a very cheap check) which doesn't pick this up.}

%endif

\section{Related Work}

We first describe the related work in the area of supercompilation, particularly what makes our supercompiler unique. We then describe some work in related areas, particularly those from which we have used ideas.

\subsection{Supercompilation}
\label{sec:comparison}

Supercompilation was introduced by \citet{supercompilation} for the Refal programming language \cite{refal}. Since this original work, there have been many suggestions of both termination strategies and generalisation/splitting strategies \cite{turchin:generalisation,sorensen:supercompilation,leuschel:homeomorphic}. The original supercompiler maintained both positive and negative knowledge \cite{secher:perfect_supercompilation}, however our implementation uses only positive knowledge (what a variable is, rather than what it cannot be). More recently supercompilation has started to converge towards a common design, described in detail by \citet{klyuchnikov:hosc}, which has much in common with the underlying design present in other papers \cite{me:supero,jonsson:supercompilation}.

Compared to an increasingly common foundation, our supercompiler is radically different. We have changed many of the ingredients of supercompilation (the treatment of let, termination criteria, how the termination histories are used), but have also changed the way these ingredients are combined (the manager). In particular, many of our choices would not work if applied in isolation to another supercompiler -- for example the termination criteria relies on the treatment of let.

\subsubsection{Let Expressions}

The biggest difference from other Core languages is that we require let expressions, and use them extensively (some might say exclusively!). The issue of let expressions in supercompilation has not previously been a primary focus. If lets are included in the Core language, the usual strategy is to substitute all linear lets and residuate all others. Movement of lets can have a dramatic impact on performance: carefully designed let-shifting transformations give an average speedup of 15\% in GHC \cite{spj:letfloating}, suggesting let expressions are critical to the performance of real programs.

Our previous work inlined all let bindings that it could show did not lead to a loss of sharing \cite{me:supero}. Unfortunately, where a let could not be removed, there was a substantial performance penalty. By going to the opposite extreme we are forced to deal with let bindings properly, which makes our new supercompiler both simpler and more robust.

\subsubsection{Termination Criteria}

The standard termination criteria used by supercompilers is the homeomorphic embedding \cite{leuschel:homeomorphic}. The homeomorphic embedding is a well-quasi ordering, from Kruskal's Tree Theorem \cite{kruskal:tree}. The idea requires that for every infinite sequence $e_1,e_2 \ldots$ there exist indicies $i < j$ such that $e_i \ntriangleleft e_j$. The intuition is $x \ntriangleleft y$ holds if by removing nodes from $y$ you cannot reach $x$. Our termination ordering uses the idea idea of a well-quasi ordering, but with a very different ordering relation.

We are unaware of any other supercompilers that have assigned names to expressions, or that have used a bag based termination ordering (most use tree orderings, or sometimes cost models/budgets). Without our particular treatment of expressions as a set of let bindings, and our particular simplification rules, it would not have been possible to use our termination ordering. For example, if we ever inline let expressions then subexpressions would be changed internally, and a single name for each subexpression would no longer be sufficient.

In some cases, our ordering is certainly less restrictive than the homeomorphic embedding. Our example in \S\ref{sec:term_example} would have stopped one step earlier with a homeomorphic embedding. Under a fairly standard interpretation of variable names and let expressions, we can show that our ordering is always less restrictive than the homeomorphic embedding -- although other differences in our treatment of expressions mean such a comparison is not necessarily meaningful. However, we did not choose our termination criteria to permit more expressions -- it was chosen for both simplicity and compile-time performance.

We use two separate termination histories, one in |reduce| and another in |optimise| -- an idea suggested in \citet{ndm:thesis}, but not previously implemented. By separating the termination histories we gain better predictability, as |reduce| is not dependent on which functions have gone before. Additionally, the histories are kept substantially smaller, again improving compile-time performance. By splitting termination checks we also reduce the coupling between the separate aspects of supercompilation, allowing us to present a simpler manager than would otherwise be possible.

As a result of the changes to termination, the operation for splitting after the termination check fails is different. In particular we can use almost identical operations when either evaluation fails to continue, or the termination check fails.

\subsection{Partial evaluation}

There has been a lot of work on partial evaluation \cite{jones:partial_evaluation}, where a program is specialised with respect to some static data. Partial evaluation works by marking all variable bindings within a program as either static or dynamic, using binding time analysis, then specialises the program with respect to the static bindings. Partial evaluation is particularly appropriate for optimising an interpreter to the expression tree of a particular program, automatically generating a compiler, and removing \textit{interpretation overhead}. The translation of an interpreter into a compiler is known as the First Futamura Projection \cite{futanama:projections}, and can often give an order of magnitude speedup.

Supercompilation and partial evaluation both remove abstraction overhead within a program. Partial evaluation is more suited to completely removing static data, such as an expression tree which is interpreted. Supercompilation is able to remove intermediate data structures, similar to deforestation, which partial evaluation cannot.

\subsection{Deforestation}

The deforestation technique \cite{wadler:deforestation} removes intermediate lists in computations. This technique has been extended in many ways to encompass higher order deforestation \cite{marlow:higher_order_deforestation} and work on other data types \cite{coutts:string_fusion}. In many cases the gains from supercompilation are just particular forms of deforestation.

Probably the most practically applied work on deforestation involves using GHC's rewrite rules to optimise programs \cite{spj:rules}. Shortcut deforestation rewrites many definitions in terms of |foldr| and |build|, then combines |foldr|/|build| pairs \cite{gill:shortcut_deforestation}. Stream fusion is similar to shortcut deforestation, but relies on |stream|/|unstream| rules \cite{coutts:stream_fusion}. In both cases the optimisation only applies to lists, and only to functions written in terms of the correct primitives. The advantage of supercompilation is that it applies to many types and functions, without any special effort from the program author.

\subsection{Lower Level Optimisations}

Our optimisation works at the Core level, but even once efficient Core has been generated there is still some work before efficient machine code can be produced. Key optimisations include strictness analysis and unboxing \cite{spj:unboxing}. In GHC both of these optimisations are done at the core level, using a core language extended with unboxed types. After this lower level core has been generated, it is then compiled to STG machine instructions \cite{spj:stg}, from which assembly code is generated. There is still work being done to modify the lowest levels to take advantage of the current generation of microprocessors \cite{marlow:pointer_tagging}. We rely on GHC to perform all these optimisations after our supercompiler generates a residual program.

The GRIN approach \cite{grin} uses whole program compilation for Haskell. It is currently being implemented in the jhc compiler \cite{jhc}, with promising initial results. GRIN works by first removing all functional values, turning them into case expressions, allowing subsequent optimisations. The intermediate language for jhc is at a much lower level than our Core language, so jhc is able to perform detailed optimisations that we are unable to express.

\section{Conclusions and Future Work}

\todo{this section is not finished}

Need more benchmarks, integrate into a production quality compiler (GHC). Our supercompiler has been designed to run faster, but does it really do so?

\subsection{Compile Time Performance}

We can implement this construct much more efficiently, in particular significantly more efficiently than a homeomorphic embedding. We can reify this sequence as |Map Name Int|.

We can change names to be a function name and subexpression index combined. We can also free up the last 6 bits to store the constructor count, allowing over 60 million subexpressions even with 32 bit integers. We now have very efficient names.

We can pre-resolve in to a decision table or finite state machine very easily. However, it's not really necessary - the basic test is dead fast, if you use ordered lists.

We currently keep a list of history built using |(:)|, but that's very inefficient. If instead we have a termination context that can manipulate expressions and store more structure it can go faster.

One intruiging possibility is that our supercompiler as described may actually be a great target for partial evaluation. The program being supercompiled is static, and in our supercompiler is not perturbed to the extent of other supercompilers.

It never rewrites expressions, allowing for the tantilising opportunity to partially evaluate the supercompiler.


\subsection{Run Time Performance}

What about merging a rules engine.

What about strictness analysis.

\subsection{Conclusions}

We have presented a supercompiler which is simple, and has found practical use. What a great result. By striving for simplicity we think we've got something better than previously.


\subsection*{Acknowledgements}

I would like to thank lots of people for helpful ideas and discussions. Particularly I'd like to thank Max Bolingbroke, Jason Reich, Simon Peyton Jones and Peter Jonsson. Thanks to Ketil Malde for providing information for the HTML parsing work.

\bibliographystyle{plainnat}
\bibliography

\end{document}
