\documentclass{llncs}

\usepackage{url}
\usepackage{bar}

%include polycode.fmt
%include supero.fmt

\newenvironment{fig}
    {\begin{figure}[tbp]\hrule}
    {\end{figure}}

\newcommand{\figend}{\hrule}
\newcommand{\perc}{\%
    }

\begin{document}

\title{Supero: Making Haskell Faster}

\author{Neil Mitchell and Colin Runciman}

\institute{University of York, UK, \url{http://www.cs.york.ac.uk/~ndm}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Haskell is a high level functional language, with features like higher order functions and lazy evaluation, which allow succinct representations of programs. These high level features can result in poor runtime speed of generated code, if care is not taken. This paper presents a whole program approach to transformation, which enables substantial speed improvements relative to the GHC, a mature optimising compiler.
\end{abstract}

\section{Introduction}

Haskell \cite{haskell} can be used to write high level specifications of programs, which can run. Take for example the task of counting the number of words in a file read from the input. In Haskell this program is a single line:

\begin{code}
main = print . length . words =<< getContents
\end{code}

The |getContents| function reads the input as a list of characters, |words| splits this list into a list of words, |length| counts the number of words, and finally |print| writes the value to the screen. The use of |(=<<)| is merely to do sequence the actions, and can be ignored for those unfamiliar with Haskell and monadic IO.

Compared to a C program to perform this task (see Figure \ref{fig:c_words}), the Haskell version is more concise, more specification orientated and easier to convince others of its correctness. Unfortunately with all these good points,  it is also three times slower than the C version.

\begin{fig}
\bigskip
\begin{verbatim}
int main()
{
	int i = 0;
	int c, last_space = 1, this_space;
	while ((c = getchar()) != EOF) {
		this_space = isspace(c);
		if (last_space && !this_space)
			i++;
		last_space = this_space;
	}
	printf("%i\n", i);
	return 0;
}
\end{verbatim}
\figend
\caption{Word counting in C}
\label{fig:c_words}
\end{fig}

There are several reasons that the Haskell version does not perform at the same speed as the C version:

\begin{description}
\item[Intermediate Lists] One of the reasons for the slowdown in the Haskell version is that many intermediate lists are produced and consumed as the program proceeds. The |getContents| function produces a list of characters, |words| consumes this list and produces a list of a list of characters, |length| then consumes the outermost list. The equivalent C program has no allocation.
\item[Higher Order Arguments] The |words| function is defined using the function |span|, which takes a predicate and splits the input list when the predicate becomes true. The predicate is passed as a higher order function, which is constant on all applications.
\item[Laziness] The Haskell program proceeds in a lazy manner, demanding one character from |getContents|, then processing it with each of the functions in the pipeline. At each stage, a lazy thunk for the remainder of the function is created.
\end{description}

By using the optimiser developed in this paper we can eliminate all these overheads, resulting in a program that performs \textit{faster} than the equivalent C version. The central idea of the optimiser is that we attempt to evaluate as much of the program at compile time as possible, leaving a residual program consisting of only actions dependent on the input data.

Our goal is to make high-level Haskell programs perform as fast as low-level Haskell programs, eliminating the need to optimise Haskell programs in the kind of detail currently required for high performance programs. We require no annotations on any part of the program, even library functions, in contrast to other approaches.

\subsection{Roadmap}

Insert here

\section{Core Language}

\begin{fig}
\begin{code}
expr  =  v                                          {-" \text{  variable} "-}
      |  c                                          {-" \text{  constructor} "-}
      |  f                                          {-" \text{  function} "-}
      |  x y                                        {-" \text{  application} "-}
      |  \v -> x                                    {-" \text{  lambda abstraction} "-}
      |  let v = x in y                             {-" \text{  let binding} "-}
      |  case x of {p_1 -> y_1 ; ... ; p_n -> y_n}  {-" \text{  case expression} "-}

pat   =  c vs_
\end{code}

Where |v| ranges over variables, |C| ranges over constructors, |f| ranges over functions, |x|, |y| and |z| range over expressions and |p| ranges over patterns.
\bigskip
\figend
\caption{Core syntax}
\label{fig:core}
\end{fig}

All our optimisations operate on a standard Core language, documented in \cite{me:yhc_core}. The expression data type is given in Figure \ref{fig:core}. A program is a mapping of function names to expressions. Our Core language is higher order and lazy, but lacks much of the syntactic sugar found in Haskell. Pattern matching occurs only in case expressions, and all case expressions are exhaustive. All names are fully qualified. Haskell's type classes have been removed by the dictionary transformation \cite{wadler:type_classes}.

The Yhc compiler, a fork of nhc \cite{nhc}, can output Core files using a single flag. Yhc can also link in all definitions from all required libraries, and can produce a single Core file representing the entire program.

The primary difference between our Core language and that of the GHC compiler \cite{ghc_core} is that ours is untyped. The Core is generated from well-typed Haskell, and is guaranteed not to fail with a type error. All the transformations we implement could be implemented equally well in a typed Core language. We prefer to work in an untyped Core language for simplicity of implementation.

In order to avoid accidental variable name clashes while performing transformations, we demand that all variables within a program are unique. All transformations may assume this invariant, and must ensure it as a postcondition.

\section{Optimisation}

Our optimisation procedure takes a Core program as input, and produces a new Core program, that has the same effect as the original -- hopefully with improved performance. We do not make small changes to the original program, but instead evaluate some of the program at compile time, leaving a residual program to be run.

Each function in the output program is associated with an expression in the input program. The expression in the output program is an optimised version of the expression in the input program. Our optimisation proceeds by starting at the |main| function, and optimising the expression associated with |main|. Once the expression has been optimised, the outermost element in the expression becomes part of the residual program. All the inner expressions are assigned names, which become functions in the residual program. Each of these new functions are then optimised as before.

\begin{fig}
\begin{code}
_O\<case x of alts_  \> =  case _O\<x\> \? of alts_
_O\<let v = x in y   \> =  let v = _O\<x\> \? in _O\<y\>
_O\<x y              \> =  _O\<x\> \? y
_O\<function         \> =  _O\<unfold f\>
                           where f {-" \hbox{ is a non-primitive, non-CAF function} "-}
_O\<x                \> =  x
\end{code}
\figend
\caption{Optimisation function, |_O|}
\label{fig:optimise}
\end{fig}

\begin{fig}
\begin{code}
case (case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) of alts_
    => case x of  {  p_1  -> case y_1 of alts_
                  ;  ...
                  ;  p_n  -> case y_n of alts_ }

case c xs_ of {... ; c vs_ -> y ; ...}
    => y[vs_/xs_]

case v of {... ; c vs_ -> x ; ...}
    => case v of {... ; c vs_ -> x[v/c vs_]; ...}

case (let v = x in y) of alts_
    => let v = x in case y of alts_

(let v = x in y) xs_
    => let v = x in y xs_

(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) xs_
    => case x of {p_1 -> y_1 xs_ ; ... ; p_n -> y_n xs_}

(\v -> x) y
    => let v = y in x

let v = x in (case y of {p_1 -> y_1 ; ... ; p_n -> y_n})
    => case y of  {  p_1  -> let v = x in y_1
                  ;  ...
                  ;  p_n  -> let v = x in y_n}
    where v {-" \hbox{is not used in} "-} y

let v = x in y
    => y[v/x]
    where x {-" \hbox{is a lambda, variable, or used once in } "-} y

let v = c x_1 ... x_n in y
    =>  let v_1 = x_1 in
        ...
        let v_n = x_n in
        y[v/c x_1 ... x_n]
    where v_1 ... v_n {-" \hbox{ are fresh} "-}
\end{code}
\figend
\caption{Simplification rules}
\label{fig:simplify}
\end{fig}

The optimisation of an expression is performed using the |_O| function in Figure \ref{fig:evaluate}, and the simplification rules in Figure \ref{fig:simplify}. We define |_OO| to be the fixed point of applying both |_O| and the simplification rules. The |_O| function proceeds much like evaluation at run time, but stops if the next expression to reduce is a free variable, a constructor, a primitive, or a CAF (constant applicative form -- see \S\ref{sec:caf} for more details). The one difference is that in a |let| expression the bound variable and the inner expression are \textit{both} optimised -- see \S\ref{sec:let}.

Let us take a simple example:

\begin{code}
main = \xs -> map inc xs

map = \f -> \xs -> case  xs of
                         []    -> []
                         y:ys  -> f y : map f ys

inc = \x -> x+1
\end{code}

This program defines a |main| function which increments each value in the list by one. Note that |f| is passed around at runtime, when it could be frozen in at compile time. By following the optimisation procedure we end up with:

\begin{code}
main = \xs -> case  xs of
                    []    -> []
                    y:ys  -> f0 y ys

f0 = \y -> \ys -> (y+1) : main ys
\end{code}

And finally by performing some trivial inlining we can obtain:

\begin{code}
main = \xs -> case  xs of
                    []    -> []
                    y:ys  -> (y+1) : main ys
\end{code}

The residual program is now optimised -- there is no runtime passing of the |inc| function, only direct a arithmetic operation.

Next we show another example, which has an instance of list deforestation \cite{wadler:deforestation}. Taking the program:

\begin{code}
main xs = map (+1) (map (*2) xs)

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

This is transformed (after trivial inlining) into:

\begin{code}
main xs = case  xs of
               []    -> []
               y:ys  -> (y*2)+1 : main ys
\end{code}

Here the intermediate list has been removed, and the higher order functions have been specialised.


\subsection{Termination}
\label{sec:termination}

The main problem with the method as presented so far is that it may not terminate. There are several ways to that non-termination can arise -- we consider, and eliminate, each one in turn.

\subsubsection{Infinite Unfolding}

Consider the definition:

\begin{code}
name = \x -> name x
\end{code}

This program would not terminate at runtime, but in a lazy language it is entirely possible that the resultant value would never be demanded, and the program would terminate. Also, while it is unlikely someone would write such a program, it is possible to arise by other transformations.

If the expression |name x| was being optimised then the optimisation function |_OO| would not terminate. We can solve this problem by either bounding the number of unfoldings, or by keeping a list of previous values in the optimisation of an expression. In practice, this situation is rare, and either choice is acceptable. We choose to bound the number of unfoldings to an excessively large number -- resulting in a slight slow down at compilation time of an infinite unfolding, but not impacting either runtime or memory consumption in the common case.

\subsubsection{Accumulating parameters}

Consider the definition:

\begin{code}
reverseAcc = \xs -> \ys -> case  xs of
                                 []    -> []
                                 z:zs  -> reverseAcc zs (z:ys)
\end{code}

This function is the standard |reverse| function, with an accumulator. The problem is that successive iterations of the optimisation produce progressively larger subexpressions. Initially a function will be created for |reverseAcc _ _|, then for |reverseAcc _ (_:_)|, then |reverseAcc _ (_:_:_)|. If left unchecked there would be an infinite number of functions created in the residual program.

The solution is to bound the size of the input expression associated with each function. The size of the expression being optimised can be reduced by lifting some aspect of the expression into a let binding, then placing this let binding in the residual program. By bounding the size of the expression, we bound the number of functions in the residual program.

If the bound is too high, optimisation will take programs will take too long, the residual program will be excessively large and some functions will have too many arguments. If the bound is too low then we will have limited optimisation. We will come back to the issue of the size of this bound in the results sections \ref{sec:haskell_results}.

\subsubsection{Direct Repetition}

While the two changes presented above do guarantee termination, it is often useful to detect an expression which appears to be repeating, and preemptively bound it. Consider the |reverseAccumulator| example -- the recursive pattern is an instance of \textit{direct repetition}. An expression |x| is directly repeating if |x ~= alpha (alpha beta)| where |beta| is an expression, |alpha| is a non-identity function from an expression to an expression and |~=| is equality where all variables are considered equal.

Some examples of expressions which have direct repetition:

\begin{code}
x:y:xs
f (f x)
case x_1 of {[] -> nil ; y:ys -> case x_2 of {[] -> nil ; z:zs -> cons}}
\end{code}

If direct repetition is encountered, then the repeating expression is lifted to a top-level let binding, and output directly into the residual program.

\subsection{Let Bindings}
\label{sec:let}

The rule for let bindings in Figure \ref{fig:optimise} is curious. While all other rules simply follow evaluation order, the let rule optimises \textit{both} the bound expression and the inner expression. This is a critical choice, which enhances the optimisation performed by the system.

In the Core language a let expression introduces a binding, which is shared. Given the expression |let v = x in y|, if |v| is referred to multiple times in |y|, then the expression |x| will only be computed once. It is important when applying |_OO| that shared let bindings are not unshared. Consider the following program:

\begin{code}
let v = expensive
in v + v
\end{code}

If the expression |v| was unshared, then |expensive| would be computed twice, slowing down the runtime. Yet, by inlining certain let expressions, better optimisation can be achieved. Taking the following fragment from a previous example:

\begin{code}
let f = inc
in f y : map f ys
\end{code}

If |f| is not inlined, then the recursive call to |map| would still contain a higher-order parameter. But at the same time, it is not easy to tell whether |inc| is expensive or not. The solution is to first optimise |inc|, leading to:

\begin{code}
let f = \x -> x + 1
in f y : map f ys
\end{code}

After this optimisation, it is now clear that |f| is a lambda, so by inlining it there will be no additional computation.

\subsection{CAF's}
\label{sec:caf}

A CAF (constant applicative form) is a top level function of zero arguments. In Haskell, these CAF values are computed at most once per program run, and retained. Consider the program:

\begin{code}
caf = expensive

main = caf + caf
\end{code}

In this program |caf| would only be computed once. If a CAF function is inlined then this may result in a computation being performed more than would otherwise occur. To ensure that we do not duplicate computations, we never inline CAF's.

\section{Benchmarks Versus C}

The most comprehensive inter-language benchmarking effort is the Programming Language Shootout \cite{shootout}. In the shootout a variety of programming tasks are implemented in many languages and benchmarked against each other. Unfortunately, in order to ensure a balanced comparison, many restrictions are placed on the programs -- some of which severely hamper lazy languages. Inspired by their attempt, we have attempted to define some simpler benchmarks on which it is possible to directly compare Haskell to C.

The set of benchmarks we have chosen are inspired by the Unix \texttt{wc} command -- namely character, word and line counting. In all cases we require the program to read from the standard input, and write out the number of elements in the file. In order to ensure that we test computation speed, not IO speed (which is usually a product of buffering strategy, rather than optimisation prowess) we demand that all input is read using the standard C |getchar| function only. Any buffering improvements, such as reading in blocks or memory mapping of files, could be performed equally in all compilers.

All the C versions are implemented following a similar pattern to Figure \ref{fig:c_words}. Characters are read in a loop, with an accumulator recording the current value. Depending on the program, the body of the loop decides when to increment the accumulator. The Haskell versions all follow the same pattern as in the Introduction, merely replacing |words| with |lines|, or removing the |words| function for character counting.

We performed all benchmarks on a Windows XP, 3GHz processor, 1Gb RAM machine. All benchmarks were run over a 50Mb log file, repeated 10 times, and the lowest value was taken. The C versions used GCC \cite{gcc} version 3.4.2 with -O3. The Haskell version used GHC \cite{ghc} 6.6.1 with -O2. The Supero version was compiled using our optimiser, then written back as a Haskell file, and compiled once more with GHC 6.6.1 and -O2.

\begin{fig}
\vspace{7mm}
\makebox[33mm][r]{
    \begin{barenv}
    \legend{1}{C$\;\;\;\;\;\;$}
    \legend{2}{Supero$\;\;\;\;\;\;$}
    \legend{3}{GHC$\;\;\;\;\;\;$}
    \end{barenv}
}
\begin{barenv}
\setwidth{20}
\setdepth{0}
\sethspace{0.05}
\setstretch{6}
\setnumberpos{empty}
% \setxaxis{0}{1}{1}
% \setxname{$n$}
\setyaxis{0}{25}{5} \setyname{Seconds}
\bar{6}{1}
\bar{6.3}{2}[characters]
\bar{12.5}{3}
\bar{0}{0}
\bar{6.2}{1}
\bar{6.5}{2}[lines]
\bar{17}{3}
\bar{0}{0}
\bar{7.3}{1}
\bar{6.9}{2}[words]
\bar{22}{3}
\end{barenv}
\vspace{7mm}
\figend
\caption{Benchmarks with C, Supero and GHC}
\label{fig:c_results}
\end{fig}

The results are given in Figure \ref{fig:c_results}. In all the benchmarks C and Supero are within 10\% of each other, while GHC trails further behind.

\subsection{Identified Haskell Speedups}

As part of our benchmarks, we have identified two bottlenecks in the Haskell version of word counting -- both of which are fixed in our benchmark results.

\paragraph{Slow \textsf{isSpace} function}

The first issue is that |isSpace| in Haskell is 79\perc more expensive than |isspace| in C. The simplest solution is to use a FFI (Foreign Function Interface) \cite{haskell_ffi} call to the C |isspace| function in all cases, removing this factor from the benchmark. A GHC bug (number 1473) has been filed about the slow performance of |isSpace|.

\begin{fig}
\begin{code}
words :: String -> [String]
words s = case  dropWhile isSpace s of
                ""  ->  []
                x   ->  w : words y
                        where (w, y) = break isSpace x

words' s = case  dropWhile isSpace s of
                 []    ->  []
                 x:xs  ->  (x:w) : drop1 z
                           where (w, z) = break isSpace xs

drop1 []      = []
drop1 (x:xs)  = words xs
\end{code}
\figend
\caption{The |words| function from the Haskell standard libraries, and an improved |words'|.}
\label{fig:words}
\end{fig}

\paragraph{Inefficient \textsf{words} function}

The second issue is that the |words| function in Haskell performs two additional |isspace| tests per word. The code for |words| is given in Figure \ref{fig:words}. By appealing to the definitions of |dropWhile| and |break| it is possible to show that in |words| the first character of |x| is not a space, and that if |y| is non-empty then the first character is a space. The revised |words'| function takes advantage of this knowledge to reduce the number of |isSpace| tests.

\subsection{Potential Supero Speedups}

We have identified three reasons why the code generated by Supero underperforms relative to the C version in some benchmarks. All of these stem from using GHC as a backend, and cannot be solved at the level of Core transformations. We suspect that by fixing these problems, the Supero execution time would improve by between 5\% and 15\%.

\paragraph{Strictness inference}

The GHC compiler is overly conservative when determining strictness for functions which use the FFI (GHC bug 1592). The |getchar| is treated as though it may raise an exception, and terminate the program, so otherwise strict arguments become potentially unevaluated. If GHC permitted some way to mark an FFI function as not generating exceptions, then this problem could be solved. The lacking of strictness information means that in the line and word counting programs, every time the accumulator is incremented, the number is first unboxed and then reboxed \cite{spj:unboxing}.

\paragraph{Heap checks}

The GHC compiler follows the standard STG machine \cite{spj:implementation} design, and inserts heap checks before allocating memory. The purpose of a heap check is to ensure that there is sufficient memory on the heap, so that allocation of memory is a cheap operation which is guaranteed to succeed. GHC also attempts to lift heap checks, so that if two branches of a case expression both have heap checks, then they are replaced with one shared heap check before the case expression. Unfortunately, in lifting the heap checks, a tail-recursive function which allocates memory only upon exit can have the heap test executed on every iteration (GHC bug 1498). This particular problem effects the character counting example, but if the strictness problems were solved, it would apply equally to all the benchmarks.

\paragraph{Stack checks}

The final source of extra computation relative to the C version are stack checks. Before using the stack to store arguments to a function call, a test is performed to check that there is sufficient space on the stack. Unlike the heap checks, it is necessary to analyse a large part of the flow of control to determine that these checks are unnecessary. It is not immediately clear how to eliminate the stack checks within the current framework of GHC.

\subsection{The Wordcount Benchmark: Supero outperforms C}

The most curious result is that Supero outperforms C on wordcounting, by about 6\% -- even with the problems discussed! The C program presented in Figure \ref{fig:c_words} is not maximally efficient. The variable \verb"last_space" is a boolean, indicating whether the previous character was a space, or not. Each time round the loop a test is performed on \verb"last_space", even though its value was determined and tested previously. The way to optimise this code is to have two specialised variants of the loop, one for when \verb"last_space" is true, and one for where it is false. When the value of \verb"last_space" changed, the program would transition to the other loop. This pattern effectively encodes the boolean variable in the program counter, and is what the Haskell program has managed to generate from the high-level code.

Unfortunately, in C it is quite challenging to capture the required flow control! The program needs two loops, where both loops can transition to the other. Using the |goto| keyword is not appropriate, as typically this turns off many critical optimisations in the C compiler. Tail recursion is certainly not appropriate, as it is not required by the C standard (nor supported in most compilers). The only way to express the necessary pattern is using nested while loops, but unlike newer imperative languages such as Java, C does not have named loops -- so the inner loop cannot break from the outer loop if it reaches the end of the file. The only solution is to place the nested while loops in a function, and use |return| to break from the inner loop. This solution would not scale to a three-valued control structure, and substantially increases the complexity of the code.

\section{Benchmarks Versus Haskell}

The standard set of Haskell benchmarks is the nofib suite \cite{nofib}. The nofib suite is divided into three categories of increasing size: imaginary, spectral and real. Many small Haskell programs increase in size substantially once the libraries are included, particularly when type classes are involved. Because of the relatively large size of even small examples, we have limited our focus to five benchmarks drawn from the imaginary section. We have deliberately chosen programs which do not perform large amounts of IO, but instead are compute bound programs.
The benchmarks are: digits-of-e1 and digits-of-e2, both of which compute the digits of |e|, by different methods; exp3\_8 performs |3^8| using peano numbers and the |Num| class; primes computes a list of prime numbers; and queens counts the possible safe layouts of queen pieces on a chess board. All benchmarks were run with parameters to ensure runtimes of between 3 and 5 seconds for GHC.

\begin{fig}
\vspace{7mm}
\begin{barenv}
\setwidth{20}
\setdepth{0}
\sethspace{0.5}
\setstretch{1}
\setnumberpos{empty}
% \setxaxis{0}{1}{1}
% \setxname{$n$}
\setyaxis{0}{100}{10} \setyname{Time relative to GHC}
\bar{0}{0}
\bar{90}{1}[digits-of-e1]
\bar{0}{0}
\bar{75}{1}[digits-of-e2]
\bar{0}{0}
\bar{35}{1}[exp3\_8]
\bar{0}{0}
\bar{87}{1}[primes]
\bar{0}{0}
\bar{78}{1}[queens]
\bar{0}{0}
\end{barenv}
\vspace{3mm}
\figend
\caption{Runtime, relative to GHC being 100.}
\label{fig:haskell_results}
\end{fig}

\begin{table}[tb]
\hrule
\vspace{3mm}

\begin{tabular}{lrrrrr}
\textbf{Program} & \hspace{5mm}\textbf{Source} & \hspace{5mm}\textbf{Residual} & \hspace{5mm}\textbf{Bound} & \hspace{5mm}\textbf{Size} & \hspace{5mm}\textbf{Time} \\
digits-of-e1 &  521 & 1676 & 13 & 110 & 90 \\
digits-of-e2 & 1235 &  515 & 12 &  99 & 75 \\
exp3\_8      &  380 & 1138 &  5 & 104 & 35 \\
primes       &  422 &  356 & 12 & 101 & 87 \\
queens       &  637 & 4265 &  8 & 116 & 78 \\
\end{tabular}
\vspace{2mm}

\textbf{Program} is the name of the program; \textbf{Source} is the number of lines of pretty printed source code including all libraries; \textbf{Residual} is the number of lines after optimisation; \textbf{Bound} is the termination bound used; \textbf{Size} is the size of the resultant binary relative to the GHC produced binary being 100; \textbf{Time} is the runtime relative to GHC being 100.

\vspace{4mm}
\hrule
\vspace{2mm}
\caption{Result on the nofib suite.}
\label{tab:haskell_results}
\end{table}

The results of these benchmarks are given in Figure \ref{fig:haskell_results}, along with detailed breakdowns in Table \ref{tab:haskell_results}. In all benchmarks Supero performs at least 10\% faster than GHC, and in one case is nearly three times faster. The resultant executable binaries were at most 10\% larger than those from GHC alone, and in one case was even marginally smaller.

\subsection{GHC's optimisations}

For these benchmarks it is important to clarify which optimisations are being performed by GHC, and which are performed by Supero. If you were to simply take the Core output from Yhc Core and compile it using GHC Core, you would not get the same performance results as GHC compiled code. There are two particular places where GHC has special optimisations that work in a range of special cases, but which Supero is unable to take advantage of.

\paragraph{Dictionary Removal} Functions which make use of type classes are given an additional dictionary argument, which is passed to all functions. In practice, GHC specialises many such functions by creating code with a particular dictionary frozen in. This optimisation is specific to type classes -- a tuple of higher order functions will not be similarly specialised. After compilation with Yhc, the type classes have already been converted to tuples, so unless Supero is able to remove the dictionaries it will loose to GHC. One particular benchmark where dictionary removal is critical is digits-of-e2.

\paragraph{List Fusion} GHC relies on names of functions, particularly |foldr|/|build| \cite{spj:rules}, to apply special optimisation rules such as list fusion. Many of GHC's library functions, for example |iterate|, are defined in terms of |foldr| to take advantage of these special properties. After transformation with Yhc, these names are destroyed, and so no rule based optimisation will be performed. One example where list fusion is critical is primes, although it occurs in most of the benchmarks to some extent.

Supero has no special purpose optimisations which rely on named functions or desugaring knowledge. The one place where no GHC specific optimisations apply is the exp3\_8 benchmark, which operates solely on peano numbers -- a type GHC has no inbuilt knowledge of. This property explains the performance advantages shown by Supero in exp3\_8, while GHC was limited to basic inline/simplify transformations, Supero managed to perform some limited fusion.

\subsection{Termination Bound}

Table \ref{tab:haskell_results} includes a column indicating the size bound that was applied to expressions. Out of the five benchmarks, both primes and queens could be run at any greater bound and would still produce the same program -- the direct repetition criteria bounds the expressions on its own. For the remaining programs, a bound was chosen which to ensure that the compilation process was quick (under two seconds). By increasing the termination bound the size of the generated program would increase, but potentially the generated program would execute faster.

The existence of a termination bound with the present degree of sensitivity is a cause for concern. In a large program it is likely that different parts of the program would require different bounds on the size of the generated expression -- something not currently possible. We suspect that the most promising direction is to augment the direct repetition criteria, and hope to obtain termination in all practical cases without resorting to a depth bound.

\section{Related Work}

\paragraph{Partial evaluation} There has been a lot of work on partial evaluation \cite{partial_evaluation}, where a program is specialised with respect to some static data. Typically the emphasis is on determining which variable can be entirely computed at compile time, and which must remain in the residual program. Partial evaluation is particularly appropriate for specialising an interpreter with a partial program to generate a compiler automatically, often with an order of magnitude speedup, known as the First Futanama Projection \cite{futanama:projections}. The difference between our work that partial evaluation is that we do fold back definitions, and perform no binding time analysis. Our method is certainly less appropriate for specialising an interpreter, but in the absence of static data, is still able to show improvements.

\paragraph{Deforestation} The deforestation technique \cite{wadler:deforestation} removes intermediate lists in computations. This technique has been extended in many ways to encompass higher order deforestation \cite{higher_order_deforestation} and work on non-list types \cite{non_list_deforestation}. Probably the most practically motivated work on deforestation has come from those attempting to restrict deforestation, in particular shortcut deforestation \cite{short_cut_deforestation}, and newer approaches such as stream fusion \cite{stream_fusion}. In this work certain named functions are automatically fused together. By rewriting library functions in terms of these special functions, fusion occurs. Fusion is limited to cases where the correct underlying function is used -- sometimes this requires unnatural definitions.

\paragraph{GRIN} The GRIN approach \cite{grin} is currently being implemented in the JHC compiler \cite{JHC}, with promising initial results. GRIN works by first translating to a monadic intermediate language, then repeatedly performing a series of optimisations, using whole program transformation. The intermediate language is at a much lower level than our Core language, so it is able to express detailed optimisations that we are unable to.

\paragraph{Other Transformations} Our work makes use of, or subsumes many other transformations in functional programming. One of the central operations within our optimisation in inlining, a technique that has been used extensively within GHC \cite{spj:inlining}. We generalise the constructor specialisation technique \cite{spj:specconstr}, by allowing specialisation on any arbitrary expression, including constructors. One optimisation we do not currently support is the use of user provided transformation rules \cite{spj:rules}, which can be used to automatically replace certain expressions with others -- for example |sort . nub| removes duplicates then sorts a list, but can be done asymptotically faster in a single operation.

\paragraph{Lower Level Optimisations} Our optimisation works at the Core level, but once optimal Core has been generated there is still some work before optimal machine code has been produced. One critical pair of optimisations are strictness analysis \cite{strictness} and unboxing \cite{spj:unboxing} -- detecting that a value is always used strictly, then removing the indirection that laziness requires. In GHC both of these optimisations are done at the Core level, using a Core language extended with the unboxed kind. After this lower level Core has been generated, it is then transformed in to STG machine instructions \cite{spj:stg}, before being transformed into assembly code. There is still work being done to modify the lowest levels to take advantage of the current generation of microprocessors \cite{dynamic tagging}. We rely on GHC to perform all these optimisations after Supero generates a residual program.

\section{Conclusion and Future Work}

We have introduced an optimiser, which is capable of outperforming the GHC compiler in a limited number of benchmarks. Our optimiser is simple (only 300 lines of Haskell), and yet is able to replicate many of the performance enhancements of GHC in a more general way. While our initial results are promising, they are still incomplete. There are three main obstacles that need to be tackled:

\begin{description}
\item[Termination] While we our confident that our compiler terminates, it is only by use of a crude depth bound whose appropriate value varies for different programs. In order to increase the applicability of our optimiser, we would like to either remove the depth bound, or reduce our reliance upon it.
\item[Benchmarks] We have presented eight benchmarks, and showed promising results on all of them. However, eight benchmarks is not nearly enough. We would like to obtain results on all the remaining benchmarks in the nofib suite.
\item[Performance] The performance results present in \S\ref{sec:haskell_results} are not the best we have achieved -- during experimentation we were able to obtain a 50\% speed up in the primes benchmark. We suspect that much better performance can be obtained.
\end{description}

The shootout has shown that low-level Haskell can compete with with low-level imperative language such as C. Our goal is that Haskell programs can be written in a high-level declarative style, yet still perform competitively.

\paragraph{Acknowledgements} Thanks to Simon Peyton Jones, Simon Marlow and Tim Chevalier for help understanding the low-level details of GHC.

\bibliographystyle{plain}

\bibliography{supero}

\end{document}
