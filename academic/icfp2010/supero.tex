\documentclass[draft]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{balance}

\include{paper}
%include paper.fmt

% Consistency:
% v,w,x,y,xs,ys,z,zs are all variables
% e is an expression
% p is a pattern
% f,g are functions
% m,n are lengths
% i,j are indexes

%format <? = "[\!["
%format ?> = "]\!]"
%format += = "+\!\!\!\!="
%format ==> = "\Longrightarrow{}"
%format <=| = "\unlhd{}"
%format <| = "\lhd{}"

%format w_1  = "\Varid{w_1}"
%format w_2  = "\Varid{w_2}"
%format w_3  = "\Varid{w_3}"
%format w_n  = "\Varid{w_n}"
%format w_12  = "\Varid{w_12}"
%format w_123  = "\Varid{w_123}"
%format v_3  = "\Varid{v_3}"
%format v_4  = "\Varid{v_4}"
%format map_1 = "\Varid{map_1}"
%format map_2 = "\Varid{map_2}"
%format e_1' = "\Varid{e_1^{\prime}}"
%format e_2' = "\Varid{e_2^{\prime}}"
%format e_m' = "\Varid{e_m^{\prime}}"
%format e_1 = "\Varid{e_1}"
%format p_m = "\Varid{p_m}"
%format s_m = "\Varid{s_m}"
%format v_j = "\Varid{v_j}"
%format v_5 = "\Varid{v_5}"
%format v_6 = "\Varid{v_6}"
%format x_3 = "\Varid{x_3}"
%format x_4 = "\Varid{x_4}"
%format x_m = "\Varid{x_m}"
%format s_m_1 = "\Varid{s_{m-1}}"
%format w_m = "\Varid{w_m}"

\newcommand{\unknown}{XXX}
\newcommand{\name}[3]{\ensuremath{\langle\mathsf{#1},\mathsf{#2},\mathsf{#3}\rangle}}
\newcommand{\lemma}[1]{\subsubsection*{\textit{Lemma: #1}}}

\newcommand{\setsup}{\supset_{\mathrm{set}}}
\newcommand{\setequiv}{\equiv_{\mathrm{set}}}
\newcommand{\bagsub}{\subset_{\mathrm{bag}}}
\newcommand{\bagequiv}{\equiv_{\mathrm{bag}}}
\newcommand{\set}{\mathrm{set}}
\newcommand{\veeskip}{\;\vee\;}

\newcommand{\subsubsubsection}[1]{\subsubsection*{#1}}


\begin{document}
\conferenceinfo{ICFP 2010}{}
% \CopyrightYear{2009}
% \copyrightdata{978-1-60558-508-6/09/09}

% \titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Rethinking Supercompilation}
% \subtitle{}

\authorinfo{Neil Mitchell}
           {\verb"ndmitchell@gmail.com"}

\maketitle

% Grammar:
% Which is giving additional information, that is giving essential information
% Run-time library, run-time performance, at runtime.
% Compile-time library, compile-time performance, at compile time.

\begin{abstract}
Supercompilation is a program optimisation technique that is particularly effective at eliminating unnecessary overheads. We have designed a new supercompiler, making many novel choices, including different termination criteria and handling of let bindings. The result is a supercompiler that focuses on simplicity, compiles programs quickly and optimises programs well. We have benchmarked our supercompiler, with some programs running more than twice as fast than when compiled with GHC.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
Languages

\keywords
Haskell, optimisation, supercompilation

\section{Introduction}
\label{sec:introduction}

Consider a program that counts the number of words read from the standard input -- in Haskell \cite{haskell} this can be compactly written as:

\begin{code}
main = print . length . words =<< getContents
\end{code}

Reading the program right to left, we first read the standard input as a string (|getContents|), then split it in to words (|words|), count the number of words (|length|), and print the result (|print|). An equivalent C program is unlikely to use such a high degree of abstraction, and is more likely to get characters and operate on them in a loop while updating some state.

Sadly, such a C program is three times faster, even using the advanced optimising compiler GHC \cite{ghc6_12}. The abstractions that make the program concise have a significant runtime cost. In a previous paper \cite{me:supero} we showed how supercompilation can remove these abstractions, to the stage where the Haskell version is \textit{faster} than the C version (by about 6\%). In the Haskell program after optimisation, all the intermediate lists have been removed, and the |length . words| part of the pipeline is translated into a state machine.

One informal description of supercompilation is that you simply ``run the program at compile time''. This description leads to two questions -- what happens if you are blocked on information only available at runtime, and how do you ensure termination? Answering these questions provides the design for a supercompiler.

\subsection{Contributions}

Our primary contribution is the design of a new supercompiler (\S\ref{sec:method}). Our supercompiler has many differences from previous supercompilers (\S\ref{sec:comparison}), including a new core language, a substantially different treatment of let expressions and an entirely new termination criteria. The result is a supercompiler with a number of desirable properties:

\begin{description}
\item[Simple] Our supercompiler is designed to be simple. From the descriptions given in this paper a reader should be able to write their own supercompiler. We have written a supercompiler following our design which is available online\footnote{\verb"http://hackage.haskell.org/package/supero"}. Much of the code (for example Figure \ref{fig:manager}) has been copied verbatim in to our implementation. The supercompiler can be implemented in under 300 lines of Haskell.
\item[Fast compilation] Previous supercompilers have reported compilation times of up to five minutes for small examples \cite{me:supero}. Our compilation times are under four seconds, and there are many further compile time improvements that could be made (\S\ref{sec:benchmarks_compile}).
\item[Fast runtime] We have benchmarked our supercompiler on a range of small examples (\S\ref{sec:benchmarks}). Some programs optimised with our supercompiler, and then compiled with GHC, are more than twice as fast than when compiled with GHC alone.
\end{description}

We give several examples of how our supercompiler performs (\S\ref{sec:manager_example}), including how it subsumes list fusion and specialisation (\S\ref{sec:examples}), and what happens when the termination criteria are needed (\S\ref{sec:term_example}).

\section{Method}
\label{sec:method}

This section describes our supercompiler. We first present a Core language (\S\ref{sec:core}), along with simplification rules (\S\ref{sec:simplify}). We then present the overall algorithm (\S\ref{sec:manager}), which combines the answers to the following questions:

\begin{itemize}
\item How do you evaluate an open term? (\S\ref{sec:eval})
\item What happens if you can't evaluate an open term further? (\S\ref{sec:eval_split})
\item How do you know when to stop? (\S\ref{sec:term})
\item What happens if you have to stop? (\S\ref{sec:term_split})
\end{itemize}

\noindent Throughout this section we use the following example:

\begin{code}
root g f x = map g (map f x)

map f []      = []
map f (x:xs)  = f x : map f xs
\end{code}

Our supercompiler always optimises the function named |root|. The |root| function applies |map| twice -- the expression |map f x| produces a list that is immediately consumed by |map g|. A good supercompiler should remove the intermediate list.

\subsection{Core Language}
\label{sec:core}

\begin{figure}
\begin{code}
type Var   =   String -- variable/function names
type Con   =   String -- constructor names

data Exp   =   App Var [Var]           -- function application
           |   Con Con [Var]           -- constructor application
           |   Let [(Var,Exp)] Var     -- let expression
           |   Case Var [(Pat, Exp)]   -- case expression
           |   Lam Var Exp             -- lambda expression

type Pat   =   Exp -- restricted to |Con|
\end{code}
\caption{Core Language}
\label{fig:core}
\end{figure}

Our Core language for expressions is given in Figure \ref{fig:core}, and has much in common with Administrative Normal Form \cite{flanagan:continuations}. We make the following observations:

\begin{itemize}
\item We require variables in many places that would normally permit expressions, including let bodies and application. A standard Core language (such as from \citet{ghc_core}) can be translated to ours by inserting let expressions.
\item In many Core languages a distinction is made between standard let bindings and recursive let bindings. We allow expressions bound at a let to refer to variables bound at the same let, but disallow recursion. An alternative description is that we allow recursive let, but only if there is an equivalent nested non-recursive let that can represent the same expression. For example, we allow |let x = y; y = C in C| but not |let x = y; y = x in C|.
\item We don't have default patterns in case expressions. These can be added without great complexity, but are of little interest when describing a supercompiler.
\item We assume programs in our Core language are well-typed, in particular that we never over-apply a constructor or perform case analysis on a function.
\item Function application takes a list of arguments, rather than just a single argument -- the reasons are explained in \S\ref{sec:binaryapp}. We use an application with no arguments to represent just a variable.
\item Variables may be either local (bound in an expression), or global (bound in the environment). We require that all global variables occur as the first argument of |App|.
\end{itemize}

We define the arity of a variable to be the number of arguments that need to be applied before reduction takes place. Bound variables have an arity based on the number of lambda arguments they have -- i.e. |v = \x y -> z| causes |v| to have arity 2. Variables without definitions are assumed to have arity 0, unless they are primitives of known arity (e.g. integer addition has arity 2). In our example |map| has arity 2 and |root| has arity 3. The arity of |f|, |g| and |x| are all 0 as they lack definitions.

We write expressions using standard Haskell syntax (e.g. |let| for |Let|, |case| for |Case| etc.). Rewriting the |map|/|map| example in our Core language gives:

\begin{code}
root = \g f x ->  let  v_1 =  map f x
                       v_2 =  map g v_1
                  in   v_2
\end{code}\begin{code}
map = \f x -> case  x of
                    []    ->   let  v_1 = []
                               in   v_1
                    y:ys  ->   let  v_1 = f y
                                    v_2 = map f y
                                    v_3 = (:) v_1 v_2
                               in   v_3
\end{code}

Our Core language can be rather verbose, so we sometimes use a superset of our Core language, assuming the expressions are translated to our Core language when necessary. For example, we might write |map| as:

\begin{code}
map = \f x -> case  x of
                    []    -> []
                    y:ys  -> f y : map f ys
\end{code}

\subsection{Simplified Core}
\label{sec:simplify}

We now define a simplified form of our Core language. When working with Core expressions we assume they are always simplified, and after constructing new expressions we always simplify them. Our simplified form requires that the root of a function be a sequence of lambda expressions, followed by a let expression -- which we call the \textit{root let}. The expressions bound at the root let must not have the following form:

\begin{itemize}
\item |App v []|, where |v| is bound at the root let -- we can remove the binding by inlining it.
\item |App v vs|, where |v| is bound to a |Con| -- the |App| can be replaced with a |Con| of higher arity.
\item |App v vs|, where |v| is bound to |App w ws| and the arity of |w| is higher than the length of |ws| -- the |App| can be replaced with an |App| with more arguments.
\item |App v vs|, where |v| is bound to a |Lam| -- the |App| can be replaced with the body of the lambda, with the variable substituted.
\item |Case v w|, where |v| is bound to a |Con| -- the |Case| can be replaced with the appropriate alternative.
\item |Let bs v| -- the bindings can be lifted into the root let, renaming variables if necessary.
\end{itemize}

We also remove any bindings which are not used, and ensure all bound variables are unique. As an example, we can simplify the following expression:

\begin{code}
let  v_1  = f
     v_2  = Con x
     v_3  = v_2 y
     v_4  = let w_1 = y in v_1 w_1
     v_5  = case v_3 of Con a b -> v_4 a
in   v_5
\end{code}

\noindent To give:

\begin{code}
let  v_4  = f y
     v_5  = v_4 x
in   v_5
\end{code}

\noindent If the arity of |f| was known to be 2, this would further simplify to:

\begin{code}
let  v_5  = f y x
in   v_5
\end{code}

\subsection{Manager}
\label{sec:manager}

\begin{figure}
\begin{code}
type Env = Var -> Maybe Exp
data Tree = Tree
    {pre :: Exp, gen :: [Var] -> Exp, children :: [Tree]}

supercompile :: Env -> [(Var,Exp)]
supercompile env =
    assign $ flatten $ optimise env $ fromJust $ env "root"

optimise :: Env -> Exp -> Tree
optimise env = f []
    where  f h e  | terminate (<=|) h e = g h e (stop h e)
                  | otherwise = g (e:h) e (reduce env e)
           g h e (gen,cs) = Tree e gen (map (f h) cs)

reduce :: Env -> Exp -> ([Var] -> Exp, [Exp])
reduce env = f []
    where f h e = case  step env e of
                        _ | terminate (<|) h e -> stop h e
                        Just e'  -> f (e:h) e'
                        Nothing  -> split e

flatten :: Tree -> [Tree]
flatten = nubBy (\t_1 t_2 -> pre t_1 == pre t_2) . f []
    where f seen t  =  if pre t `elem` seen then [] else
                       t : concatMap (f (pre t:seen)) (children t)

assign :: [Tree] -> [(Var,Exp)]
assign ts = [(f t, gen t (map f (children t))) |  t <- ts]
    where  f t = fromJust (lookup (pre t) names)
           names = zip (map pre ts) functionNames
\end{code}
\caption{The |supercompile| function.}
\label{fig:manager}
\end{figure}

\begin{figure}
\begin{code}
step :: Env -> Exp -> Maybe Exp -- \S\ref{sec:eval}
split :: Exp -> ([Var] -> Exp, [Exp]) -- \S\ref{sec:eval_split}

type History = [Exp]
(<|),(<=|) :: Exp -> Exp -> Bool -- \S\ref{sec:term}
terminate  :: (Exp -> Exp -> Bool)
           -> History -> Exp -> Bool -- \S\ref{sec:term}
stop :: History -> Exp -> ([Var] -> Exp, [Var]) -- \S\ref{sec:term_split}
\end{code}
\caption{Auxiliary definitions for Figure \ref{fig:manager}.}
\label{fig:manager2}
\end{figure}

Our supercompiler is based around a manager, that integrates the answers to the questions of supercompilation. The manager itself has two main purposes: to ensure termination, and to create recursive functions. In our experience the creation of recursive functions is often the most delicate part of a supercompiler, so we deliberately include all the details. The code for our manager is given in Figure \ref{fig:manager}, making use of a some auxiliary functions whose types are given in Figure \ref{fig:manager2}. We first give an intuition for how the manager works, then describe each part separately.

Our supercompiler takes a source program, and generates a target program. Functions in these programs are distinct -- target expressions cannot refer to source functions. The source and target program are equivalent, but hopefully the target program runs faster. We use the type |Env| to represent a mapping from source function names to expressions, allowing a result of |Nothing| to indicate a primitive function.

The manager first builds a tree (the type |Tree|), where each node has a source expression (|pre|) and an equivalent target expression. The target expression may call target functions, but these functions do not yet have names. Therefore, we store target expressions as a generator that when given the function names produces the target expression (|gen|), and a list of trees for the functions it calls (|children|). We then flatten this tree, ensuring identical functions are only included once, and assign names to each node before generating the target program. If a target function is recursive then the initial tree will be infinite, but the flattened tree will always be finite due to the termination scheme defined in \S\ref{sec:term}.

\newcommand{\function}[1]{\paragraph{\textsf{#1:}}\hspace{-3mm}}

\function{supercompile} This function puts all the parts together. Reading from right to left, we first generate a potentially infinite tree by optimising the function |root|, we then flatten the tree to a finite number of functions, and finally assign names to each of the result functions.

\function{optimise} This function constructs the tree of result functions. While the tree may be infinite, we demand that any infinite path from the root must encounter the same |pre| value more than once. We require that for any infinite sequence of distinct expressions |h|, there must exist an |i| such that |terminate (<=||) (take i h) (h !! i+1)| returns |True|. If we are forced to terminate we call |stop|, which splits the expression into several subexpressions. We require that |stop h| only produces subexpressions which pass the termination test. If the termination criteria do not force us to stop, then we call |reduce| to evaluate the expression.

\function{reduce} This function optimises an expression by repeatedly evaluating it with calls to |step|. If we can't evaluate any further we call |split|. We use a local termination test to ensure the evaluation terminates. We require that for any infinite sequence of expressions |h|, there must exist an |i| such that |terminate (<||) (take i h) (h !! i+1)| returns |True|.

\function{flatten} This function takes a tree and extracts a finite number of functions from it, assuming the termination restrictions given in |optimise|. Our |flatten| function will only keep one tree associated with each source expression. These trees may have different target expressions if one resulted from a call to |stop|, while another resulted from a call to |reduce| -- but all are semantically equivalent.

\function{assign} This function assigns names to each target function, and constructs the target expressions by calling |gen|. We assume the function |functionNames| returns an infinite list of function names.

\subsubsection{Notation}
\label{sec:child_notation}

To represent values of type |([Var] -> Exp, [Exp])| we use |<? bullet ?>| notation to identify particular children of an expression. As an example:

\begin{code}
\g f -> map g (<? map f xs ?>)
\end{code}

\noindent This expression denotes a value whose first component is the function that when given ``|name|'' returns:

\begin{code}
\g f -> map g (name f)
\end{code}

\noindent And the second component is the singleton list containing the expression:

\begin{code}
\f -> map f xs
\end{code}

To translate from the original expression using |<? bullet ?>| notation we pass as arguments all the variables bound in the outer expression and free in the child expression (|f| in this example). The first component of the result is a function that replaces each child with a name, and the second component is the list of children.

\subsubsection{Example}
\label{sec:manager_example}

Revisiting our initial example, |supercompile| first calls |optimise| with:

\begin{code}
\g f x -> map g (map f x)
\end{code}

The termination history is empty, so we call |reduce|, which calls |step| repeatedly until we reach the expression:

\begin{code}
\g f x ->  let  v = case  w of
                          []    -> []
                          y:ys  -> g y : map g ys
                w = case  x of
                          []    -> []
                          z:zs  -> f z : map f zs
           in   v
\end{code}

The |step| function now returns |Nothing|, since we cannot evaluate further without the result of |x|. We therefore call |split|, which results in (using the notation from \S\ref{sec:child_notation}):

\begin{code}
\g f x -> case  x of
                []    -> <? let v = ...; w = ...; x = []    in v ?>
                z:zs  -> <? let v = ...; w = ...; x = z:zs  in v ?>
\end{code}

Looking at the first child expression, where |x = []|, the simplification rules from \S\ref{sec:simplify} immediately produce |[]| as the result. The second child starts as:

\begin{code}
\g f z zs ->
    let  v  = case  w  of [] -> []; y  :ys  -> g  y  : map g  ys
         w  = case  x  of [] -> []; z  :zs  -> f  z  : map f  zs
         x  = z:zs
    in   v
\end{code}

\noindent Which simplifies to:

\begin{code}
\g f z zs ->  let  v   = q : qs
                   q   = g y
                   qs  = map g ys
                   y   = f z
                   ys  = map f zs
              in   v
\end{code}

Calling |step| produces |Nothing|, as the root of this expression is a constructor |(:)| which can't be evaluated. We therefore call |split| which results in:

\begin{code}
\g f z zs ->  let  q   = <? g (f z) ?>
                   qs  = <? map g (map f zs) ?>
                   v   = q : qs
              in   v
\end{code}

When optimising |g (f z)| we get no optimisation, as there is no available information. To optimise |map g (map f zs)| we repeat the exact same steps we have already done. However, the |flatten| function will spot that both |Tree| nodes have the same |pre| expression (modulo variable renaming), and reduce them to one node, creating a recursive function. We then assign names using |assign|. For the purposes of display (not optimisation), we apply a number of simplifications given in \S\ref{sec:postprocess}. The end result is:

\begin{code}
root g f x = case  x of
                   []    -> []
                   z:zs  -> g (f z) : root g f zs
\end{code}

The final version has automatically removed the intermediate list, with no additional knowledge about the |map| function or its fusion rules.

\subsection{Evaluation}
\label{sec:eval}

\begin{figure}
\begin{code}
force :: Exp -> Maybe Var
force (Case  v _  )  = Just v
force (App   v _  )  = Just v
force _              = Nothing

next :: Exp -> Maybe Var
next (Lam _ x) = next x
next (Let bind v) = last $ Nothing : f v
    where f v = case  lookup v bind of
                      Nothing -> []
                      Just e -> Just v : maybe [] f (force e)
\end{code}
\caption{Function to determine the next evaluated binding.}
\label{fig:stack}
\end{figure}

Evaluation is based around the |step| function. Given an expression, |step| either replaces a variable with its associated value from the environment and returns |Just|, or if no suitable variable is found returns |Nothing|. We always replace the variable that would be evaluated next during normal evaluation.

To determine which variable would be evaluated next, we define the functions |force| and |next| in Figure \ref{fig:stack}. The function |force| determines which variable will be evaluated next given an expression -- either a case scrutinee or an applied variable. The function |next| determines which variable bound at the root let will be evaluated next, by following the forced variables of the let bindings. Looking at the original example:

\begin{code}
\g f x ->  let  v_1 =  map f x
                v_2 =  map g v_1
           in   v_2
\end{code}

The function |next| returns |Just v_2|. Calling |force| on the expression |map g v_1| returns |map|, but since |map| is not bound at the root let we go no further. Therefore, to evaluate this expression we will start by evaluating |v_2|, and thus |map|. To perform an evaluation step we insert a fresh variable |v_3| bound to the body of |map|, and replace the |map| variable in |v_2| with |v_3|. This transformation results in:

\begin{code}
\g f x ->  let  v_1 =  map f x
                v_2 =  v_3 g v_1
                v_3 =  \f x -> case  x of
                                     []    -> []
                                     y:ys  -> f y : map f ys
           in   v_2
\end{code}

Simplification immediately removes the lambda at |v_3|, replacing |v_2| with a case expression on |v_1|.

More generally, we match any expression with the following pattern:

\begin{code}
\free ->  let  s    = f w_1 w_n
               v_1  = e_1
               v_n  = e_n
          in   v
where Just e' = env f
\end{code}

We use |s| to represent the next binding to be evaluated, as returned by |next|. We allow any other variables |v_1..v_n| to be present, bound to expressions |e_1..e_n|. Given this configuration we can rewrite to:

\begin{code}
\free ->  let  s'   = e'
               s    = s' w_1 w_n
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

As always, after generating a new expression we immediately apply the simplification rules (\S\ref{sec:simplify}).

\subsection{Evaluation Splitting}
\label{sec:eval_split}

If evaluation cannot proceed, we split to produce a target expression, and a list of child expressions for further optimisation, using the |<? bullet ?>| notation described in \S\ref{sec:child_notation}. When splitting an expression there are three concerns:

\paragraph{Permit further optimisation:} We aim to place any constructs blocking evaluation in the target expression, so that the children are suitable for further optimisation.

\paragraph{No unbounded loss of sharing:} A complex variable cannot be duplicated if that causes it to be evaluated multiple times at runtime. The target program cannot remove sharing present in the source program.

\paragraph{Keep expressions together:} If we split variables bound at the same let expression in to separate child expressions we lose optimisation opportunities, as the right-hand side of the variable is no longer available for optimisation. We aim to keep as many expressions together as possible, but not if that loses sharing.

\smallskip
We split in one of three different ways, depending on the type of the next expression to be evaluated (as described in \S\ref{sec:eval}). We now describe each of the three ways to split, in each case we start with an example, then define the general rule.

\subsubsection{Case Expression}
\label{sec:eval_split_case}

If the next expression is a case expression then we make the target a similar case expression, and under each alternative we create a child expression with the case scrutinee bound to the appropriate pattern. For example, given:

\begin{code}
\x ->  let   v = case  x of
                       []    -> []
                       y:ys  -> add y ys
       in    v
\end{code}

\noindent We split to produce:

\begin{code}
\x ->  case x of
       []    -> <?  let  v =  case x of [] -> []; y:ys -> add y ys
                         x =  []
                    in   v ?>
       y:ys  -> <?  let  v =  case x of [] -> []; y:ys -> add y ys
                         x =  y:ys
                    in   v ?>
\end{code}

Looking more closely at the second child, we start with the expression:

\begin{code}
\y ys ->  let  v  = case x of [] -> []; y:ys -> add y ys
               x  = y : ys
          in   v
\end{code}

\noindent This expression immediately simplifies to:

\begin{code}
\y ys ->  let  v = add y ys
          in   v
\end{code}

More generally, if |s| is the next expression to evaluate:

\begin{code}
\free ->  let  s    = case x of p_1 -> e_1' ; p_m -> e_m'
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

\noindent After |split| it becomes:

\begin{onepage}\begin{code}
\free -> case x of
    p_1  -> <? let  s    = case x of p_1 -> e_1'; p_m -> e_m'
                    v_1  = e_1; v_n = e_n; x = p_1 in v ?>
    p_m  -> <? let  s    = case x of p_1 -> e_1'; p_m -> e_m'
                    v_1  = e_1; v_n = e_n; x = p_m in v ?>
\end{code}\end{onepage}

\subsubsection{Lambda}
\label{sec:eval_split_lambda}

If the next binding to be evaluated is a lambda, then we place a lambda in the target program. The key point when splitting a lambda is that we do not reduce sharing. Consider the following example:

\begin{code}
\x ->  let  s    = \y -> add v_1 y
            v_1  = expensive v_2
            v_2  = f x
       in   s
\end{code}

The |add| function takes two arguments, but only has one so far. We cannot move the argument |y| upwards to form |\x y -> ...|, as this action potentially duplicates the expensive computation of |v_1|. Instead, we create child expressions for every variable binding, and for the body of the lambda:

\begin{code}
\x ->  let  s    = \y -> <? add v_1 y ?>
            v_1  = <? expensive v_2 ?>
            v_2  = <? f x ?>
       in   s
\end{code}

Unfortunately, we have now split the bindings for |v_1| and |v_2| apart, when there is no real need. We therefore move binding |v_2| under |v_1|, because it is only referred to by |v_1|, to give:

\begin{code}
\x ->  let  s    = \y -> <? add v_1 y ?>
            v_1  = <? let v_2 = f x in expensive v_2 ?>
       in   s
\end{code}

We will now optimise the body of |v_1|, and the body of the lambda, which will be able to evaluate |add|. More generally, given:

\begin{code}
\free ->  let  s    = \x -> e'
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

\noindent We rewrite:

\begin{code}
\free ->  let  s    = \x -> <? e' ?>
               v_1  = <? e_1 ?>
               v_n  = <? e_n ?>
          in   v
\end{code}

We then repeatedly move any binding |v_i| under |v_j| if either: 1) |v_i| is only used within the body of |v_j|; or 2) the expression bound to |v_i| is cheap. We define an expression to be cheap if it is a constructor, or an application to a variable |v| with fewer arguments than the arity of |v| (a partial application). The intention of moving bindings is to increase sharing, which can be done provided we don't duplicate work (condition 1), or if the work duplicated is bounded (condition 2).

\subsubsection{Anything Else}
\label{sec:eval_split_other}

The final rule applies if the next expression is not a case expression or a lambda, including a constructor, a variable, and an application of a variable not bound in the environment. We do not deal with variables bound in the environment, as these are handled by |step|. Given the example:

\begin{onepage}\begin{code}
\x y ->  let  v_1 = expensive x
              v_2 = v_1 x
              v_3 = Con v_2 y v_2
         in   v_3
\end{code}\end{onepage}

\noindent We turn each binding into a child, apart from the next binding to be evaluated:

\begin{code}
\x y ->  let  v_1 = <? expensive x ?>
              v_2 = <? v_1 x ?>
              v_3 = Con v_2 y v_2
         in   v_3
\end{code}

\noindent We then perform the same sharing transformation as for lambda expressions, noting that |v_1| is only used within |v_2|, to give:

\begin{code}
\x y ->  let  v_2 = <? let v_1 = expensive x in v_1 x ?>
              v_3 = Con v_2 x v_2
         in   v_3
\end{code}

More generally, given an expression:

\begin{code}
\free ->  let  s    = e'
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

\noindent We rewrite to:

\begin{code}
\free ->  let  s    = e'
               v_1  = <? e_1 ?>
               v_n  = <? e_n ?>
          in   v
\end{code}

We then repeatedly move any binding |v_i| under |v_j| according to the criteria given in \S\ref{sec:eval_split_lambda}.

\subsection{Termination}
\label{sec:term}

The termination rule is responsible for ensuring that whenever we proceed along a list of expressions we eventually stop. The intuition is that each expression has a set of bindings at the root let, and each of these bindings has a name indicating where it came from in the source program. Compared to all earlier expressions in a list, each root let must contain either different names, or fewer names.

In this section we first describe the |terminate|, $\lhd$ and $\unlhd$ functions from a mathematical perspective, then how we apply these functions to expressions. Finally, we show an example of how these rules ensure termination.

\subsubsection{Termination Rule}
\label{sec:term_rule}

Our termination orderings are defined over bags (also known as multisets) of values drawn from a finite alphabet $\Sigma$. A bag of values is unordered, but may contain elements more than once. We define our orderings as:

\vspace{-\bigskipamount}

\[
x \lhd y = \set(x) \not\equiv \set(y)  \veeskip \# x < \# y
\]
\[
x \unlhd y = x \equiv y \veeskip x \lhd y
\]

We use $\set(x)$ to transform a bag to a set, and $\#$ as the cardinality operator to take the number of elements in a bag. A sequence $x_1 \ldots x_n$ is well-formed under $\lhd$ if for all indices $i < j \Rightarrow x_j \lhd x_i$ (and respectively for $\unlhd$).

The following sequences are well-formed under both $\unlhd$ and $\lhd$:

\begin{code}
[a,aaaaab,aaabb,b]
[abc,ab,accc,a]
[aaaaabbb,aaab,aab]
\end{code}

The following sequences are well-formed under $\unlhd$, but not under $\lhd$:

\begin{code}
[aaa,aaa]
[aabb,ab,ab]
\end{code}

The following sequences are not well-formed under $\unlhd$ or $\lhd$:

\begin{code}
[abc,abcc]
[aa,aaa]
\end{code}

We define the |terminate| function referred to in Figure \ref{fig:manager2} as:

\begin{code}
terminate  :: (Exp -> Exp -> Bool) -> History -> Exp -> Bool
terminate (<) h e = not $ all (e <) h
\end{code}

The |terminate| function returns |False| if given a well-formed sequence (|h|), adding the expression |e| will keep the sequence well-formed.

\lemma{Any well-formed sequence under $\lhd$ is finite}

Given a finite alphabet $\Sigma$, any well-formed sequence under $\lhd$ is finite. Consider a well-formed sequence $x_1\ldots$. We can partition this sequence into at most $2^\Sigma$ subsequences using set equality. Consider any subsequence $y_1\ldots$. For any two elements in this subsequence, $\set(y_i) \not\equiv \set(y_j)$ will be false, due to the partitioning. Therefore, for the sequence to be well-formed, $i < j \Rightarrow \# y_j < \# y_i$. Therefore there can be at most $\#y_1+1$ elements in any particular subsequence. Combined with a finite number of subsequences, we conclude that any well-formed sequence is finite.

\lemma{Any well-formed sequence under $\unlhd$ has a finite number of distinct elements.}

Given a finite alphabet $\Sigma$, any well-formed sequence under $\unlhd$ has a finite number of distinct elements. For a sequence to be well-formed under $\unlhd$ but not $\lhd$ it must have elements which are duplicates. If we remove all duplicates we end up with a well-formed sequenced under $\lhd$, which must be finite. Therefore there must be a finite number of distinct elements.

\subsubsection{Tracking Names}

Every expression in the source program is assigned a name. A name is a triple, \name{\mathit{f}}{\mathit{e}}{\mathit{a}} where $f$ is a function name, $e$ is an expression index and $a$ is an argument count. We label every expression in the source program with $f$ being the function it comes from and $e$ being a unique index within that function. The argument count $a$ for constructors and applications is the number of arguments, and for all other expressions is 0. When manipulating expressions, we track names:

\begin{itemize}
\item When renaming a bound variable, or substituting one variable for another, we do not change any names.
\item If we move a subexpression, we keep the name already assigned to that subexpression.
\item If we increase the number of arguments to an application or constructor, we increase the argument count of that expression. For example, |let v = C x; w = v y in w| being transformed to |let w = C x y in w| would have the new name for |w| set to the old name of |v|, but with an argument count of 2 instead of 1.
\item When splitting on a case we introduce a new constructor (see \S\ref{sec:eval_split_case}), for this constructor we use the name assigned to the pattern from the case alternative.
\end{itemize}

We map an expression to a bag of names by taking the names of all expressions bound at the root let.

\lemma{For any source program, there are a finite number of names}

All subexpressions are assigned expression indices in advance, so there are only a finite number of function name/index values. We only increase the argument count when increasing the number of arguments applied to a constructor or application, which is bounded by the arity of that constructor or the source function. Therefore, there are only a finite number of names.

\lemma{There are a finite number of expressions for any bag}

Given a bag of names, there are only a finite number of expressions that could have generated it. We first assume that when simplifying an expression we always normalise the free variables -- naming the let body |v_1|, and naming all other variables as they are reached from |v_1|. Each name refers to one particular subexpression, but may have different variable names. A finite number of subexpressions can only be combined to produce a finite number of expressions, if we ignore variable names, which the normalisation handles.

\lemma{The termination properties required by \S\ref{sec:manager} are satisfied}

The termination properties in \S\ref{sec:manager} are satisfied by the lemmas in this section. We have shown that the alphabet of names, $\Sigma$, is finite. For |terminate (<=||)| we have shown that there can only be a finite number of distinct name bags, and that each name bag can only correspond to a finite number of expressions. For |terminate (<||)| we have shown that there can only be a finite number of name bags.

\subsubsection{Termination Splitting}
\label{sec:term_split}

If we are forced to terminate we call |stop|, which splits the expression into several subexpressions. We require that |stop h e| only produces subexpressions which are not forced to terminate by |terminate (<=||) h|. We trivially satisfy this requirement by using the termination criteria when defining |stop|.

Given an expression:

\begin{code}
\free ->  let  v_1 = e_1
               v_n = e_n
          in   v
\end{code}

We first split every variable bound at the let, to give:

\begin{code}
\free ->  let  v_1 = <? e_1 ?>
               v_n = <? e_n ?>
          in   v
\end{code}

We now merge variable |v_i| under |v_j| using the same conditions as |split|, described in \S\ref{sec:eval_split_lambda}. In addition, we do not merge |v_i| under |v_j| if the resulting expression bound to |v_j| would violate the termination criteria |terminate (<=||) h|. Combined with with the property that all initial children are singleton name bags, which trivially satisfy $\unlhd$ for any expression, our merge restriction ensures no children violate the termination criteria.

As a heuristic, we merge variable |v| before |w| if the name associated with |v| occurs fewer times in the original expression. By favouring names used fewer times we hope to encourage splitting apart an expression that is growing in number, and will thus continue to grow in recursive calls. This heuristic has no effect on the correctness or termination, but can sometimes result in better optimisation.

\subsubsection{Example}
\label{sec:term_example}

Many simple example programs (such as |map|/|map|) never trigger the termination criteria. The standard example of a function that does require termination is |reverse|, which can be written in a simplified form as:

\begin{code}
root xs = rev [] xs
rev acc xs = case  xs of
                   []    -> acc
                   y:ys  -> rev (y:acc) ys
\end{code}

The |rev| function builds up an accumulator argument, which will be equal to the size of |xs|. To specialise on the accumulator argument would require an infinite number of specialisations. To supercompile this program, the |optimise| function starts with an empty termination history and the expression |rev [] xs|, and calls |reduce|, resulting in:

\begin{code}
\xs -> case  xs of
             []    -> <? [] ?>
             y:ys  -> <? rev (y:[]) ys ?>
\end{code}

Focusing on the second alternative, we now add |rev [] xs| to the termination history, and continue optimising |rev (y:[]) ys|. This leads to the sequence of expressions:

\begin{code}
\x_1 -> rev [] x_1
\x_1 x_2 -> rev (x_1:[]) x_2
\x_1 x_2 x_3 -> rev (x_1:x_2:[]) x_3
...
\end{code}

We can rewrite these expressions in our core language, with annotations for names:

\newlength{\lenroot}
\newlength{\lenrev}
\settowidth{\lenroot}{|root|}
\settowidth{\lenrev}{|rev|}
\addtolength{\lenroot}{-\lenrev}
\newcommand{\nameroot}[1]{\name{root}{#1}{0}}
\newcommand{\namerev}[1]{\name{rev\hspace{\lenroot}}{#1}{0}\hspace{1mm}}

\begin{code}
\x_1 ->
    let  v_1 = {-"\nameroot{1}"-}  rev v_2 x_1
         v_2 = {-"\nameroot{2}"-}  []
    in   v_1
\x_1 x_2 ->
    let  v_1 = {-"\namerev{1}"-}   rev v_2 x_2
         v_2 = {-"\namerev{2}"-}   x_1:v_3
         v_3 = {-"\nameroot{2}"-}  []
    in   v_1
\x_1 x_2 x_3 ->
    let  v_1 = {-"\namerev{1}"-}   rev v_2 x_3
         v_2 = {-"\namerev{2}"-}   x_1:v_3
         v_3 = {-"\namerev{2}"-}   x_2:v_4
         v_4 = {-"\nameroot{2}"-}  []
    in   v_1
\end{code}

Under $\unlhd$ the first two expressions create a well-formed sequence, but the first three expressions do not. The first expression is permitted because the history is empty. The second expression is permitted because it has a different set of names from the first. The third expression has the same set of names as the second, and has a higher cardinality. Therefore, when optimising, we call |stop| on the third expression. After calling |stop| we get:

\begin{code}
\x_1 x_2 x_3 ->
    let  v_1 = <?  let  v_1  = {-"\namerev{1}"-}   rev v_2 x_3
                        v_2  = {-"\namerev{2}"-}   x_1:v_3
                   in   v_1 ?>
         v_3 = <?  let  v_3  = {-"\namerev{2}"-}   x_2:v_4
                        v_4  = {-"\nameroot{2}"-}  []
                   in   v_3 ?>
    in   v_1
\end{code}

Part of the accumulator has been bound to |v_3|, and separated from the main expression. Continuing to optimise we get the sequence:

\begin{code}
\x_1 -> rev [] x_1                          -- |reduce|
\x_1 x_2 -> rev (x_1:[]) x_2                -- |reduce|
\x_1 x_2 x_3 -> rev (x_1:x_2:[]) x_3        -- |stop|
\x_1 x_2 x_3 -> rev (x_1:x_2) x_3           -- |reduce|
\x_1 x_2 x_3 x_4 -> rev (x_1:x_2:x_3) x_4   -- |stop|
\x_1 x_2 x_3 -> rev (x_1:x_2) x_3           -- |reduce|
... -- repeat the last 2 lines
\end{code}

As required, we have a finite number of distinct expressions, and end up with a recursive function in the target program.

\subsection{Post-processing}
\label{sec:postprocess}

Our |split| function is structured to produce only one simple expression per target function -- for example a target function will never contain two constructors. While most opportunities to remove intermediate structure have been exploited, the target program will usually contain lots of small functions. We can eliminate many of these functions by inlining all functions which are only called once. For example, given the source program:

\begin{code}
root x = x : x : []
\end{code}

\noindent After supercompilation, we get the target program:

\begin{code}
root x = x : f x
f x = x : nil
nil = []
\end{code}

\noindent We can then inline all functions that are only called once:

\begin{code}
root x = x : x : []
\end{code}

It is important that the only optimisation intended from this post-processing is the reduction of function call overhead. This use of inlining is substantially different from other compilers \cite{spj:inlining}, where inlining is used to bring expressions together to trigger other optimisations.

\subsection{Alternative Designs}
\label{sec:extensions}

In this section we describe some possible design alternatives for our supercompiler.

\subsubsection{Binary Application}
\label{sec:binaryapp}

The first version of this supercompiler had binary application, rather than vector application. The |App Var [Var]| constructor was replaced by a combination of |Var Var| and |App Var Var|. The reason for originally choosing binary application is that it is closer to other Core languages, and the simplification does not need to track arity information. There were three main reasons for moving to vector application:

\begin{itemize}
\item Vector application simplifies splitting with primitive functions, by providing the arity information directly.
\item Vector application makes it easier to identify partial applications when increasing sharing (see \S\ref{sec:eval_split_lambda}).
\item Vector application reduces the number of names in an expression, improving the time taken to compile.
\end{itemize}

\subsubsection{Alternative Termination Orderings}

Our original termination ordering was:

\vspace{-\bigskipamount}

\[
x \lhd y = x \setsup y \vee x \bagsub y
\]

Both this ordering and the one described in \S\ref{sec:term_rule} can be proved using the same argument. We switched to use our new ordering because it is simpler, follows more directly from the proof, and can be implemented very efficiently. Choosing a termination ordering is a tricky business -- no termination ordering can be the best for all programs, so there is always scope for experimentation.

\subsubsection{Recursive Lets}

Our Core language does not include recursive lets. Recursive lets bound to functions can be efficiently removed using lambda-lifting \cite{lambda_lift}. Recursive lets bound to values can be removed, but doing so may cause the program to run arbitrarily slower \cite{me:thesis}. Alternatively, we can take functions with value recursive lets and make them primitives, losing optimisation potential, but preserving complexity. In practice, the most common function with a value recursive let is |repeat|, and our supercompiler is nearly always able to fuse away the list generated by |repeat|.

\subsubsection{Common Subexpression Elimination}

Common Subexpression Elimination (CSE) involves detecting when a program will compute two identical expressions, and reducing them both to a single shared expression. Our Core language is well suited to CSE -- two variables can be merged if they have the same bound expression. The advantage of CSE is that performance can be increased, sometimes asymptotically. The disadvantages are that CSE can introduce space leaks \cite{chitil:cse}, and the additional sharing may stop variables from being moved when splitting. We have not investigated the performance impact of CSE on supercompilation, but think it is a worthwhile area for future research.

\subsubsection{Inlining Simple Functions}

The GHC compiler inlines many non-recursive functions during the simplification phase \cite{spj:inlining}. It is certainly possible that our simplification rules could be extended to inline some functions, such as |id|, provided no new names were introduced (and thus termination was unaffected). Another alternative would be to inline simple functions in the source program before supercompilation started (such as |otherwise| and |(.)|). The primary motivation for inlining simple functions would be to reduce the complexity of the main supercompilation phase, and avoid inopportune termination splits. We have deliberately \textit{not} performed any inlining other than in the |step| function, as there is a risk that doing so would hide weaknesses in our supercompiler. However, we think simple inlining would be worth investigating.

\section{Comparison to Other Optimisations}
\label{sec:examples}

Supercompilation naturally subsumes many other optimisations, including constructor specialisation \cite{spj:specconstr} and deforestation \cite{gill:shortcut_deforestation,wadler:deforestation}. However, there are some optimisations that supercompilation (in the form presented here) does not address -- in particular strictness analysis and unboxing \cite{spj:unboxing}, and the generation of native code. In order to benefit from these optimisations we use GHC to compile the resulting Core after supercompilation \cite{ghc6_12}.

We now give an example where our supercompiler massively outperforms GHC, and discuss the optimisations being performed. Our example is:

\begin{code}
root n = map square (iterate (+1) 1) !! n
    where square x = x * x
\end{code}

Running this program with |n=400000|, GHC takes 0.149 seconds, while our supercompiler combined with GHC takes 0.011 seconds. Running for larger values of |n| is infeasible as the GHC only variant overflows the stack. After optimising with our supercompiler, then compiling with GHC, the resulting inner-loop is:

\begin{code}
go :: Int# -> Int# -> Int#
go x y = case x of
    0 -> y * y
    _ -> go (x - 1) (y + 1)
\end{code}

All the intermediate lists have been removed, there are no functional values, all the numbers have been unboxed and all arithmetic is performed on unboxed values (GHC uses |Int#| as the type of unboxed integers). Supercompilation has fused all the intermediate lists and specialised all functional arguments, leaving GHC to perform strictness analysis and unboxing.

The program compiled with GHC alone is much less efficient. GHC uses programmer supplied rewrite rules to eliminate intermediate lists \cite{spj:rules}, which fuses the |map|/|iterate| combination. Unfortunately, GHC does not contain a rule to fuse the input list to the |(!!)| operator. The GHC rules match specific function names in the source program, meaning that redefining |map| locally would inhibit the fusion. In contrast, our supercompiler does not rely on rules so is able to fuse the functions regardless of their names, and is able to perform fusion on data types other than lists.

GHC specialises the resulting |map|/|iterate| combination with the |square| function, but fails to specialise with increment -- passing |(+1)| as a higher-order function. GHC can specialise functions to particular data values using constructor specialisation, but does not currently do the same transformation for functional arguments. To allow specialisation, some functions are written in a particular style:

\begin{code}
foldr f z xs = go xs
    where  go []      = z
           go (y:ys)  = f y (go ys)
\end{code}

In this definition, provided lambda-lifting is not performed, the function |foldr| is considered non-recursive. GHC can inline non-recursive functions, allowing the definition of |foldr| to be replicated in an expression where |f| is known, eliminating the functional argument. In contrast, our supercompiler has specialised all the functions to their functional arguments, even when written in a natural style.

GHC fails to eliminate all the lists and higher-order functions, which in turn means the integers are not detected as strict, and are not unboxed. In contrast, our supercompiler has reduced the program sufficiently for everything to be unboxed.

\section{Benchmarks}
\label{sec:benchmarks}

\begin{table}
\begin{tabular}{@@{}lrrrr}
\textbf{Program} & \textbf{Compile time} & \textbf{Runtime} & \textbf{Memory} & \textbf{Size} \\
append & 0.1 + 0.6 & 0.85 & 0.84 & 1.00 \\
bernouilli & 2.4 + 1.3 & 1.04 & 0.96 & 1.02 \\
charcount & 0.1 + 0.6 & 0.14 & 0.01 & 0.99 \\
digits-of-e2 & 2.0 + 0.8 & 0.40 & 0.45 & 0.99 \\
exp3\_8 & 0.5 + 0.8 & 0.93 & 1.00 & 1.08 \\
factorial & 0.1 + 0.6 & 0.98 & 1.00 & 1.00  \\
linecount & 0.2 + 0.6 & 0.01 & 0.01 & 0.98 \\
primes & 0.1 + 0.6 & 0.58 & 0.81 & 0.99 \\
raytracer & 0.1 + 0.6 & 0.56 & 0.44 & 1.00 \\
rfib & 0.1 + 0.7 & 0.77 & 1.01 & 0.98 \\
sumsquare & 1.2 + 0.9 & 0.38 & 0.23 & 1.03 \\
sumtree & 0.1 + 0.6 & 0.14 & 0.01 & 1.00 \\
tak & 0.1 + 0.6 & 0.79 & 1.01 & 0.98 \\
treeflip & 0.1 + 0.6 & 0.57 & 0.45 & 1.01 \\
wordcount & 0.3 + 0.7 & 0.19 & 0.30 & 1.00 \\
x2n1 & 0.1 + 0.8 & 0.90 & 0.99 & 1.00 \\
\end{tabular}
\\ \\
\textbf{Program} is the name of the program as given in \S\ref{sec:benchmarks}. \textbf{Compile time} is the number of seconds to compile the program ($a+b$), including both compilation with our supercompiler ($a$) and the subsequent compilation with GHC ($b$). The final three columns are relative to \texttt{ghc -O2} being 1.00, with a lower number being better. \textbf{Runtime} is how long the optimised program takes to run. \textbf{Memory} is the amount of memory allocated on the heap. \textbf{Size} is the size of the optimised program on disk.
\caption{Benchmark results.}
\label{tab:results}
\end{table}

In this section we run our supercompiler over a range of benchmarks drawn from other papers. The results are given in Table \ref{tab:results}. The benchmarks we use are:

\begin{itemize}
\item sumsquare is the introductory example used in the stream fusion paper \cite{coutts:stream_fusion}.
\item charcount, linecount and wordcount are taken from our previous supercompiler work \cite{me:supero}, and wordcount is used as the example in \S\ref{sec:introduction}. For the purpose of benchmarking, we have removed the actual IO operations, leaving just the actual computation.
\item append, factorial, treeflip, sumtree and raytracer have been used to benchmark other supercompilers \cite{jonsson:supercompilation}, and originate from papers on deforestation \cite{wadler:deforestation,kort:raytracer}.
\item bernouilli, digits-of-e2, exp3\_8, primes, rfib, tak and x2n1 are all taken from the Imaginary section of the nofib benchmark suite \cite{nofib}.
\end{itemize}

We have manually translated all the examples from their source language to our Core language. We have taken care to ensure that we have not simplified the programs in translation -- in particular we have inserted explicit dictionaries for all examples that require type classes \cite{wadler:type_classes}, and have translated list-comprehensions to |concatMap| as described by the Haskell report \cite{haskell}.

For comparison purposes we compiled all the benchmarks with GHC 6.12.1 \cite{ghc6_12}, using the \texttt{-O2} optimisation setting. For the supercompiled results we first ran our supercompiler, then compiled the result using GHC. To run the benchmarks we used a 32bit Windows machine with a 2.5GHz processor and 4Gb of RAM.

\subsection{Comparison to GHC}

The benchmarks are nearly all faster than GHC, with some programs running substantially faster than GHC alone. The improvement in speed is usually accompanied by either a similar memory usage, or a substantial reduction. The resulting executables are all very close in size to compilation with GHC alone -- partly because the run-time system accounts for a substantial proportion of the executable size.

\paragraph{Numerical Computation} Some of the benchmarks mainly test numerical performance -- for example factorial, x2n1 and tak. In these benchmarks we have been able to inline some of the functions even though they are recursive, which has been equivalent to a small amount of loop unrolling, and has sometimes improved execution speed.

\paragraph{Complete Elimination} Some of the benchmarks allow us to completely eliminate most intermediate values -- for example charcount, sumtree and raytracer. In these cases the execution time and memory are both substantially reduced. Most of these benchmarks have previously been used to test supercompilers, and our supercompiler performs the same optimisations.

\paragraph{Partial Elimination} Some of the benchmarks have a combination of data structures and numerical computation -- for example primes, digits-of-e2 and exp3\_8. In these benchmarks we perform specialisation, and remove some intermediate values, but due to the nature of the benchmarks not all intermediate values can be eliminated. In digits-of-e2 we are able to fuse long pipelines of list fusions. In exp3\_8 most of our performance increase comes from eliminating intermediate values of the data type |data Nat = Z || S Nat|.

\paragraph{Bernouilli} The benchmark on which we do worst is bernouilli. The bernouilli program seems reasonably similar in terms of list operations to other benchmarks such as primes, but our supercompiler is unable to outperform GHC -- the exact reasons are still unclear. Interestingly, both our previous supercompiler and the stream fusion work also failed to outperform GHC on this benchmark, so the reason may be that GHC does a particularly good job on this benchmark.

\subsection{Compilation Speed}
\label{sec:benchmarks_compile}

In the benchmarks presented, our supercompiler always takes under four seconds to compile. We have given the compilation times as two figures -- the time taken to run the supercompiler, followed by the time taken to compile the result with GHC. In all cases, the resulting GHC compilation time is dominated by the linker. Compared to our previous supercompiler, where compile times ranged from a few seconds to five minutes, our new supercompiler is substantially faster.

While we have designed our supercompiler with compilation speed in mind, we haven't focused on optimising the compiler -- all functions are implemented as simply as possible. Profiling shows that 80\% of the compilation time is spent simplifying expressions, as described in \S\ref{sec:simplify}. Our simplification method is currently written as a transformation that is applied until a fixed point is reached -- we believe the simplification can be implemented in one pass, leading to a substantial reduction in compile time.

We have implemented the termination check exactly as described in \S\ref{sec:term_rule}, traversing and comparing the entire history at each step. For our termination check it is simple to change the history to a mapping from a name bag to an integer (being the highest permitted cardinality) -- reducing the algorithmic complexity. We could also optimise the representation of names, using a single integer for both the function name and subexpression index.

\section{Related Work}

We first describe related work in the area of supercompilation, particularly what makes our supercompiler unique. We then describe some work from other areas, particularly work from which we have used ideas.

\subsection{Supercompilation}
\label{sec:comparison}

Supercompilation was introduced by \citet{supercompilation} for the Refal programming language \cite{refal}. Since this original work, there have been many suggestions of both termination strategies and generalisation/splitting strategies \cite{turchin:generalisation,sorensen:supercompilation,leuschel:homeomorphic}. The original supercompiler maintained both positive and negative knowledge \cite{secher:perfect_supercompilation}, however our implementation uses only positive knowledge (what a variable is, rather than what it cannot be). More recently, supercompilation has started to converge towards a common design, described in detail by \citet{klyuchnikov:hosc}, but which has much in common with the underlying design present in other papers \cite{me:supero,jonsson:supercompilation}.

Compared to an increasingly common foundation, our supercompiler is radically different. We have changed many of the ingredients of supercompilation (the treatment of let, termination criteria, how the termination histories are used), but have also changed the way these ingredients are combined (the manager). In particular, many of our choices would not work if applied in isolation to another supercompiler -- for example the termination criteria relies on the treatment of let.

\subsubsection{Let Expressions}

Compared to other supercompilers, our Core language requires many more let expressions. Previous supercompilation work has tended to ignore let expressions -- if let is mentioned the usual strategy is to substitute all linear lets and residuate all others. At the same time, movement of lets can have a dramatic impact on performance: carefully designed let-shifting transformations give an average speedup of 15\% in GHC \cite{spj:letfloating}.

Our previous work inlined all let bindings that it could show did not lead to a loss of sharing \cite{me:supero}. Unfortunately, where a let could not be removed, there was a substantial performance penalty. By going to the opposite extreme we are forced to deal with let bindings properly, making our new supercompiler both simpler and more robust.

\subsubsection{Termination Criteria}

The standard termination criteria used by supercompilers is the homeomorphic embedding \cite{leuschel:homeomorphic}. The homeomorphic embedding is a well-quasi ordering, from Kruskal's Tree Theorem \cite{kruskal:tree}. The criteria requires that for every infinite sequence $e_1,e_2 \ldots$ there exist indicies $i < j$ such that $e_j \ntriangleleft e_i$. The intuition of homeomorphic embedding is that $x \ntriangleleft y$ holds if by removing nodes from $y$ you cannot obtain $x$. Our termination ordering uses the idea of a well-quasi ordering, but with a very different ordering relation.

We are unaware of any other supercompilers that have assigned names to expressions, or that have used a bag based termination ordering (most use tree orderings, or sometimes cost models/budgets). Without our particular treatment of expressions as a set of let bindings, and our particular simplification rules, it is not possible to use our termination ordering. For example, if we ever inline let bindings then subexpressions would be changed internally, and a single name for each subexpression would no longer be sufficient.

In some cases, our ordering is certainly less restrictive than the homeomorphic embedding. The example in \S\ref{sec:term_example} would have stopped one step earlier with a homeomorphic embedding. Under a fairly standard interpretation of variable names and let expressions, we can show that our ordering is always less restrictive than the homeomorphic embedding -- although other differences in our treatment of expressions mean such a comparison is not necessarily meaningful. However, we did not choose our termination criteria to permit more expressions -- it was chosen for both simplicity and compilation speed.

We use two separate termination histories, one in |reduce| and another in |optimise| -- an idea suggested by \citet{me:thesis}, but not previously implemented. By separating the termination histories we gain better predictability, as |reduce| is not dependent on which functions have gone before. Additionally, the histories are kept substantially smaller, again improving compile-time performance. By splitting termination checks we also reduce the coupling between the separate aspects of supercompilation, allowing us to present a simpler manager than would otherwise be possible.

As a result of the changes to termination and the Core language, the operation for splitting when the termination check fails is radically different. In particular, we can use almost identical operations when either evaluation fails to continue, or the termination check fails.

\subsection{Partial evaluation}

There has been a lot of work on partial evaluation \cite{jones:partial_evaluation}, where a program is specialised with respect to some static data. Partial evaluation works by marking all variable bindings within a program as either static or dynamic, using binding time analysis, then specialises the program with respect to the static bindings. Partial evaluation is particularly appropriate for optimising an interpreter with respect to the expression tree of a particular program, automatically generating a compiler, and removing \textit{interpretation overhead}. The translation of an interpreter into a compiler is known as the First Futamura Projection \cite{futanama:projections}, and can often give an order of magnitude speedup.

Supercompilation and partial evaluation both remove abstraction overhead within a program. Partial evaluation is more suited to completely removing static data, such as an expression tree which is interpreted. Supercompilation is able to remove intermediate data structures, which partial evaluation cannot usually do.

\subsection{Deforestation}

Deforestation \cite{wadler:deforestation} removes intermediate lists from computations. This technique has been extended in many ways to encompass higher order deforestation \cite{marlow:higher_order_deforestation} and work on other data types \cite{coutts:string_fusion}. In many cases the gains from supercompilation are just particular forms of deforestation.

Probably the most practically applied work on deforestation uses GHC's rewrite rules to optimise programs \cite{spj:rules}. Shortcut deforestation rewrites many definitions in terms of |foldr| and |build|, then combines |foldr|/|build| pairs \cite{gill:shortcut_deforestation}. Stream fusion works similarly, but relies on |stream|/|unstream| rules \cite{coutts:stream_fusion}. In both cases the optimisation only applies to lists, and only to functions written in terms of the correct primitives. The advantage of supercompilation is that it applies to many types and functions, without any special effort from the program author.

\subsection{Lower Level Optimisations}

Our optimisation works at the Core level, but even once efficient Core has been generated there is still some work before efficient machine code can be produced. Key optimisations include strictness analysis and unboxing \cite{spj:unboxing}. In GHC both of these optimisations are done at the core level, using a core language extended with unboxed types. After this lower level core has been generated, it is then compiled to STG machine instructions \cite{spj:stg}, from which assembly code is generated. There is still work being done to modify the lowest levels to take advantage of the current generation of microprocessors \cite{marlow:pointer_tagging}. We rely on GHC to perform all these optimisations after our supercompiler generates a target program.

The GRIN approach \cite{grin} uses whole program compilation for Haskell. It is currently being implemented in the jhc compiler \cite{jhc}, with promising initial results. GRIN works by first removing all functional values, turning them into case expressions, allowing subsequent optimisation. The intermediate language for jhc is at a much lower level than our Core language, so jhc is able to perform detailed optimisations that we are unable to express.

\section{Conclusions and Future Work}

We have described a novel supercompiler, with a focus on simplicity, which can compile our benchmarks in a few seconds, and in some benchmarks offers substantial performance improvements over GHC alone. We see two main avenues for future work: increasing the range of benchmarks, and improving the runtime performance.

\subsection{More Benchmarks}

In order to run more benchmarks we need to automatically translate Haskell to our Core language. In previous papers we used the Yhc compiler to generate Core \cite{me:yhc_core}, but sadly Yhc is not maintained and no longer works. Given that our supercompiler relies on GHC to perform strictness analysis and generate native code, it seems sensible to use GHC to generate our Core language -- perhaps as a compiler plug-in, or working on external Core, or integrated into the compiler.

Our supercompiler processes the whole program in one go, which naturally leads to questions of scalability. In the tests we have run we have not had a problem with compilation time, but it is something to be aware of as benchmarks increase in size. We believe that our supercompiler could be sped up massively, using some of the techniques mentioned in \S\ref{sec:benchmarks_compile}. In addition, we could split programs into separate components by defining some functions to be primitive -- although this will remove optimisation potential.

\subsection{Runtime Performance}

Our performance results are good, but there are always opportunities to improve. We currently rely on GHC's strictness analysis to run after we have optimised the program, but by integrating a strictness analysis we may be able to do better. The most common uses of GHC's rules engine, particularly list/stream fusion, are automatically performed by our supercompiler. However, some transformations such as replacing |head . sort| with |minimum|, are too complex to automatically infer. It may be of benefit to integrate a rules engine in to our supercompiler.

In some cases the author of a program has a particular idea about some intermediate data structure they expect to be eliminated. If these structures remain in the optimised program the performance penalty is sometimes dramatic. Perhaps a user could mark some values they expect to be removed, and then be warned if they remain.

\subsection{Conclusions}

Supercompilation is a powerful technique which generalises many of the transformations performed by optimising compilers. We were initially drawn to supercompilation for two reasons. Firstly, all intermediate values have the potential to be eliminated, regardless of their type or the functions which operate on them. Secondly, supercompilation is a single-pass optimisation, avoiding the tricky problem of ordering compiler phases for best optimisation. With these advantages supercompilation has the potential to drastically simplify an optimising compiler, while still achieving great performance. Our supercompiler builds on these advantages, rethinking supercompilation to make it simpler and improve compilation times.

\subsection*{Acknowledgements}

I would like to thank Max Bolingbroke, Jason Reich, Simon Peyton Jones and Peter Jonsson for helpful discussions. Thanks to Ketil Malde for providing further inspiration to continue researching supercompilation. Thanks to Max Bolingbroke and Mike Dodds for comments on earlier drafts.

\bibliographystyle{plainnat}
\balance
\bibliography

\end{document}
