\documentclass{llncs}

\usepackage{url}

%include polycode.fmt
%include supero.fmt

\begin{document}

\title{Fast Haskell}

\author{Neil Mitchell and Colin Runciman}

\institute{University of York, UK, \url{http://www.cs.york.ac.uk/~ndm}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Haskell is a high level functional language, with features like higher order functions and lazy evaluation, which allow succinct representations of programs. These high level features can result in poor runtime speed of generated code, if care is not taken. This paper presents a whole program approach to transformation, which enables significant speed ups.
\end{abstract}

\section{Introduction}

Haskell can be used to write high level specifications of programs, which can run. Take for example the task of counting the number of words in a file read from the input. In Haskell this program is a single line:

\begin{code}
main = print . length . words =<< getContents
\end{code}

The |getContents| function reads the input as a list of characters, |words| splits this list into a list of words, |length| counts the number of words, and finally |print| writes the value to the screen. The use of |(=<<)| is merely to do sequence the actions, and can be ignored for those unfamiliar with Haskell and monadic IO.

Compared to a C program to perform this task (see Figure \ref{fig:c_words}), the Haskell version is more concise, more specification orientated and easier to convince others of its correctness. Unfortunately with all these good points,  it is also three times slower than the C version.

\begin{figure}[tb]
\begin{verbatim}
int main()
{
	int i = 0;
	int c, last_space = 1, this_space;
	while ((c = getchar()) != EOF) {
		this_space = isspace(c);
		if (last_space && !this_space)
			i++;
		last_space = this_space;
	}
	printf("%i\n", i);
	return 0;
}
\end{verbatim}
\caption{Word counting in C}
\label{fig:c_words}
\end{figure}

There are several reasons that the Haskell version does not perform at the same speed as the C version:

\begin{description}
\item[Intermediate Lists] One of the reasons for the slowdown in the Haskell version is that many intermediate lists are produced and consumed as the program proceeds. The |getContents| function produces a list of characters, |words| consumes this list and produces a list of a list of characters, |length| then consumes the outermost list. The equivalent C program has no allocation.
\item[Higher Order Arguments] The |words| function is defined using the function |span|, which takes a predicate and splits the input list when the predicate becomes true. The predicate is passed as a higher order function, which is constant on all applications.
\item[Laziness] The Haskell program proceeds in a lazy manner, demanding one character from |getContents|, then processing it with each of the functions in the pipeline. At each stage, a lazy thunk for the remainder of the function is created.
\end{description}

By using the optimiser developed in this paper we can eliminate all these overheads, resulting in a program that performs \textit{faster} than the equivalent C version. The central idea of the optimiser is that we attempt to evaluate as much of the program at compile time as possible, leaving a residual program consisting of only actions dependent on the input data.

Our goal is to make high-level Haskell programs perform as fast as low-level Haskell programs, eliminating the need to optimise Haskell programs in the kind of detail currently required for high performance programs. We require no annotations on any part of the program, even library functions, in contrast to other approaches.

\subsection{Roadmap}

This paper first introduces a Core language

\section{Core Language}

\begin{figure}
\begin{code}
type FunName  = String
type VarName  = String
type ConName  = String

type Core = [FunDef]

data FunDef  =  FunDef FunName (Maybe Expr)

data Expr  =  Fun   FunName
           |  Con   ConName
           |  Var   VarName
           |  App   Expr [Expr]
           |  Lam   [VarName] Expr
           |  Let   [(VarName,Expr)] Expr
           |  Case  Expr [(Pat, Expr)]
           |  Lit   Lit

data Lit = ... -- literal values

data Pat = PatCon  ConName [VarName]
         | PatLit  Lit
         | PatDefault
\end{code}
\caption{Core syntax}
\label{fig:core}
\end{figure}

All our optimisations operate on a standard Core language, given in Figure \ref{fig:core}. This language is both higher order and lazy, but lacks much of the syntactic sugar found in Haskell. Pattern matching occurs only in case expressions, and all case expressions are exhaustive. Full details on the Core language are given in \cite{me:yhc_core}.

A program (the |Core| type) is a list of function definitions (the |FunDef| type). A function definition is either a primitive (in which case the associated expression is not present), or a definition introduced by the program. An expression (the |Expr| type) consists of many standard elements in a functional language, functions (|Fun|) and constructors (|Con|), application (|App|), case/let/lambda expressions and literals. A case expression associates a list of patterns with expressions to execute if the pattern is matched.

To generate core representations of programs, it is natural to start with a full Haskell compiler, and we chose Yhc \cite{me:yhc_core}, a fork of nhc \cite{nhc}. The core language of Yhc, PosLambda, was intended only as an internal representation, and exposes certain details that are specific to the compiler. We have therefore introduced a new Core language to Yhc, to which PosLambda can easily be translated. All names are fully qualified. Haskell's type classes have been removed (see \S\ref{sec:dict}). Only top-level functions remain; all local functions have been lambda lifted. All constructor applications are fully saturated. Pattern matching occurs only in case expressions; alternatives match only the top level constructor and are exhaustive, including an |error| alternative if necessary.

The primary difference between our Core language and that of a compiler such as GHC is that ours is untyped. The Core is generated from well-typed Haskell, and can be guaranteed not to go wrong with a type error. All the transformations we implement could be implemented equally in a typed Core language. We prefer to work in an untyped Core language for simplicity of implementation, but realise that the benefits of a closer guarantee of correctness from a typed Core may attract mature compilers to use a typed core.

In order to avoid inadvertant name clashes in generated code, we demand the invariant that all variables in a program are unique.

\section{Optimisation}

Our optimisation procedure takes as input a program in Core, and produces a program in Core that has the same effect as the original -- although hopefully which performs better. We do not make small changes to the original program, but instead evaluate the program at compile time, leaving a residual program which cannot be evaluated.

Our optimisation proceeds by starting at the |main| function, and optimising the expression using the evaluation rules in Figure \ref{fig:evaluate}, and the simplification rules in Figure \ref{fig:simplify}. Once no more rules are applicable, the outermost element in the Core expression become part of the residual program. All the inner expressions are assigned names, which become functions in the residual program. Each of these new functions are then optimised as before.

Each function in the resultant program has a uniquely generated name, an expression in the source program, and an expression in the residual program.

The optimisation of an expression is performed with the |_E| function. This function proceeds much like evaluation at run time, but stops if the most next expression to reduce is either a free variable, or a primitive, or a CAF (constant applicative form -- see \ref{sec:caf} for more details).


\begin{figure}[tb]
\begin{code}
_E\<case x of alts_  \> =  case _E\<x\> of alts_
_E\<let v = e in x   \> =  let v = _E\<e\> in _E\<x\>
_E\<x xs_            \> =  _E\<x\> xs_
_E\<f                \> =  _E\<unfold f\>
                           where f {-" \hbox{ is a non-primitive, non-CAF function} "-}
_E\<x                \> =  x
\end{code}
\caption{Evaluate function, |_E|}
\label{fig:evaluate}
\end{figure}

\begin{figure}[tb]
\begin{code}
case (case x of ...) of ...

case C ... of ...

case v of {C xs_ -> y; ...} => case v of {C xs_ -> y[v/C xs_]; ...}

let v = e in x => x[v/e]
   where e {-" \hbox{ is a lambda, constructor, variable, used once etc.} "-}
\end{code}
\caption{Simplifications}
\label{fig:simplify}
\end{figure}

To


The generated program is simply an associate between function names and expressions. We start from the expression in main, optim

Take the procedure |main| and evaluate it. Then peel off one level of the residual program.





The optimisation takes a Core language to a Core language. Works from one program to another. Describe the optimisation in detail, leaving out termination issues.

\subsection{Let Bindings}

Let bindings are optimised and reduced if possible. They encode sharing so simply inlining a let binding may duplicate an arbitrary amount of computation. 

\subsection{CAF's}
\label{sec:caf}



\subsection{Termination}

How we bound the terminated language.

\section{Small Examples}

To show some examples of the kind of optimisations

\begin{code}
map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys

main xs = map (+1) (map (*2) xs)
\end{code}

\begin{code}
main xs = map xs

map xs = case  xs of
               []    -> []
               y:ys  -> (y*2)+1 : map ys
\end{code}






\section{Benchmarks Versus C}

The most comprehensive inter-language benchmarking effort is the Programming Language Shootout \cite{shootout}, where a variety of tasks are benchmarked in multiple languages with multiple implementations. Unfortunately, in order to ensure a balanced comparison, many of the restrictions on the benchmarks directly harm lazy languages. Inspired by their approach, we have attempted to define some simpler benchmarks on which it is possible to directly compare Haskell to C.

The set of benchmarks we have chosen are inspired by the Unix \texttt{wc} command -- namely character, word and line counting. In all cases we require to read a file from the standard input, and perform the computation. In order to ensure that we test computation speed, not IO speed (which is usually a product of the underlying buffering system, rather than the compilation prowess) we demand that all input is read using the standard C |getchar| function only. Any buffering improvements, such as chunked reading or memory mapping of files, could be performed equally in all compilers.

All the C versions are transformed into state machines, where characters are read in a loop, and an accumulator is kept -- much the same as in Figure \ref{fig:c_words}. The Haskell versions all follow the same pattern as in the Introduction, merely replacing |words| with |lines|, or removing the |words| function for character counting.

We performed all benchmarks on a Windows XP, 3GHz hyperthreaded, 1Gb RAM machine. The C versions used GCC version n.n with -O3. The pure Haskell version used GHC 6.6.1 with -O2. The Supero version was compiled using our optimiser, then written back as a Haskell file, and compiled once more with GHC 6.6.1 with -O2.

The result of the benchmarks are given here.

\subsection{Identified Haskell Speedups}

In our exploration of the performance bottlenecks in the Haskell variant we identified two reasons for Hsakell underperforming.

The first is that |isSpace| in Haskell is significantly more expensive than the C |isspace|. The solution we to use an FFI call to the C |isspace| in all cases, removing this source of variance. A GHC bug has been filed about the slow performance of |isspace|, so hopefully this will improve in time.

The second is that the |words| function in Haskell performs two additional |isspace| tests per word read. We have transformed the |words| function to eliminate these unnecessary tests, and have submitted this code for inclusion in a future release of the standard libraries.

\subsection{Potential Haskell Speedups}

There are a number of reasons that the Haskell underperforms relative to the C version.

Lack of strictness information

Heap checks

Stack checks

\subsection{Haskell Outperforms C}

The reason Haskell outperforms C is that the C has redundant flow information. By moving the boolean into the program counter you can go faster. Unfortunately releasing this flow control in C is challenging! Using the |goto| keyword is not appropriate, as typically this turns off many critical optimisations in the C compiler. It can be done, using a nested |while| loop, in a function, and using |return| to break from the inner loop. This solution would not scale to a three-valued control structure, but could be used to achieve higher performance in word counting -- at the cost of code complication and duplication.

\section{Benchmarks Versus Haskell}

The standard benchmark suite in Haskell is the nofib suite, and more recently the nobench suite has added a few benchmarks and removed many.

\section{Related Work}

Partial evaluation - not really that related.

Supercompilation - quite related.

Deforestation - quite related.

Other transformations - we generalise SpecConstr, inlining etc.

\section{Conclusion}


\end{document}
