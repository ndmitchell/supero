\documentclass[draft]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{balance}

\include{paper}
%include paper.fmt

% Consistency:
% v,w,x,y,xs,ys,z,zs are all variables
% e is an expression
% p is a pattern
% f,g are functions
% m,n are lengths
% i,j are indexes

%format <? = "[\!["
%format ?> = "]\!]"
%format += = "+\!\!\!\!="
%format ==> = "\Longrightarrow{}"

%format w_1  = "\Varid{w_1}"
%format w_2  = "\Varid{w_2}"
%format w_3  = "\Varid{w_3}"
%format w_12  = "\Varid{w_12}"
%format w_123  = "\Varid{w_123}"
%format v_3  = "\Varid{v_3}"
%format v_4  = "\Varid{v_4}"
%format map_1 = "\Varid{map_1}"
%format map_2 = "\Varid{map_2}"
%format e_1' = "\Varid{e_1^{\prime}}"
%format e_2' = "\Varid{e_2^{\prime}}"
%format e_m' = "\Varid{e_m^{\prime}}"
%format e_1 = "\Varid{e_1}"
%format p_m = "\Varid{p_m}"
%format s_m = "\Varid{s_m}"
%format v_j = "\Varid{v_j}"


\newcommand{\unknown}{XXX}

\begin{document}

\conferenceinfo{ICFP 2010}{}
% \CopyrightYear{2009}
% \copyrightdata{978-1-60558-508-6/09/09}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Simpler Supercompilation}
% \subtitle{}

\authorinfo{Neil Mitchell}
           {Standard Chartered, UK}
           {\verb"ndmitchell@gmail.com"}

\maketitle

\begin{abstract}
Supercompilation is a program optimisation technique, which can often eliminate the overhead of programmer abstractions. We present a new design for a supercompiler, rethinking many of the decisions that have become common in previous supercompilation work. The result is a supercompiler that we find simpler, and offers good performance both at compile time and runtime. We have implemented our supercompiler and benchmarked it on a selection of programs from the nofib benchmark suite, which run faster than both GHC and our previous supercompiler. We then put our supercompiler to practical use, optimising an HTML parsing library, and checking the equivalence of open expressions used as rewrite rules.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
Languages

\keywords
Haskell, optimisation, supercompilation

\section{Introduction}

\todo{perhaps switch to tagsoup as the openning example, so I can come back to it?}

Supercompilation is a technique for program optimisation, that is particularly suited to removing the overhead introduced by abstractions. Consider a program that counts the number of words read from the standard input -- in Haskell \cite{haskell} this can be compactly written as:

\begin{code}
main = print . length . words =<< getContents
\end{code}

Reading the program right to left, we first read the standard input as a string (|getContents|), then split it in to words (|words|), count the number of words (|length|), and print the result (|print|). An equivalent C program is unlikely to use such a high degree of abstraction, and is more likely to get characters and operate on them in a loop using some state which is updated.

Sadly the C program is three times faster, even using the advanced optimising compiler GHC \cite{GHC}. The abstractions that make the program concise have a significant runtime cost. In a previous paper on supercompilation \cite{me:supero} we showed how supercompilation can remove these abstractions, to the stage where the Haskell is even faster than the C version (by about 6\%). In the Haskell program after optimisation all the intermediate lists have been removed, and the |length . words| part of the pipeline is translated into a state machine.

One informal description of supercompilation is that you simply ``run the program at compile time''. This description leads to two questions -- what happens if you are blocked on information only available at runtime, and how do you ensure termination? Answering these questions provides the design for a supercompiler. This paper strives to provide simple answers, but which do not reduce the optimisation power. In doing so, we make a large number of departures from the current concensus on supercompiler design.

We hope that from the descriptions given in this paper a user is able to write their own supercompiler. Indeed, we have written a supercompiler following this design which is available online \unknown{}.

\subsection{Contributions}

Our primary contribution is the design of a new supercompiler (\S\ref{sec:method}). Our supercompiler has many differences from previous supercompilers (\S\ref{sec:comparison}), including a new core language, a substantially different treatment of let expressions, a transformation that never manipulates inside expressions and an entirely new termination criteria. The result is relatively simple, yet still powerful.

In addition, we make the following contributions:

\begin{itemize}
\item We give examples of how our supercompiler performs (\S\ref{sec:examples}), including how it subsumes list fusion and specialisation, and what happens when the termination criteria are needed.
\item We benchmark the supercompiler on a small range of examples (\S\ref{sec:benchmarks}) -- our supercompiler achieves an improvement of \unknown{}\% when used in combination with GHC, as opposed to GHC alone.
\item We use our supercompiler to optimise an HTML parser, showing a \unknown{}\% speed increase (\S\ref{sec:tagsoup}). This HTML parser is run daily on 40Gb of input, and previously consumed over two hours of CPU time per day.
\item We use our supercompiler to prove the equality of open expressions for a program rewriter (\S\ref{sec:hlint}). As a result we found one bug.
\end{itemize}

\section{Method}
\label{sec:method}

This section describes our supercompiler. We first present a Core language (\S\ref{sec:core}), along with simplification rule (\S\ref{sec:simplify}). We then present the overall algorithm (\S\ref{sec:manager}), which uses answers to the following questions:

\begin{itemize}
\item How do you evaluate an open term? (\S\ref{sec:eval})
\item What happens if you can't evaluate an open term further? (\S\ref{sec:eval_split})
\item How do you know when to stop? (\S\ref{sec:term})
\item What happens if you have to stop? (\S\ref{sec:term_split})
\end{itemize}

Throughout this section we use the following example:

\begin{code}
root g f x = map g (map f x)

map f x = case x of
    []    -> []
    y:ys  -> f y : map f ys
\end{code}

This example applies |map| twice -- the expression |map f x| produces a list that is immediately consumed by |map g|. A good supercompiler should remove the intermediate list.

\subsection{Core Language}
\label{sec:core}

\begin{figure}
\begin{code}
type Var  = String -- variable names
type Fun  = String -- function names
type Con  = String -- constructor names

data Exp  =  Var Var
          |  Fun Fun
          |  Con Con [Var]
          |  App Var Var
          |  Let [(Var,Exp)] Var
          |  Case Var [(Pat, Exp)]

type Pat = Exp -- restricted to Con
\end{code}
\caption{Core Language}
\label{fig:core}
\end{figure}

Our Core language for expressions is given in Figure \ref{fig:core}, and has much in common with Administrative Normal Form \cite{flanagan:continuations}. There are a few points to note:

\begin{itemize}
\item We require variables in many places that would normally permit expressions. A standard Core language (such as from \citet{ghc_core}) could be translated to ours by inserting let bindings.
\item In many Core languages a distinction is made between let and letrec. We allow expressions bound at a let to refer to variables bound at the same let, but disallow recursion. An alternative description is that we allow recursive let, but only if there is an equivalent nested non-recursive let that can represent the same expression. For example we allow |let x = y; y = C in C| but not |let x = y; y = x in C|.
\item We don't have default patterns in case expressions, or literals. Both can be added (see \S\ref{sec:extensions}), but are of little interest when describing a supercompiler.
\end{itemize}

We write expressions using standard Haskell syntax (i.e. |let| for |Let|, |case/of| for |Case| etc.). Rewriting our |map/map| example in our Core language gives:

\begin{code}
root g f x =  let  v_1 =  map
                   v_2 =  let  w_1 = map
                               w_2 = w_1 f
                               w_3 = w_2 x
                          in   w_3
                   v_3 =  v_1 g
                   v_4 =  v_3 v_2
              in   v_4

map f x = case x of
    [] ->   let  q = []
            in   q
    y:ys -> let  v_1 = f y
                 w_1 = map
                 w_2 = w_1 f
                 w_3 = w_2 ys
                 q = (:) v_1 w_3
            in   q
\end{code}

Our Core language is rather verbose, so we sometimes use a standard Core language, that could always be translated to our Core language. For example with |map|:

\begin{code}
map f x = case x of
    [] -> []
    y:ys -> f y : map f ys
\end{code}

Here we assume that if we were interested in the actual bodies in the alternative we'd translate it.


\subsection{Simplified Core}
\label{sec:simplify}

We now define a simplified form for our Core language. When working with Core expressions we assume they are always simplified, and we can easily simplify Core expressions. Our simplified form requires that all variables are unique, that the root of a function be a |Let|, and that the expressions bound at the root |Let| must not have the following form:

\begin{itemize}
\item |App v w|, where |v| is bound to a |Con| -- the |App| can be replaced with a Con of higher arity.
\item |Case v w|, where |v| is bound to a |Con| -- the |Case| can be replaced with the appropriate alternative.
\item |Let vs w| -- the nested bindings can be brought to the root |Let|, renaming variables if necessary.
\item |Var v| -- we can remove the binding by replacing the bound variable with |v| everywhere.
\end{itemize}

\todo{also eliminate all unused variables, and the root may be zero bindings.}

For example the simplified version of |root| is:

\begin{code}
root g f x =  let  v_1 = map
                   w_1 = map
                   w_2 = w_1 f
                   w_3 = w_2 x
                   v_3 = v_1 g
                   v_4 = v_3 w_3
              in   v_4
\end{code}

For termination we also require that binding's are given in a predictable order, and that all variable names are normalised. We don't typically write functions in this style, since it makes them harder to read, but we could. The termination relies on it!

\subsection{Manager}
\label{sec:manager}

\begin{figure}
\begin{code}
data Lambda = Lambda [Var] Exp
type Env = Fun -> Lambda
data Tree = Tree  {pre :: Lambda
                  ,gen :: [Fun] -> Lambda
                  , children :: [Tree]}

supercompile :: Env -> [(Fun,Lambda)]
supercompile env =
    assign $ flatten $ optimise env $ env "main"

optimise :: Env -> Lambda -> Tree
optimise env = f newTerm
    where f t x = Tree x gen (map (f (t+=x)) cs)
            where (gen,cs) = if terminate t x  then stop t x
                                               else reduce env x

reduce :: Env -> Lambda -> ([Fun] -> Lambda, [Lambda])
reduce env = f newTerm
    where f t x | terminate t x = stop t x
                | Just x' <- step env x = f (t += x) x'
                | otherwise = split x

flatten :: Tree -> [Tree]
flatten = nubBy (\x y -> pre x == pre y) . f []
    where f seen t  =  if pre t `elem` seen then [] else
                       t : concatMap (f (t:seen)) (children t)

assign :: [Tree] -> [(Fun,Lambda)]
assign ts = [(f t, gen t (map f (children t))) |  t <- ts]
    where f = flip lookup . zip freshNames (map pre ts)
\end{code}
\caption{The |supercompile| function.}
\label{fig:manager}
\end{figure}

Our supercompiler is based around a manager, that calls the other parts of the supercompiler. The manager has two main purposes, to ensure termination (by calling the termination functions) and to tie back recursive functions. In our experience the act of tieing recursion is often one of the trickiest aspects when writing a supercompiler, so we deliberately choose to give all the details. The complete code of our manager is given in Figure \ref{fig:manager}. We start by giving an intuition for how the manager works, then describe each function individually.

The supercompiler starts with a source program, and generates a target program. Expressions from these programs are distinct -- target expressions cannot refer to source functions and vice versa. The source and target program are semantically identical, but it is hoped the target program will be more optimised. We use the type |Lambda| to represent an expression which may be enclosed within a lambda, and the type |Env| to represent a mapping from function names to lambdas. While we can think of |Lambda| as a function, another interpretation is that |Exp| is an \textit{open expression}, whose free variables are listed in |Lambda|, turning it in to a \textit{closed expression}.

The manager first builds a tree tree, where each node in the tree has a source expression (|pre|) which represents it's semantics. The target expression associated with the source expression may call other target functions, however these functions do not yet have names. Therefore, we store target expressions as a generator that when given the function names produces the target expression (|gen|), and a list of trees representing the target functions it needs to call (|children|). We then flatten this tree, ensuring identical functions are only represented once, and assign names to get the target program. If a target function is recursive then the initial tree will be infinite, but the flattened tree will always be finite due to the termination criteria.

We rely on a termination scheme, which we define in \S\ref{sec:term}. The function |newTerm| constructs a new termination context, |(+=)| adds an expression to the termination context, and |terminate t x| returns |True| if it is not permissible to continue given a particular expression and termination context. We demand that for any infinite sequence of expressions, |terminate| will eventually return |True| if we keep adding with |(+=)|. More concretely, |terminates newTerm xs|, where |xs| is an infinite list, will always return |True| rather than diverging:

\begin{code}
terminates t (x:xs) = terminate t x || terminates (t += x) xs
\end{code}

\newcommand{\function}[1]{\paragraph{\textsf{#1:}}\hspace{-3mm}}

\function{supercompile} This function puts all the parts together. Reading from right to left, we first generated an infinite tree by optimising the expression |main|, we then flatten the tree to a finite number of resultant functions, and finally assign names to each of the result functions.

\function{optimise} This function constructs the infinite tree of result functions. While the tree is infinite, we demand that on any path from the root of the tree we must encounter the same |pre| value more than once. To ensure this criteria we use a termination context. If we are forced to terminate we call |stop|, which splits the function into several functions and ensures each of them pass the termination criteria. If the termination criteria do not force us to stop, then we call |reduce| to perform reductions on the expression.

\function{reduce} This function optimises a function by repeatedly evaluating it calling |step|. If we can't evaluate any further we call |split|. We use a local termination context to ensure termination of this part.

\function{flatten} This function takes a tree and extracts a finite number of functions from it, assuming the termination criteria given in |optimise|. It is important to note that of the functions available when we call |flatten| we can think of non-deterministically picking one for each |pre| value. Given that each function may have been compiled with a different termination context in |optimise| these may have different results -- but all are semantically equivalent.

\function{assign} This function assigns names to each target function, and constructs the target expressions by calling |gen|. We assume the function |freshNames| returns the infinite list of fresh variable names.

\subsubsection{The Example}
\label{sec:manager_example}

So, revisiting our initial example, we first call |optimise| with:

\begin{code}
\g f x -> map g (map f x)
\end{code}

The termination context is empty, so we call |reduce| which calls |step| until we reach the code:

\begin{code}
\g f x ->  let  v = case  w of
                          [] -> []
                          y:ys -> g y : map g ys
                w = case  x of
                          [] -> []
                          z:zs -> f z : map f zs
           in   v
\end{code}

Now we can't step any further since what we really need is the result of |x|. We therefore split to achieve:

\begin{code}
\g f x ->  case  x of
                 []    -> <? let v = ...; w = ...; x = [] in v ?>
                 z:zs  -> <? let v = ...; w = ...; x = z:zs in v ?>
\end{code}

We then optimise the previous expression, but with |x| bound to one of the alternatives. To represent this we use the |<? bullet ?>| notation, this entire expression represents a |([Fun] -> Lambda, [Lambda])|. The |<? bullet ?>| parts are replaced with function names, all the free variables needed in the child expressions become lambdas. When optimising, the case where |x| is empty is |[]| is simple -- the simplification rules from \S\ref{sec:simplify} kick in to immediately give |[]| as the result. For the other case, we first get:

\begin{code}
\g f z zs ->
    let  v = case  w  of {[] -> []; y  :ys  -> g  y  : map g  ys  }
         w = case  x  of {[] -> []; z  :zs  -> f  z  : map f  zs  }
         x = z:zs
    in   v
\end{code}

Which then get's simplified to:

\begin{code}
\g f z zs ->  let  v = q : qs
                   q = g y
                   qs = map g ys
                   y = f z
                   ys = map f zs
              in   v
\end{code}

Now we try step, but this stops immediately, as the root of this expression is a |(:)| that can't be evaluated. We therefore call split and end up with:

\begin{code}
\g f z zs ->  let  q = <? g (f z) ?>
                   qs = <? map g (map f zs) ?>
              in   q : qs
\end{code}

Now when optimising |g (f z)| we will get no optimisation, as there is no information known. When optimising |map g (map f zs)| we will immediately hit the termination criteria, and |stop| will be called. However, the |flatten| function will notice that in both times we start from the same expression, and therefore give them both the same name creating a recursive function. In this example the calls to |map/map| have different variable names, so don't look identical, but if we reassigned the variables in a systematic way after each step we would (but it's also a lot harder to follow if the variables keep getting renamed!). The end function, with names assigned, is:

\begin{code}
main g f x = case x of
    [] -> []
    z:zs -> g (f z) : main g f zs
\end{code}

Note that the end version has exactly one case and has automatically fused the intermediate list. Here we've applied a number of simplifications listed in \S\ref{sec:postprocess}.

\subsection{Evaluation}
\label{sec:eval}

\begin{figure}
\begin{code}
force :: Exp -> Maybe Var
force (Case v _  ) = Just v
force (App v _   ) = Just v
force (Var v     ) = Just v
force _ = Nothing

stack :: Lambda -> [(Var, Exp)]
stack (Lambda _ (Let bind v)) = f v
    where f v = case  lookup v bind of
                      Nothing  -> []
                      Just x   -> maybe [] f (force x) ++ [(v,x)]
\end{code}
\caption{Function to compute the evaluation stack.}
\label{fig:stack}
\end{figure}

Evaluation is based around the |step| function. Given an expression, |step| inlines a function and returns |Just|, or if no function can be inlined returns |Nothing|. To choose which function to inline, we look at which function would be entered next if we were simply evaluating the expression. We replace this function with it's body, provided it has enough arguments not to result in a lambda.

To be more precise, we define the evaluation stack of an expression as the sequence of let bindings that would be evaluated in order, along with their associated expressions. A function to calculate the stack is given in Figure \ref{fig:stack}. Looking at the original example:

\begin{code}
\g f x =  let  v_1 = map
               w_1 = map
               w_2 = w_1 f
               w_3 = w_2 x
               v_3 = v_1 g
               v_4 = v_3 w_3
          in   v_4
\end{code}

The evaluation stack for this expression will be |[v_1,v_3,v_4]|. The stack is computed from the right, starting at the body of the let. From |v_4|  we arrive at the expression |v_3 w_3|, which will need to first evaluate |v_3|. Similarly the evaluation of |v_3| will require evaluating |v_1| first. Therefore, to evaluate this expression we will start by evaluating |v_1|, and thus |map|.

We can define the step function as matching the following stack configuration:

\begin{code}
stack[0]              = (v_1, Fun x)
stack[i `elem` 1..n]  = (v_i, App _ w_i)
    where  Lambda args bod = env x
           n = length args
\end{code}

This configuration requires the top of the stack to be a function |x|, with |n| arguments. We then require the next |n| entries on the stack to be |App| values. Given this stack configuration we can replace the variable |v_n| with:

\begin{code}
Let (zip args (map Var [w_1..w_n])) bod
\end{code}

For simplicity we assume |args| are distinct from |w_1..w_n|, if this is not the case than |args| and |body| can be renamed. After changing |v_n| we apply the simplification rules (\S\ref{sec:simplify}).

Going back to our example, we'd replace |v_4| with:

\begin{code}
let  f = g
     x = w_3
in   case  x of
           [] -> []
           y:ys -> f y : map f ys
\end{code}

The simplification will immediately eliminate the |f| and |x| bindings.

\subsection{Evaluation Splitting}
\label{sec:eval_split}

If evaluation cannot proceed we split the expression to produce a target function, and a list of children expressions for further optimisation. When splitting an expression there are three concerns:

\paragraph{Permit further optimisation:} When splitting we hope that the construct that is blocking evaluation will be placed in the target expression, and that the children will be suitable for further evaluation.

\paragraph{No loss of sharing:} A variable may not be duplicated into multiple children expressions, if that means that at runtime it will be evaluated multiple times. In essence, the target program cannot evaluate some functions more than the source program.

\paragraph{Keep expressions together:} Every time we separate expressions by turning a bound variable (whose right-hand side is available for optimisation) in to a free variable we have lost some optimisation opportunities. In many cases this goal is in direct contradiction with no loss of sharing, in which case we always favour no loss of sharing.

We write the residual expression in unrestricted Core language, which we assume is transformed in to our Core language. We split in three different ways depending on the type of expression at the top the stack (as defined by Figure \ref{fig:stack}). We now describe the three different ways to split, starting with examples, then defining the rules.

\subsubsection{Case Expression}

If the top of the stack is a case expression then we make the target a case expression, and under each alternative we create a child expression identical to the original expression, but with the case scrutinee bound to the appropriate pattern. For example, given:

\begin{code}
\x ->  let   v = case  x of
                       [] -> []
                       y:ys -> add y ys
       in    v
\end{code}

We produce the residual expression:

\begin{code}
\x ->  case x of
       []    -> <?  let  v =  case x of [] -> []; y:ys -> add y ys
                         x =  []
                    in   x ?>
       y:ys  -> <?  let  v =  case x of [] -> []; y:ys -> add y ys
                         x =  y:ys
                    in   x ?>
\end{code}

We take each expression in |<? bullet ?>| brackets, and simplify it. We then take all the free variables bound in the expression and passing them as arguments. We then put them back together with function names. For example, the first variable immediately simplifies to |[]|. This expression takes no variables. Looking more closely at the second alternative, we start with the lambda expression:

\begin{code}
\y ys ->  let  v = case x of [] -> []; y:ys -> add y ys
               x = y : ys
          in v
\end{code}

This lambda expression immediately simplifies to:

\begin{code}
\y ys ->  let  v = add y ys
          in v
\end{code}

One important point is that for the first alternative we \textit{do not} pass in the variables |y| and |ys| -- these variables are not included in the lambda. In general we do not pass onwards any free variables which are obviously redundant after the simplification rules have been applied. By making this restriction we ensure that the number of free variables in a function cannot grow without bound -- as the expressions are bounded and therefore they can only use a finite number of free variables. We limit the free variables for all types of split operation.

The general rule is that the target is the case on the top of the stack, and the alternatives are the entire expression but with the scrutinee variable bound to the associated patterns. More generally, if |s_1| is the top of the stack:

\begin{code}
\free ->  let  s_1  = case x of p_1 -> e_1' ; p_m -> e_m'
               v_1  = e_1
               v_n  = e_n
          in   v
\end{code}

\noindent becomes:

\begin{code}
\free -> case x of
    p_1 -> <? let  s_1 = case x of p_1 -> e_1'; p_m -> e_m'
                   v_1 = e_1; v_n = e_n; x = p_1 in v ?>
    p_m -> <? let  s_1 = case x of p_1 -> e_1'; p_m -> e_m'
                   v_1 = e_1; v_n = e_n; x = p_m in v ?>
\end{code}

\subsubsection{Function}

If the top of the stack is a function, that means that it must have insufficient arguments for the evaluation to take place. To deal with this under application we need to introduce a lambda. When introducing a lambda it is important that we do not reduce the sharing.

At the point we are stopping the set of variables bound in the let can be divided in to those which are on the stack and those which are not. For each variable not on the stack, we add the stack variables with their expressions as bindings. For the body of the let, we add a lambda along with all the stack variables and their bodies. To give an example, imagine we have:

\begin{code}
\x ->  let  s_1 = add
            s_2 = s_1 v_2
            v_1 = expensive
            v_2 = v_1 x
       in   s_2
\end{code}

It would certainly be tempting to rewrite |\x -> ...| as |\x y -> ...|, but this potentially duplicates the expensive computation of |v_2|. Instead we look at the variables on the stack (|s_1| and |s_2|) and duplicate their bindings at every site:

\begin{code}
\x ->  let  v_1 =  let s_1 = add; s_2 = s_1 v_2 in expensive
            v_2 =  let s_1 = add; s_2 = s_1 v_2 in v_1 x
       in   \y ->  let s_1 = add; s_2 = s_1 v_2 in s_2 y
\end{code}

We immediately discard any stack variables which introduce recursion (i.e. |s_2| bound under |v_2|), then simplify all the bindings:

\begin{code}
\x ->  let  v_1 =  expensive
            v_2 =  v_1 x
       in   \y ->  let s_1 = add; s_2 = s_1 v_2 in s_2 y
\end{code}

We could now produce the residual expression:

\begin{code}
\x ->  let  v_1 =  <? expensive ?>
            v_2 =  <? v_1 x ?>
       in   \y ->  <? let s_1 = add; s_2 = s_1 v_2 in s_2 y ?>
\end{code}

However, note that we have now split the bindings for |v_1| and |v_2| apart, when there is no real need. We therefore move binding |v_1| under |v_2|, because it is only referred to by |v_4|, to give:

\begin{code}
\x ->  let  v_2 =  <? let v_1 = expensive in v_1 x ?>
       in   \y ->  <? let s_1 = add; s_2 = s_1 v_2 in y ?>
\end{code}

We now optimise the body of |v_2| and the under the lambda which introduces |y|. Note that assuming |add| has arity two, we'll now be able to inline |add|. We loose the sharing on some PAP's, but they aren't too important, since they didn't do anything good anyway. Be careful that |y| must be the last variable in the inner lambda, since then we can eliminate it without requiring a language supporting let.

More generally, given:

\begin{code}
\free ->  let  v_1  = e_1
               v_n  = e_n
               s_1  = f
               s_2  = e_2'
               s_m  = e_m'
          in   s_n
\end{code}

We rewrite:

\begin{code}
\free ->  let  v_1 =  <? let s_1 = f; s_2 = e_2'; s_m = e_m' in e_1 ?>
               v_n =  <? let s_1 = f; s_2 = e_2'; s_m = e_m' in e_n ?>
          in   \y ->  <? let s_1 = f; s_2 = e_2'; s_m = e_m' in s_m y ?>
\end{code}

We then move any binding |v_i| under |v_j| if |v_i| is only used within the body of |v_j|.

\subsubsection{Anything Else}

The final case applies to anything else, including a constructor, a single variable, and an application to an unknown variable. The split is defined in a similar way as for function, but ignoring the complexity of a lambda. Taking the example of:

\begin{code}
\x y ->  let  v_1 = expensive
              v_2 = v_1 x
              v_3 = Con v_2 y v_2
         in   v_3
\end{code}

We simply put all the variables other than the first on the stack inside |<? bullet ?>| brackets:

\begin{code}
\x y -> let v_1 = <? expensive ?>
            v_2 = <? v_1 x ?>
            v_3 = Con v_2 y v_2
        in v_3
\end{code}

We then perform the same sharing operation, noting that |v_1| is only used within |v_2|, to give:

\begin{code}
\x y ->  let  v_2 = <? let v_1 = expensive in v_1 x ?>
              v_3 = Con v_2 x v_2
         in   v_3
\end{code}

More generally, given an expression:

\begin{code}
\free ->  let  s_1 = e_1'
               v_1 = e_1
               v_n = e_n
          in   v
\end{code}

We rewrite to:

\begin{code}
\free ->  let  s_1 = e_1'
               v_1 = <? e_1 ?>
               v_n = <? e_n ?>
          in   v
\end{code}

Then do the sharing trick. Note that the expression |e_1| ends up in the target program, which is fine provided it is not a function, which it isn't because of the second rule. This rule is valid for case expressions, but is unnecessarily restrictive as case doesn't need to worry about sharing.

\subsection{Termination}
\label{sec:term}

The termination rule is responsible for checking that whenever we proceed down a list of expressions we eventually stop. We first describe the mathematical basis of the termination rule, then explain how to apply it to expressions. Finally we show an example of how the termination rule catches non-termination. The intuition is that each expression is a list of expressions bound within the root let, and each subexpression tracks where it came from in the source program. Each successive root let expression must contain either new subexpressions, or fewer subexpressions compared to previous versions.

\subsubsection{Termination Rule}

For termination we need an ordering $\lhd$ such that for every infinite sequence $e_1,e_2 \ldots$ there exist indicies $i < j$ such that $e_i \ntriangleleft e_j$. Orderings which satisfy this criteria are known as well-quasi orderings, and come from Kruskal's Tree Theorem \cite{kruskal:tree}. Most other supercompilers rely on the homeomorphic embedding \cite{leuschel:homeomorphic} over expressions. We instead rely on an ordering over bags (also known as multisets) of values drawn from the finite alphabet $\Sigma$. Our ordering is:

\[
x \lhd y = x \subset_{set} y \vee x \supset_{bag} y
\]

We call a sequence $e_1 \ldots e_n$ well formed iff:

\[
\forall i,j \bullet i < j \Rightarrow e_i \lhd e_j
\]

Under our ordering, the following sequences are well formed:

\begin{code}
[a,aaaaab,aaab,b]
[abc,ab,ac,a]
[aaaaabbb,aaab,aab]
\end{code}

But the following sequences are not:

\begin{code}
[aaa,aaa]
[aabb,ab,abb]
[abc,acc]
\end{code}

Given a finite alphabet $\Sigma$, our ordering is safe. There can only be $2^{\#\Sigma}$ functions that introduce new variables. Given a sequence which doesn't introduce new variables it must terminate as to be a bag subset it must be strictly decreasing.

\subsubsection{Tracking Names}

Every expression throughout the program is assigned a name, which is carefully tracked. A name is a triple, $<f,e,c>$ where $f$ is a function name, $e$ is an expression index and $c$ is a constructor count. We label every expression in the source program with $f$ being the function it comes from, $e$ being a unique index within that function and $c$ being $0$. When manipulating expressions we need to track and update names:

\begin{itemize}
\item If we rename bound variables, we do not update the expression names.
\item If we extract a subexpression we use the name already assigned to that subexpression.
\item If we insert new constructor bindings (when splitting on a case) we use the name assigned to the pattern in the original case expression.
\item If we add variables to the end of a constructor, we increase the constructor count. For example, |let v = C x; w = v y in ...| being transformed to |let v = C x; w = C x y in ...| would have the name for |w| set to the name from |C x| with the constructor count incremented.
\end{itemize}

It is easy to see there are a finite number of names. Each function labels it's subexpressions in advance, so there can only be a finite number. We only increase the constructor count to get new names, and we can only do this up to the arity of a given constructor. Therefore, there are only a finite number of names.

We gain could merge the function name and subexpression index, but that would be harder to track down which expressions come from where.

\subsubsection{Example}

Many small examples programs do not make use of the termination criteria. When calculating the map/map program the recursive program it does hit the termination criteria, but the |flatten| function ignores it. For a program to really hit the termination criteria it usually has to store a buffer of memory which it uses later. The classic example of this pattern is |reverse|, which builds up an accumulated list which it uses later. This results in a termination condition in the |optimise| function. We start with the program:

\begin{code}
main xs = rev [] xs
rev acc xs = case xs of
    []    -> acc
    y:ys  -> rev (y:acc) ys
\end{code}

Here the |rev| call builds up an accumulator argument, which will be equal to the size of |xs|. We can't keep the accumulator around, or we'd fail. The |optimise| function starts with an empty termination context and the expression |rev [] xs|, and calls |reduce|, which results in:

\begin{code}
\xs -> case xs of
    []    -> <? [] ?>
    y:ys  -> <? rev (y:acc) ys ?>
\end{code}

Focusing on the second alternative, we now add |rev [] xs| to the termination context, and continue optimising |rev (y:acc) ys|. This leads to the sequence of expressions:

\begin{code}
\xs -> rev [] xs
\x_1 xs -> rev (x_1:[]) xs
\x_1 x_2 xs -> rev (x_1:x_2:[]) xs
...
\end{code}

We can rewrite these lines in our core language, with annotations for the names:

\begin{code}
\xs ->
    let  v_1 = {- <main,1,0> -} rev
         v_2 = {- <main,2,0> -} []
         v_3 = {- <main,3,0> -} v_1 v_2
         v_4 = {- <main,4,0> -} v_3 xs
    in   v_4
\x_1 xs ->
    let  v_1 = {- <rev,1,0> -} rev
         v_2 = {- <main,2,0> -} []
         v_3 = {- <rev,2,0> -} x_1:v_2
         v_4 = {- <rev,3,0> -} v_1 v_3
         v_5 = {- <rev,4,0> -} v_4 xs
    in   v_5
\x_1 x_2 xs ->
    let  v_1 = {- <rev,1,0> -} rev
         v_2 = {- <main,2,0> -} []
         v_3 = {- <rev,2,0> -} x_1:v_2
         v_4 = {- <rev,2,0> -} x_2:v_3
         v_5 = {- <rev,3,0> -} v_1 v_4
         v_6 = {- <rev,4,0> -} v_5 xs
    in   v_6
\end{code}

Applying our termination criteria, the first two items lead to a well formed sequence, but comparing the second and third we find $e_2 \ntriangleleft e_3$. The first item is permitted because it is the first. The second is permitted as it introduces many new names (such as $<rev,1,0>$). The third item has no additional names to the second, and is not a bag subset (it is in fact a bag superset). Therefore when optimising we stop after the second and reduce it.




Notes: The first would be a homeomorphic embedding - we are in this case less severe than the homeo, but that's not the goal. Because of the simplify rules we end up with a fairly normal form, so it's pretty good that way.



Taking the second and third lines, we can rewrite them in their let form (using vector application to keep the exaple simpler), annotated with names:

\begin{code}
let v = <foldl,1,0> foldl
    

We follow it through, and show that it requires termination at the right place. Note that we have unfolded exactly one buffering of the accumulator, which is rather nice.

\begin{code}
! <<reverse xs>>
\xs -> foldl snoc [] xs

<<foldl snoc [] xs>>
\xs -> case xs of
    [] -> []
    x:xs -> foldl snoc (snoc [] x) xs

! <<foldl snoc (snoc [] x) xs>>
\x xs -> case xs of
    [] -> x:[]
    y:ys -> foldl snoc (snoc (snoc [] x) y) ys

! <<foldl snoc (snoc (snoc [] x) y) ys>>
! <<foldl snoc (snoc t y) ys>>

! <<foldl snoc (snoc t y) xs>>
\x xs -> case xs of
    [] -> x:[]
    z:zs -> foldl snoc (snoc (snoc t y) z) zs

! <<foldl snoc (snoc (snoc t y) z) zs>>
! <<foldl snoc (snoc t y) xs>>
\end{code}


\subsubsection{Argument for Termination}

We use |(+=)| to add to the sequence. We use |terminate| to check that nothing violates the sequence.

Note that we rely on free variables being normalised in a consistent manner to check for termination.

\subsection{Termination Splitting}
\label{sec:term_split}

We don't need to ensure anything too bad, just that at the end we no longer hit the termination criteria. If we were to bind all variables separately at the top-level, then we'd no longer have termination (by the rule of singleton sets never being too small).

We start with all variables at the top, then increase sharing as in \S\ref{sec:eval_split}, with the rule that none of the termination criteria are violated. Running over the map/map fusion example from above we never hit the termination criteria.

As a very special case, if there is only one context there we reduce the number of free variables. This ensures we can't have increasingly more free variables even though there is nothing changing (as a singleton can never hit the termination criteria).

\subsection{Argument for Correctness}

We have not shown the correctness of our supercompiler, but have tested it extensively. The termination argument follows from each section separately, in that we only ever transform an expression in semantics preserving ways.

\subsection{Post-processing}
\label{sec:postprocess}

We often need to post-process quite a bit, in particular we make the following simple transformations:

If two functions are equal we can make them the same.

If a function is called in exactly one place in a non-recursive way we can inline it.

We could have merged these functions in with the rest of the transformations but it would be a lot harder.

\subsection{Comparison to Other Supercompilers}
\label{sec:comparison}

It has no lambda, which makes it different from most, but exactly like Jason Reich's.

It has let everywhere, unlike most supercompilers which almost relegate let to a second class system.

It doesn't use homeomorphic embedding.

It never rewrites expressions, allowing for the tantilising opportunity to partially evaluate the supercompiler.

We use a local termination pile, which means the optimisation of a program is not based on what has passed before. This hopefully leads to better predictability.

It's simple. The details provided in this paper are complete -- we have left nothing out.

Better separation with the manager and the other aspects.

Much easier handling of recursive functions.

\subsubsection{No Lambda}
\label{sec:nolambda}

We don't have lambdas because to simplify them you can duplicate an arbitrary amount of crap. Given a lambda you can bind it twice with different values, which complicates things. Note that a case can only ever resolve to one value, and only bind variables once.

\subsection{Extensions}
\label{sec:extensions}

A real supercompiler needs slightly more flexibility, and the one we have implemented does indeed have all these

\subsubsection{Primitives}

Primitive functions are crucially important, but can be handled by simply treating them like free variables - everything works out.

\subsubsection{Literals}

We add primitives in to the language, and allow |Patt| to be both |Con| and |Lit|. In all other respects we can ignore primitives.

\subsubsection{Case Defaults}

Again, we just augment Patt. There are slight complexities to evaluation splitting when you are dealing with the default branch (don't bind the variable, just project out the right branch), but these aren't too challenging.

\subsubsection{Common Subexpression Elimination}

If we require a simplified let has no duplicate expressions we can do this trivially. We don't because we'd need to be careful about avoiding spaceleaks.

\subsubsection{Inlining Simple Functions}

One possible extension would be to automatically expand some functions whose termination was guaranteed - for example |($)|, |(.)|, |const|, |id|, |otherwise|. This could only be done for expressions which decreased in size.

\section{Examples}
\label{sec:examples}

Here are some examples of supercompiled functions, with a little bit of commentary.

\subsection{Specialisation}

Give map head as an example.

\subsection{Fusion}

You've already seen map/map, but it's important to remember that with our supercompiler lists are not special in any way. For example, if we wrote the identical program on the data type |data MyList alpha = MyNil || MyCons alpha (MyList alpha)| we'd get exactly the same fusion. To give an example:

\begin{code}
data Tree alpha = Leaf alpha | Branch (Tree alpha) (Tree alpha)

root x = flatten (mapTree (+1) x)
flatten -- with continuations
mapTree -- standard fmap on Tree
\end{code}

And we get the right result out.

\subsection{Residual values}

Of course, we can't specialise out |reverse|, for example:

\begin{code}
reverse = foldl (flip (:)) []
\end{code}

But we do get a nice efficient version of |reverse|, that has |foldl| specialised and optimised.

\section{Benchmarks}
\label{sec:benchmarks}

The standard nofib ones, eek!

\subsection{Execution Speed}

See how great we do, yay.

\subsection{Compilation Speed}

Our compiler is quite quick, we've broken in down in to the time to compile and the result. The tricks we use to speed up compilation are variable normalising, using a map for let expressions, and we could also reduce the termination keys in to |Int|, but we haven't bothered yet.

Our compiler is whole program, although we could split it up by defining interface points which are not violated. We haven't bothered to do so yet.

\section{Case Studies}

We have aimed for a practical supercompiler, and in this section we outline two practical purposes to which we have already deployed our supercompiler.

\subsection{Optimisation of HTML Parsing}
\label{sec:tagsoup}

The TagSoup library \cite{tagsoup} is a simple parser for XML/HTML, based on the HTML 5 specification. Given a String, TagSoup produces a list of tokens (such as tag open, tag close, attribute). The parser was deliberately written in a way that mirrors the HTML 5 specification, which is based around a state passing approach. Each rule has been modelled in the most direct way, and then a supporting library simplifies it. For example, section 9.2.4.10 of the HTML 5 specification states:

\begin{quote}
9.2.4.10 Attribute value (double-quoted) state

Consume the next input character:

U+0022 QUOTATION MARK (") - Switch to the after attribute value (quoted) state.

U+0026 AMPERSAND (\&) - Switch to the character reference in attribute value state, with the additional allowed character being U+0022 QUOTATION MARK (").

EOF - Parse error. Reconsume the EOF character in the data state.

Anything else - Append the current input character to the current attribute's value. Stay in the attribute value (double-quoted) state.
\end{quote}

And the corresponding code is:

\begin{code}
-- 9.2.4.10 Attribute value (double-quoted) state
attValueDQuoted S{..} = pos $ case hd of
    '\"' -> afterAttValueQuoted tl
    '&' -> charRefAttValue attValueDQuoted (Just '\"') tl
    _ | eof -> errWant "\"" & dat s
    _ -> hd & attValueDQuoted xml tl
\end{code}

Here |tl| is the next state, |hd| is the current character, and the initial |pos| call emits position information. The |(&)| operator is used to place a token on the output stream. However, this high level of abstraction has a noticeable performance penalty, for each output token there are several intermediate values created. While work on list fusion can often reduce intermediate lists, the values here have much more structure than lists, and thus this work is not appropriate. There is a strong desire not to complicate the specification by adding details that improve performance.

The code is split as 308 lines translated from the spec, followed by 191 lines implementing the operations and putting together the results in the right format.

We slightly prime the supercompiler. The optimisation is controlled by 4 booleans, and by freezing them we manage to take fast paths -- for example when generating a stream without position information then |pos| calls are entirely eliminated. We did the same trick with GHC, but it was negligible -- mainly we suspect that the SpecConstr wasn't able to specialise through all the loops, which would have eliminated it. Plus there is no opportunity for fusion.

One might ask whether optimising an HTML parser is worthwhile, but the answer is decidedly yes. The TagSoup library is used for DNA processing, and nightly gets run over 40Gb of XML files. The bottleneck is currently TagSoup, but with these transformations we eliminate that.

Note that our HTML parser is a perfect use case for supercompilation. There is no hot-spot in the program that takes up more time, the problem is that the overhead of the abstraction is throughout. The abstraction is nicely chosen to map to an external document, so the abstraction cannot easily be altered. We are also constrained by speed. Supercompilation delivers nicely, removing all the abstractions to make them valuable at compile time, and yet removed by runtime.

\subsection{Equality of Open Expressions}
\label{sec:hlint}

The HLint program \cite{hlint} is a tool for helping improve Haskell source code. A large number of its hints are based on replacing one open expression with another. For example, if the user writes |concat (map f (xs++ys))| it will suggest replacing it with |concatMap f (xs++ys)|. It works by having a list of open expressions it uses for replacement:

\begin{code}
forall f x . concat (map f x) ==> concatMap f x
\end{code}

These rules are written in a supporting file, and there are many of them. Recently one bug was filed stating that one of the rules (involving |foldl| and |map| fusion) was incorrect. After fixing that another incorrect hint was discovered. The intention is clearly that in most cases the left and right expressions are equal. More accurately, the left and right open expressions should be equal. A supercompiler transforms a program and often produces equivalent expressions.

We supercompiled the left and right hand sides. Of the \unknown{} rules, they fit in to three categories:

\paragraph{Type class based on both sides}

If both the left and the right have type classes, i.e. |not (a == b) ==> a /= b|, we can't check anything. This rule is not true in Haskell unless the standard typeclass rules have been followed. This pattern accounts for \unknown{} rules.

\paragraph{Generalisation}

For example |(\(x,y) -> (f x, g y)) ==> f *** g|, while this is somewhat true, it is actually a generalisation. (Note that there is a second condition that f and g do not contain x or y in them). The first expression works on tuples only, while the second is generalise to all arrrows -- of which tuples are one special case. This pattern accounts for 2 rules.

\paragraph{Remaining examples}

These are examples where the equality of both sides is true, and both should be equivalent. Of the remaining ones, we can prove them. Most examples are trivial, but some are slightly more involved (i.e. map fusion rules). For example we were able to prove:

\begin{code}
\end{code}

We can't prove \unknown{} examples. Here are two representative examples:

\paragraph{Generalisation}

\begin{code}
(if a then True else False) ==> a
\end{code}

Here the examples are not equivalent, technically, as the right could be of any type. It would be possible to compress the right hand side in a post processing, but we don't yet do that.

\paragraph{Strictness}

We cannot prove:

\begin{code}
error "Use isPrefixOf" = (take i s == t) ==> ((i == length t) && (t `isPrefixOf` s))
\end{code}

The idea here is that people write |take 4 s == "ICFP"|, when they should have written |"ICFP" `isPrefixOf` s|. Usually the first term |(4 == length "ICFP")| will be eliminated. In addition the rule has a side condition that both |i| and |t| must be concrete literals. The supercompiler produces different expressions for both sides. this is in fact important -- the first expression is lazy in the spine of the |t|, while the right hand side is not. Unless the condition is applied, which the supercompiler can't see.

We currently have no way to tell the supercompiler that an expression is fully evaluated, but perhaps we should and include strictness information throughout.

\paragraph{Finding a bug}

We found a bug. In HLint 1.6.19 there was a rule to reduce |foldr/map| with the rule:

\begin{code}
foldr f z (map g x) ==> foldr (f . g) z x
\end{code}

Unfortunately we also got the equivalent for |foldr1|, namely:

\begin{code}
foldr1 f (map g x) ==> foldr1 (f . g) x
\end{code}

This rule is not true, and our supercompiler spotted it. \footnote{Note that in this case the types are different, which should have given the game away. Unfortunately our type check method simply checks the types can unify (a very cheap check) which doesn't pick this up.}

\section{Related Work}

We have extensively covered the related work in supercompilation in \S\ref{sec:comparison}. Our work is definately classed as a supercompiler, but makes a large number of novel decisions. In particular, where often supercompiler authors have a choice at some points, many of our design decisions are forced upon us. We hope that this leads to decisions that naturally fit together. Whatever the result, our supercompiler is certainly a new point on the design space.

Work on list fusion etc. is usually limited to lists, which isn't great.

Partial evaluation has it's place, but supercompilers tend to be better for changing data -- for example the tagsoup would have frozen the parameters with partial evaluation, but not eliminated intermediate data structures.

One intruiging possibility is that our supercompiler as described may actually be a great target for partial evaluation. The program being supercompiled is static, and in our supercompiler is not perturbed to the extent of other supercompilers.

Many program optimisation techniques reduce abstraction, some such as fusion and specialisation have been covered extensively. However, the neat thing about supercompilation is that fusion (particularly list fusion) falls out naturally by the way the program is written -- no specific rules such as foldr/build or stream/unstream. In particular, we can fuse away |words . length|, even though in general there is no fusion rule to do that.



\section{Conclusions and Future Work}

Need more benchmarks, integrate into a production quality compiler (GHC). Our supercompiler has been designed to run faster, but does it really do so?

\subsection{Compile Time Performance}

We can implement this construct much more efficiently, in particular significantly more efficiently than a homeomorphic embedding. We can reify this sequence as |Map Name Int|.

We can change names to be a function name and subexpression index combined. We can also free up the last 6 bits to store the constructor count, allowing over 60 million subexpressions even with 32 bit integers. We now have very efficient names.

We can pre-resolve in to a decision table or finite state machine very easily. However, it's not really necessary - the basic test is dead fast, if you use ordered lists.

\subsection{Run Time Performance}

What about merging a rules engine. What about strictness analysis.

\subsection{Conclusions}

We have presented a supercompiler which is simple, and has found practical use. What a great result.


\subsection{Acknowledgements}

Thanks to lots of people for helpful ideas and discussions. Particularly I'd like to thank Jason Reich, Simon Peyton Jones, Max Bolingbroke and Peter Jonsson.

\bibliographystyle{plainnat}
\bibliography

\end{document}
